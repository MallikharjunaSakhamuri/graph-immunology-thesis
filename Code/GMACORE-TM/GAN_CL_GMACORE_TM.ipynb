{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "5379f427",
      "metadata": {
        "id": "5379f427"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "from rdkit import Chem\n",
        "from rdkit.Chem import AllChem, Descriptors\n",
        "from typing import Tuple\n",
        "import numpy as np\n",
        "from torch_geometric.data import Data\n",
        "import random\n",
        "from torch_geometric.loader import DataLoader\n",
        "from torch_geometric.data import Data\n",
        "from rdkit import RDLogger\n",
        "from rdkit.Chem import RemoveHs\n",
        "from datetime import datetime\n",
        "\n",
        "import torch\n",
        "import numpy as np\n",
        "from torch_geometric.data import DataLoader\n",
        "import os\n",
        "import json\n",
        "from tqdm import tqdm\n",
        "import pickle\n",
        "from datetime import datetime\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from torch_geometric.nn import GCNConv, global_mean_pool, MessagePassing\n",
        "from typing import Tuple, List, Optional\n",
        "import copy\n",
        "from dataclasses import dataclass\n",
        "\n",
        "# Suppress RDKit warnings\n",
        "RDLogger.DisableLog('rdApp.warning')\n",
        "timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "8de17bc6",
      "metadata": {
        "id": "8de17bc6"
      },
      "outputs": [],
      "source": [
        "class MolecularFeatureExtractor:\n",
        "    def __init__(self):\n",
        "        self.atom_list = list(range(1, 119))\n",
        "        self.chirality_list = [\n",
        "            Chem.rdchem.ChiralType.CHI_UNSPECIFIED,\n",
        "            Chem.rdchem.ChiralType.CHI_TETRAHEDRAL_CW,\n",
        "            Chem.rdchem.ChiralType.CHI_TETRAHEDRAL_CCW,\n",
        "            Chem.rdchem.ChiralType.CHI_OTHER\n",
        "        ]\n",
        "        self.bond_list = [\n",
        "            Chem.rdchem.BondType.SINGLE,\n",
        "            Chem.rdchem.BondType.DOUBLE,\n",
        "            Chem.rdchem.BondType.TRIPLE,\n",
        "            Chem.rdchem.BondType.AROMATIC\n",
        "        ]\n",
        "        self.bonddir_list = [\n",
        "            Chem.rdchem.BondDir.NONE,\n",
        "            Chem.rdchem.BondDir.ENDUPRIGHT,\n",
        "            Chem.rdchem.BondDir.ENDDOWNRIGHT\n",
        "        ]\n",
        "\n",
        "    def calc_atom_features(self, atom: Chem.Atom) -> Tuple[list, list]:\n",
        "        \"\"\"Calculate atom features with better error handling\"\"\"\n",
        "        try:\n",
        "            # Basic features\n",
        "            atom_feat = [\n",
        "                self.atom_list.index(atom.GetAtomicNum()),\n",
        "                self.chirality_list.index(atom.GetChiralTag())\n",
        "            ]\n",
        "\n",
        "            # Physical features with error handling\n",
        "            phys_feat = []\n",
        "\n",
        "            # Molecular weight contribution\n",
        "            try:\n",
        "                contrib_mw = Descriptors.ExactMolWt(Chem.MolFromSmiles(f'[{atom.GetSymbol()}]'))\n",
        "                phys_feat.append(contrib_mw)\n",
        "            except:\n",
        "                phys_feat.append(0.0)\n",
        "\n",
        "            # LogP contribution\n",
        "            try:\n",
        "                contrib_logp = Descriptors.MolLogP(Chem.MolFromSmiles(f'[{atom.GetSymbol()}]'))\n",
        "                phys_feat.append(contrib_logp)\n",
        "            except:\n",
        "                phys_feat.append(0.0)\n",
        "\n",
        "            # Add other physical properties\n",
        "            phys_feat.extend([\n",
        "                atom.GetFormalCharge(),\n",
        "                int(atom.GetHybridization()),\n",
        "                int(atom.GetIsAromatic()),\n",
        "                atom.GetTotalNumHs(),\n",
        "                atom.GetTotalValence(),\n",
        "                atom.GetDegree()\n",
        "            ])\n",
        "\n",
        "            return atom_feat, phys_feat\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"Error calculating atom features: {e}\")\n",
        "            return [0, 0], [0.0] * 9\n",
        "\n",
        "    def get_atom_features(self, mol: Chem.Mol) -> Tuple[torch.Tensor, torch.Tensor]:\n",
        "        \"\"\"Extract atom features for the whole molecule\"\"\"\n",
        "        atom_feats = []\n",
        "        phys_feats = []\n",
        "\n",
        "        if mol is None:\n",
        "            return torch.tensor([[0, 0]], dtype=torch.long), torch.tensor([[0.0] * 9], dtype=torch.float)\n",
        "\n",
        "        for atom in mol.GetAtoms():\n",
        "            atom_feat, phys_feat = self.calc_atom_features(atom)\n",
        "            atom_feats.append(atom_feat)\n",
        "            phys_feats.append(phys_feat)\n",
        "\n",
        "        x = torch.tensor(atom_feats, dtype=torch.long)\n",
        "        phys = torch.tensor(phys_feats, dtype=torch.float)\n",
        "\n",
        "        return x, phys\n",
        "\n",
        "    def remove_unbonded_hydrogens(mol):\n",
        "        params = Chem.RemoveHsParameters()\n",
        "        params.removeDegreeZero = True\n",
        "        mol = Chem.RemoveHs(mol, params)\n",
        "        return mol\n",
        "\n",
        "\n",
        "    def get_bond_features(self, mol: Chem.Mol) -> Tuple[torch.Tensor, torch.Tensor]:\n",
        "        \"\"\"Extract bond features with better error handling\"\"\"\n",
        "        if mol is None:\n",
        "            return torch.tensor([[0], [0]], dtype=torch.long), torch.tensor([[0.0] * 5], dtype=torch.float)\n",
        "\n",
        "        row, col, edge_feat = [], [], []\n",
        "\n",
        "        for bond in mol.GetBonds():\n",
        "            try:\n",
        "                start, end = bond.GetBeginAtomIdx(), bond.GetEndAtomIdx()\n",
        "\n",
        "                # Add edges in both directions\n",
        "                row += [start, end]\n",
        "                col += [end, start]\n",
        "\n",
        "                # Bond features\n",
        "                bond_type = self.bond_list.index(bond.GetBondType())\n",
        "                bond_dir = self.bonddir_list.index(bond.GetBondDir())\n",
        "\n",
        "                # Calculate additional properties\n",
        "                feat = [\n",
        "                    bond_type,\n",
        "                    bond_dir,\n",
        "                    int(bond.GetIsConjugated()),\n",
        "                    int(self._is_rotatable(bond)),\n",
        "                    self._get_bond_length(mol, start, end)\n",
        "                ]\n",
        "\n",
        "                edge_feat.extend([feat, feat])\n",
        "\n",
        "            except Exception as e:\n",
        "                print(f\"Error processing bond: {e}\")\n",
        "                continue\n",
        "\n",
        "        if not row:  # If no valid bonds were processed\n",
        "            return torch.tensor([[0], [0]], dtype=torch.long), torch.tensor([[0.0] * 5], dtype=torch.float)\n",
        "\n",
        "        edge_index = torch.tensor([row, col], dtype=torch.long)\n",
        "        edge_attr = torch.tensor(edge_feat, dtype=torch.float)\n",
        "\n",
        "        return edge_index, edge_attr\n",
        "\n",
        "    def _is_rotatable(self, bond: Chem.Bond) -> bool:\n",
        "        \"\"\"Check if bond is rotatable\"\"\"\n",
        "        return (bond.GetBondType() == Chem.rdchem.BondType.SINGLE and\n",
        "                not bond.IsInRing() and\n",
        "                len(bond.GetBeginAtom().GetNeighbors()) > 1 and\n",
        "                len(bond.GetEndAtom().GetNeighbors()) > 1)\n",
        "\n",
        "    def _get_bond_length(self, mol: Chem.Mol, start: int, end: int) -> float:\n",
        "        \"\"\"Get bond length with error handling\"\"\"\n",
        "        try:\n",
        "            conf = mol.GetConformer()\n",
        "            if conf.Is3D():\n",
        "                return Chem.rdMolTransforms.GetBondLength(conf, start, end)\n",
        "        except:\n",
        "            pass\n",
        "        return 0.0\n",
        "\n",
        "    def process_molecule(self, smiles: str) -> Data:\n",
        "        \"\"\"Process SMILES string to graph data\"\"\"\n",
        "        try:\n",
        "            mol = Chem.MolFromSmiles(smiles)\n",
        "            if mol is None:\n",
        "                print(f\"Invalid SMILES: {smiles}\")\n",
        "                return None  # Skip invalid molecules\n",
        "            mol = RemoveHs(mol)\n",
        "\n",
        "            # Add explicit hydrogens\n",
        "            mol = Chem.AddHs(mol, addCoords=True)\n",
        "\n",
        "            # Sanitize molecule\n",
        "            Chem.SanitizeMol(mol)\n",
        "\n",
        "            # Check if the molecule has atoms\n",
        "            if mol.GetNumAtoms() == 0:\n",
        "                print(\"Molecule has no atoms, skipping.\")\n",
        "                return None\n",
        "\n",
        "            # Generate 3D coordinates\n",
        "            if not mol.GetNumConformers():\n",
        "                status = AllChem.EmbedMolecule(mol, AllChem.ETKDG())\n",
        "                if status != 0:\n",
        "                    print(\"Failed to generate 3D conformer\")\n",
        "                    return None  # Skip failed molecules\n",
        "\n",
        "                # Try MMFF or UFF optimization\n",
        "                try:\n",
        "                    AllChem.MMFFOptimizeMolecule(mol)\n",
        "                except:\n",
        "                    AllChem.UFFOptimizeMolecule(mol)\n",
        "\n",
        "            # Extract features\n",
        "            x_cat, x_phys = self.get_atom_features(mol)\n",
        "            edge_index, edge_attr = self.get_bond_features(mol)\n",
        "\n",
        "            return Data(\n",
        "                x_cat=x_cat,\n",
        "                x_phys=x_phys,\n",
        "                edge_index=edge_index,\n",
        "                edge_attr=edge_attr,\n",
        "                num_nodes=x_cat.size(0)\n",
        "            )\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"Error processing molecule {smiles}: {e}\")\n",
        "            return None\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "35a311ed",
      "metadata": {
        "id": "35a311ed"
      },
      "outputs": [],
      "source": [
        "class MemoryQueue:\n",
        "    \"\"\"Memory queue with temporal decay for contrastive learning\"\"\"\n",
        "    def __init__(self, size: int, dim: int, decay: float = 0.99999):\n",
        "        self.size = size\n",
        "        self.dim = dim\n",
        "        self.decay = decay\n",
        "        self.ptr = 0\n",
        "        self.full = False\n",
        "\n",
        "        # Initialize queue\n",
        "        self.queue = nn.Parameter(F.normalize(torch.randn(size, dim), dim=1), requires_grad=False)\n",
        "        self.queue_age = nn.Parameter(torch.zeros(size), requires_grad=False)\n",
        "\n",
        "#         self.register_buffer(\"queue\", torch.randn(size, dim))\n",
        "#         self.register_buffer(\"queue_age\", torch.zeros(size))  # Track age of each entry\n",
        "        self.queue = F.normalize(self.queue, dim=1)\n",
        "\n",
        "    def update_queue(self, keys: torch.Tensor):\n",
        "        \"\"\"Update queue with new keys\"\"\"\n",
        "        batch_size = keys.shape[0]\n",
        "\n",
        "        # Increment age of all entries\n",
        "        self.queue_age += 1\n",
        "\n",
        "        # Add new keys\n",
        "        if self.ptr + batch_size <= self.size:\n",
        "            self.queue[self.ptr:self.ptr + batch_size] = keys\n",
        "            self.queue_age[self.ptr:self.ptr + batch_size] = 0\n",
        "        else:\n",
        "            # Handle overflow\n",
        "            rem = self.size - self.ptr\n",
        "            self.queue[self.ptr:] = keys[:rem]\n",
        "            self.queue[:batch_size-rem] = keys[rem:]\n",
        "            self.queue_age[self.ptr:] = 0\n",
        "            self.queue_age[:batch_size-rem] = 0\n",
        "            self.full = True\n",
        "\n",
        "        self.ptr = (self.ptr + batch_size) % self.size\n",
        "\n",
        "    def get_decay_weights(self) -> torch.Tensor:\n",
        "        \"\"\"Get temporal decay weights for queue entries\"\"\"\n",
        "        return self.decay ** self.queue_age\n",
        "\n",
        "    def compute_contrastive_loss(self, query: torch.Tensor, positive_key: torch.Tensor,\n",
        "                                temperature: float = 0.07) -> torch.Tensor:\n",
        "        \"\"\"Compute contrastive loss with temporal decay\"\"\"\n",
        "        # Normalize embeddings\n",
        "        query = F.normalize(query, dim=1)\n",
        "        positive_key = F.normalize(positive_key, dim=1)\n",
        "        queue = F.normalize(self.queue, dim=1)\n",
        "\n",
        "        # Compute logits\n",
        "        l_pos = torch.einsum('nc,nc->n', [query, positive_key]).unsqueeze(-1)\n",
        "        l_neg = torch.einsum('nc,ck->nk', [query, queue.T])\n",
        "\n",
        "        # Apply temporal decay to negative samples\n",
        "        decay_weights = self.get_decay_weights()\n",
        "        l_neg = l_neg * decay_weights.unsqueeze(0)\n",
        "\n",
        "        # Temperature scaling\n",
        "        logits = torch.cat([l_pos, l_neg], dim=1) / temperature\n",
        "        labels = torch.zeros(logits.shape[0], dtype=torch.long, device=query.device)\n",
        "\n",
        "        return F.cross_entropy(logits, labels)\n",
        "\n",
        "class GraphGenerator(nn.Module):\n",
        "    \"\"\"Generator network with proper feature handling\"\"\"\n",
        "    def __init__(self, node_dim: int, edge_dim: int, hidden_dim: int = 128):\n",
        "        super().__init__()\n",
        "\n",
        "        # Node feature processing\n",
        "        self.node_encoder = nn.Sequential(\n",
        "            nn.Linear(node_dim, hidden_dim),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(hidden_dim, hidden_dim)\n",
        "        )\n",
        "\n",
        "        # Edge feature processing\n",
        "        self.edge_encoder = nn.Sequential(\n",
        "            nn.Linear(edge_dim, hidden_dim),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(hidden_dim, hidden_dim)\n",
        "        )\n",
        "\n",
        "        # Graph convolution layers\n",
        "        self.conv1 = GCNConv(hidden_dim, hidden_dim)\n",
        "        self.conv2 = GCNConv(hidden_dim, hidden_dim)\n",
        "        self.conv3 = GCNConv(hidden_dim, hidden_dim)\n",
        "\n",
        "        # Importance prediction layers\n",
        "        self.node_importance = nn.Sequential(\n",
        "            nn.Linear(hidden_dim, hidden_dim // 2),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(hidden_dim // 2, 1),\n",
        "            nn.Sigmoid()\n",
        "        )\n",
        "\n",
        "        self.edge_importance = nn.Sequential(\n",
        "            nn.Linear(hidden_dim * 2, hidden_dim),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(hidden_dim, 1),\n",
        "            nn.Sigmoid()\n",
        "        )\n",
        "\n",
        "    def normalize_features(self, x_cat, x_phys):\n",
        "        \"\"\"Normalize categorical and physical features separately\"\"\"\n",
        "        # Convert categorical features to one-hot\n",
        "        x_cat = x_cat.float()\n",
        "\n",
        "        # Normalize physical features\n",
        "        x_phys = x_phys.float()\n",
        "        if x_phys.size(0) > 1:  # Only normalize if we have more than one sample\n",
        "            x_phys = (x_phys - x_phys.mean(0)) / (x_phys.std(0) + 1e-5)\n",
        "\n",
        "        return x_cat, x_phys\n",
        "\n",
        "    def forward(self, data) -> Tuple[torch.Tensor, torch.Tensor]:\n",
        "        # Normalize features\n",
        "        x_cat, x_phys = self.normalize_features(data.x_cat, data.x_phys)\n",
        "\n",
        "        # Concatenate features\n",
        "        x = torch.cat([x_cat, x_phys], dim=-1)\n",
        "\n",
        "        edge_index = data.edge_index\n",
        "        edge_attr = data.edge_attr.float()  # Ensure float type\n",
        "\n",
        "        # Initial feature encoding\n",
        "        x = self.node_encoder(x)\n",
        "        edge_attr = self.edge_encoder(edge_attr)\n",
        "\n",
        "        # Graph convolutions\n",
        "        x = F.relu(self.conv1(x, edge_index))  # Removed edge_attr from GCNConv\n",
        "        x = F.relu(self.conv2(x, edge_index))\n",
        "        x = self.conv3(x, edge_index)\n",
        "\n",
        "        # Predict importance scores\n",
        "        node_scores = self.node_importance(x)\n",
        "\n",
        "        # Edge scores using both connected nodes\n",
        "        edge_features = torch.cat([\n",
        "            x[edge_index[0]],\n",
        "            x[edge_index[1]]\n",
        "        ], dim=-1)\n",
        "        edge_scores = self.edge_importance(edge_features)\n",
        "\n",
        "        return node_scores, edge_scores\n",
        "\n",
        "def get_model_config(dataset):\n",
        "    \"\"\"Get model configuration based on dataset features\"\"\"\n",
        "    sample_data = dataset[0]\n",
        "\n",
        "    # Calculate input dimensions\n",
        "    node_dim = sample_data.x_cat.shape[1] + sample_data.x_phys.shape[1]\n",
        "    edge_dim = sample_data.edge_attr.shape[1]\n",
        "\n",
        "    config = GanClConfig(\n",
        "        node_dim=node_dim,\n",
        "        edge_dim=edge_dim,\n",
        "        hidden_dim=128,\n",
        "        output_dim=128,\n",
        "        queue_size=65536,\n",
        "        momentum=0.999,\n",
        "        temperature=0.07,\n",
        "        decay=0.99999,\n",
        "        dropout_ratio=0.25\n",
        "    )\n",
        "\n",
        "    return config\n",
        "\n",
        "class GraphDiscriminator(nn.Module):\n",
        "    \"\"\"Discriminator/Encoder network\"\"\"\n",
        "    def __init__(self, node_dim: int, edge_dim: int, hidden_dim: int = 128, output_dim: int = 128):\n",
        "        super().__init__()\n",
        "\n",
        "        # Feature encoding\n",
        "        self.node_encoder = nn.Sequential(\n",
        "            nn.Linear(node_dim, hidden_dim),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(hidden_dim, hidden_dim)\n",
        "        )\n",
        "\n",
        "        self.edge_encoder = nn.Sequential(\n",
        "            nn.Linear(edge_dim, hidden_dim),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(hidden_dim, hidden_dim)\n",
        "        )\n",
        "\n",
        "        # Graph convolution layers\n",
        "        self.conv1 = GCNConv(hidden_dim, hidden_dim)\n",
        "        self.conv2 = GCNConv(hidden_dim, hidden_dim)\n",
        "        self.conv3 = GCNConv(hidden_dim, output_dim)\n",
        "\n",
        "        # Projection head for contrastive learning\n",
        "        self.projection = nn.Sequential(\n",
        "            nn.Linear(output_dim, hidden_dim),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(hidden_dim, output_dim)\n",
        "        )\n",
        "\n",
        "    def normalize_features(self, x_cat, x_phys):\n",
        "        \"\"\"Normalize categorical and physical features separately\"\"\"\n",
        "        # Convert categorical features to one-hot\n",
        "        x_cat = x_cat.float()\n",
        "\n",
        "        # Normalize physical features\n",
        "        x_phys = x_phys.float()\n",
        "        if x_phys.size(0) > 1:  # Only normalize if we have more than one sample\n",
        "            x_phys = (x_phys - x_phys.mean(0)) / (x_phys.std(0) + 1e-5)\n",
        "\n",
        "        return x_cat, x_phys\n",
        "\n",
        "    def forward(self, data):\n",
        "        # Normalize features\n",
        "        x_cat, x_phys = self.normalize_features(data.x_cat, data.x_phys)\n",
        "\n",
        "        # Concatenate features\n",
        "        x = torch.cat([x_cat, x_phys], dim=-1)\n",
        "\n",
        "        edge_index = data.edge_index\n",
        "        edge_attr = data.edge_attr.float()  # Ensure float type\n",
        "        batch = data.batch\n",
        "\n",
        "        # Initial feature encoding\n",
        "        x = self.node_encoder(x)\n",
        "        edge_attr = self.edge_encoder(edge_attr)\n",
        "\n",
        "        # Graph convolutions\n",
        "        x = F.relu(self.conv1(x, edge_index))  # Removed edge_attr from GCNConv\n",
        "        x = F.relu(self.conv2(x, edge_index))\n",
        "        x = self.conv3(x, edge_index)\n",
        "\n",
        "        # Global pooling\n",
        "        x = global_mean_pool(x, batch)\n",
        "\n",
        "        # Projection\n",
        "        x = self.projection(x)\n",
        "\n",
        "        return x\n",
        "\n",
        "@dataclass\n",
        "class GanClConfig:\n",
        "    \"\"\"Configuration for GAN-CL training\"\"\"\n",
        "    node_dim: int\n",
        "    edge_dim: int\n",
        "    hidden_dim: int = 128\n",
        "    output_dim: int = 128\n",
        "    queue_size: int = 65536\n",
        "    momentum: float = 0.999\n",
        "    temperature: float = 0.07\n",
        "    decay: float = 0.99999\n",
        "    dropout_ratio: float = 0.25\n",
        "\n",
        "class MolecularGANCL(nn.Module):\n",
        "    \"\"\"Combined GAN and Contrastive Learning framework\"\"\"\n",
        "    def __init__(self, config: GanClConfig):\n",
        "        super().__init__()\n",
        "        self.config = config\n",
        "\n",
        "        # Add weight initialization\n",
        "        def init_weights(m):\n",
        "            if isinstance(m, nn.Linear):\n",
        "                torch.nn.init.xavier_uniform_(m.weight)\n",
        "                m.bias.data.fill_(0.01)\n",
        "\n",
        "        # Initialize networks\n",
        "        self.generator = GraphGenerator(\n",
        "            config.node_dim,\n",
        "            config.edge_dim,\n",
        "            config.hidden_dim * 2\n",
        "        )\n",
        "\n",
        "        self.encoder = GraphDiscriminator(\n",
        "            config.node_dim,\n",
        "            config.edge_dim,\n",
        "            config.hidden_dim,\n",
        "            config.output_dim\n",
        "        )\n",
        "        self.encoder.apply(init_weights)\n",
        "\n",
        "        # Modified loss weights\n",
        "        self.contrastive_weight = 1.0\n",
        "        self.adversarial_weight = 0.1  # Increased from 0.05\n",
        "        self.similarity_weight = 0.01  # Decreased from 0.1\n",
        "\n",
        "        # Temperature annealing\n",
        "        self.initial_temperature = 0.1\n",
        "        self.min_temperature = 0.05\n",
        "\n",
        "        # Create momentum encoder\n",
        "        self.momentum_encoder = copy.deepcopy(self.encoder)\n",
        "        for param in self.momentum_encoder.parameters():\n",
        "            param.requires_grad = False\n",
        "\n",
        "        # Initialize memory queue\n",
        "        self.memory_queue = MemoryQueue(\n",
        "            config.queue_size,\n",
        "            config.output_dim,\n",
        "            config.decay\n",
        "        )\n",
        "\n",
        "    @torch.no_grad()\n",
        "    def _momentum_update(self):\n",
        "        \"\"\"Update momentum encoder\"\"\"\n",
        "        for param_q, param_k in zip(self.encoder.parameters(),\n",
        "                                  self.momentum_encoder.parameters()):\n",
        "            param_k.data = self.config.momentum * param_k.data + \\\n",
        "                          (1 - self.config.momentum) * param_q.data\n",
        "\n",
        "    def drop_graph_elements(self, data, node_scores: torch.Tensor,\n",
        "                          edge_scores: torch.Tensor) -> Data:\n",
        "        \"\"\"Apply dropout to graph based on importance scores\"\"\"\n",
        "        # Select elements to keep based on scores and dropout ratio\n",
        "#         node_mask = (node_scores < self.config.dropout_ratio).float()\n",
        "#         edge_mask = (edge_scores < self.config.dropout_ratio).float()\n",
        "\n",
        "        node_mask = (torch.rand_like(node_scores) > self.config.dropout_ratio).float()\n",
        "        edge_mask = (torch.rand_like(edge_scores) > self.config.dropout_ratio).float()\n",
        "\n",
        "        # Apply masks\n",
        "        x_cat_new = data.x_cat * node_mask\n",
        "        x_phys_new = data.x_phys * node_mask\n",
        "        edge_attr_new = data.edge_attr * edge_mask\n",
        "\n",
        "        # Create new graph data object\n",
        "        return Data(\n",
        "            x_cat=x_cat_new,\n",
        "            x_phys=x_phys_new,\n",
        "            edge_index=data.edge_index,\n",
        "            edge_attr=edge_attr_new,\n",
        "            batch=data.batch\n",
        "        )\n",
        "\n",
        "    def get_temperature(self, epoch, total_epochs):\n",
        "        \"\"\"Anneal temperature during training\"\"\"\n",
        "        progress = epoch / total_epochs\n",
        "        return max(self.initial_temperature * (1 - progress), self.min_temperature)\n",
        "\n",
        "    def forward(self, data, epoch=0, total_epochs=50):\n",
        "        # Get current temperature\n",
        "        temperature = self.get_temperature(epoch, total_epochs)\n",
        "\n",
        "        # Get importance scores from generator\n",
        "        node_scores, edge_scores = self.generator(data)\n",
        "\n",
        "        # Create perturbed graph\n",
        "        perturbed_data = self.drop_graph_elements(data, node_scores, edge_scores)\n",
        "\n",
        "        # Get embeddings\n",
        "        query_emb = self.encoder(perturbed_data)\n",
        "        with torch.no_grad():\n",
        "            key_emb = self.momentum_encoder(data)\n",
        "            original_emb = self.encoder(data).detach()\n",
        "\n",
        "        # Compute losses with modified weights\n",
        "        contrastive_loss = self.memory_queue.compute_contrastive_loss(\n",
        "            query_emb, key_emb, temperature\n",
        "        ) * self.contrastive_weight\n",
        "\n",
        "        adversarial_loss = -F.mse_loss(query_emb, original_emb) * self.adversarial_weight\n",
        "        similarity_loss = F.mse_loss(query_emb, original_emb) * self.similarity_weight\n",
        "\n",
        "        return contrastive_loss, adversarial_loss, similarity_loss\n",
        "\n",
        "    def get_embeddings(self, data) -> torch.Tensor:\n",
        "        \"\"\"Get embeddings for downstream tasks\"\"\"\n",
        "        with torch.no_grad():\n",
        "            return self.encoder(data)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "3eb075ba",
      "metadata": {
        "id": "3eb075ba"
      },
      "outputs": [],
      "source": [
        "def extract_molecule_metadata(dataset):\n",
        "    \"\"\"Extract metadata from PyG graph data without relying on SMILES strings\"\"\"\n",
        "    from rdkit import Chem\n",
        "    from rdkit.Chem import Descriptors\n",
        "    from tqdm import tqdm\n",
        "    import numpy as np\n",
        "    import torch\n",
        "    import networkx as nx\n",
        "    from collections import defaultdict\n",
        "\n",
        "    metadata = []\n",
        "\n",
        "    for i, data in enumerate(tqdm(dataset, desc=\"Extracting molecule metadata\")):\n",
        "        # Set graph ID\n",
        "        mol_id = f\"molecule_{i}\"\n",
        "\n",
        "        # Initialize empty dictionaries for metadata\n",
        "        properties = {}\n",
        "        features = {}\n",
        "        functional_groups = {}\n",
        "        ring_info = {\"ring_counts\": {}, \"ring_sizes\": {}}\n",
        "\n",
        "        # Extract basic graph properties directly from the PyG data\n",
        "        if hasattr(data, 'num_nodes') and hasattr(data, 'edge_index'):\n",
        "            try:\n",
        "                # Convert to networkx graph for analysis\n",
        "                G = to_networkx(data)\n",
        "\n",
        "                # Calculate graph-level properties\n",
        "                num_edges = data.edge_index.size(1) // 2  # Count unique edges\n",
        "                properties = {\n",
        "                    \"num_nodes\": data.num_nodes,\n",
        "                    \"num_edges\": num_edges,\n",
        "                    \"avg_node_degree\": 2 * num_edges / data.num_nodes if data.num_nodes > 0 else 0\n",
        "                }\n",
        "\n",
        "                # Calculate average path length if graph is connected\n",
        "                if nx.is_connected(G):\n",
        "                    try:\n",
        "                        properties[\"avg_path_length\"] = nx.average_shortest_path_length(G)\n",
        "                    except:\n",
        "                        properties[\"avg_path_length\"] = 0.0\n",
        "                else:\n",
        "                    properties[\"avg_path_length\"] = 0.0\n",
        "\n",
        "                # Add more sophisticated graph properties\n",
        "                try:\n",
        "                    properties[\"clustering_coefficient\"] = nx.average_clustering(G)\n",
        "                except:\n",
        "                    properties[\"clustering_coefficient\"] = 0.0\n",
        "\n",
        "                try:\n",
        "                    properties[\"graph_diameter\"] = nx.diameter(G) if nx.is_connected(G) else 0\n",
        "                except:\n",
        "                    properties[\"graph_diameter\"] = 0\n",
        "\n",
        "                try:\n",
        "                    properties[\"assortativity\"] = nx.degree_assortativity_coefficient(G)\n",
        "                except:\n",
        "                    properties[\"assortativity\"] = 0.0\n",
        "\n",
        "                # Graph features\n",
        "                features = {\n",
        "                    \"is_connected\": nx.is_connected(G),\n",
        "                    \"num_connected_components\": nx.number_connected_components(G),\n",
        "                    \"has_cycles\": not nx.is_tree(G),\n",
        "                    \"max_degree\": max(dict(G.degree()).values()) if G.number_of_nodes() > 0 else 0,\n",
        "                    \"density\": nx.density(G),\n",
        "                    \"is_bipartite\": nx.is_bipartite(G) if G.number_of_nodes() > 0 else False\n",
        "                }\n",
        "\n",
        "                # Get centrality measures\n",
        "                if G.number_of_nodes() > 0:\n",
        "                    try:\n",
        "                        degree_centrality = nx.degree_centrality(G)\n",
        "                        features[\"max_centrality\"] = max(degree_centrality.values()) if degree_centrality else 0\n",
        "                        features[\"avg_centrality\"] = sum(degree_centrality.values()) / len(degree_centrality) if degree_centrality else 0\n",
        "                    except:\n",
        "                        features[\"max_centrality\"] = 0\n",
        "                        features[\"avg_centrality\"] = 0\n",
        "                else:\n",
        "                    features[\"max_centrality\"] = 0\n",
        "                    features[\"avg_centrality\"] = 0\n",
        "\n",
        "                # Analyze node features if available\n",
        "                if hasattr(data, 'x_cat') and hasattr(data, 'x_phys'):\n",
        "                    # Atomic element distribution (from x_cat)\n",
        "                    atom_types = {}\n",
        "                    if data.x_cat.size(1) > 0:\n",
        "                        for i in range(data.num_nodes):\n",
        "                            atom_type = int(data.x_cat[i, 0].item())\n",
        "                            atom_types[atom_type] = atom_types.get(atom_type, 0) + 1\n",
        "\n",
        "                    features[\"atom_type_distribution\"] = atom_types\n",
        "\n",
        "                    # Physical property statistics (from x_phys)\n",
        "                    if data.x_phys.size(1) > 0:\n",
        "                        phys_means = data.x_phys.mean(dim=0).tolist()\n",
        "                        phys_stds = data.x_phys.std(dim=0).tolist()\n",
        "\n",
        "                        # Map indices to meaningful property names for the first few common properties\n",
        "                        phys_prop_names = ['contrib_mw', 'contrib_logp', 'formal_charge',\n",
        "                                        'hybridization', 'is_aromatic', 'num_h', 'valence', 'degree']\n",
        "\n",
        "                        for idx, name in enumerate(phys_prop_names):\n",
        "                            if idx < len(phys_means):\n",
        "                                properties[f\"avg_{name}\"] = phys_means[idx]\n",
        "                                properties[f\"std_{name}\"] = phys_stds[idx]\n",
        "\n",
        "                # Cycle analysis\n",
        "                cycles = list(nx.cycle_basis(G))\n",
        "                cycle_count = len(cycles)\n",
        "                ring_info[\"ring_counts\"][\"total\"] = cycle_count\n",
        "\n",
        "                # Count rings by size\n",
        "                ring_sizes = defaultdict(int)\n",
        "                for cycle in cycles:\n",
        "                    size = len(cycle)\n",
        "                    ring_sizes[str(size)] = ring_sizes.get(str(size), 0) + 1\n",
        "\n",
        "                # Ensure we have entries for common ring sizes\n",
        "                for size in range(3, 11):\n",
        "                    if str(size) not in ring_sizes:\n",
        "                        ring_sizes[str(size)] = 0\n",
        "\n",
        "                ring_info[\"ring_sizes\"] = dict(ring_sizes)\n",
        "\n",
        "                # Estimate ring types\n",
        "                ring_info[\"ring_counts\"][\"single\"] = 0\n",
        "                ring_info[\"ring_counts\"][\"fused\"] = 0\n",
        "\n",
        "                # Identify single vs fused rings by checking for shared nodes\n",
        "                if cycles:\n",
        "                    # Build a mapping of nodes to cycles they belong to\n",
        "                    node_to_cycles = defaultdict(list)\n",
        "                    for cycle_idx, cycle in enumerate(cycles):\n",
        "                        for node in cycle:\n",
        "                            node_to_cycles[node].append(cycle_idx)\n",
        "\n",
        "                    # Count single rings (no shared nodes with other rings)\n",
        "                    shared_cycles = set()\n",
        "                    for node, cycle_list in node_to_cycles.items():\n",
        "                        if len(cycle_list) > 1:\n",
        "                            for c in cycle_list:\n",
        "                                shared_cycles.add(c)\n",
        "\n",
        "                    ring_info[\"ring_counts\"][\"single\"] = cycle_count - len(shared_cycles)\n",
        "                    ring_info[\"ring_counts\"][\"fused\"] = len(shared_cycles)\n",
        "\n",
        "                # Edge feature analysis if available\n",
        "                if hasattr(data, 'edge_attr') and data.edge_attr.size(0) > 0:\n",
        "                    # Analyze bond types (assuming first dimension is bond type)\n",
        "                    bond_types = {}\n",
        "                    for i in range(data.edge_attr.size(0)):\n",
        "                        if data.edge_attr.size(1) > 0:\n",
        "                            bond_type = int(data.edge_attr[i, 0].item())\n",
        "                            bond_types[bond_type] = bond_types.get(bond_type, 0) + 1\n",
        "\n",
        "                    # Divide by 2 since each bond is counted twice in undirected graph\n",
        "                    for bt in bond_types:\n",
        "                        bond_types[bt] = bond_types[bt] // 2\n",
        "\n",
        "                    functional_groups[\"bond_types\"] = bond_types\n",
        "\n",
        "                    # Count functional group proxies based on patterns in the graph\n",
        "                    # This is just an estimate since we don't have chemical information\n",
        "                    conjugated_bonds = 0\n",
        "                    for i in range(data.edge_attr.size(0)):\n",
        "                        if data.edge_attr.size(1) > 1 and data.edge_attr[i, 2].item() > 0:  # IsConjugated flag\n",
        "                            conjugated_bonds += 1\n",
        "\n",
        "                    functional_groups[\"conjugated_bonds\"] = conjugated_bonds // 2\n",
        "\n",
        "            except Exception as e:\n",
        "                # If any error occurs during analysis, use minimal information\n",
        "                print(f\"Error analyzing graph {i}: {e}\")\n",
        "\n",
        "        metadata.append({\n",
        "            \"graph_id\": mol_id,\n",
        "            \"properties\": properties,\n",
        "            \"features\": features,\n",
        "            \"functional_groups\": functional_groups,\n",
        "            \"ring_info\": ring_info\n",
        "        })\n",
        "\n",
        "    return metadata\n",
        "\n",
        "def to_networkx(data):\n",
        "    \"\"\"Convert PyG data to networkx graph for analysis\"\"\"\n",
        "    import networkx as nx\n",
        "\n",
        "    G = nx.Graph()\n",
        "\n",
        "    # Add nodes\n",
        "    for i in range(data.num_nodes):\n",
        "        G.add_node(i)\n",
        "\n",
        "    # Add edges (removing duplicates and self-loops)\n",
        "    edge_index = data.edge_index.cpu().numpy()\n",
        "    edges = set()\n",
        "    for i in range(edge_index.shape[1]):\n",
        "        u, v = edge_index[0, i], edge_index[1, i]\n",
        "        if u != v and (u, v) not in edges and (v, u) not in edges:\n",
        "            G.add_edge(u, v)\n",
        "            edges.add((u, v))\n",
        "\n",
        "    return G"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "8e90813e",
      "metadata": {
        "id": "8e90813e"
      },
      "outputs": [],
      "source": [
        "def save_embedding_file(embeddings, molecule_indices, training_info, model_config, filepath):\n",
        "    \"\"\"Save embeddings with training metadata\"\"\"\n",
        "    data = {\n",
        "        \"embeddings\": embeddings,\n",
        "        \"molecule_indices\": molecule_indices,\n",
        "        \"training_info\": training_info,\n",
        "        \"model_config\": {k: v for k, v in model_config.__dict__.items()\n",
        "                         if not k.startswith('_') and not callable(v)}\n",
        "    }\n",
        "\n",
        "    with open(filepath, 'wb') as f:\n",
        "        pickle.dump(data, f)\n",
        "\n",
        "def save_embeddings_with_molecules(embeddings, dataset, filepath):\n",
        "    \"\"\"Save embeddings with corresponding molecule information and graph-level properties\"\"\"\n",
        "    # Create a list to store molecule data\n",
        "    molecule_data = []\n",
        "\n",
        "    # Extract important info from each molecule in the dataset\n",
        "    for data in dataset:\n",
        "        # Create a dictionary with basic graph properties\n",
        "        mol_info = {\n",
        "            \"num_nodes\": data.num_nodes,\n",
        "            \"edge_index\": data.edge_index.tolist() if hasattr(data, 'edge_index') else None,\n",
        "            \"x_cat\": data.x_cat.tolist() if hasattr(data, 'x_cat') else None,\n",
        "            \"x_phys\": data.x_phys.tolist() if hasattr(data, 'x_phys') else None,\n",
        "            \"edge_attr\": data.edge_attr.tolist() if hasattr(data, 'edge_attr') else None\n",
        "        }\n",
        "\n",
        "        # Calculate additional graph properties if possible\n",
        "        try:\n",
        "            if hasattr(data, 'edge_index') and hasattr(data, 'num_nodes'):\n",
        "                # Graph density\n",
        "                num_edges = len(data.edge_index[0]) // 2  # Undirected edges counted once\n",
        "                max_edges = data.num_nodes * (data.num_nodes - 1) // 2\n",
        "                density = num_edges / max_edges if max_edges > 0 else 0\n",
        "                mol_info[\"graph_density\"] = density\n",
        "\n",
        "                # Average degree\n",
        "                avg_degree = num_edges * 2 / data.num_nodes if data.num_nodes > 0 else 0\n",
        "                mol_info[\"avg_degree\"] = avg_degree\n",
        "\n",
        "                # Count atom types if available\n",
        "                if hasattr(data, 'x_cat') and data.x_cat is not None:\n",
        "                    atom_types = {}\n",
        "                    for atom in data.x_cat:\n",
        "                        atom_type = int(atom[0])\n",
        "                        atom_types[atom_type] = atom_types.get(atom_type, 0) + 1\n",
        "                    mol_info[\"atom_type_counts\"] = atom_types\n",
        "\n",
        "                # Count bond types if available\n",
        "                if hasattr(data, 'edge_attr') and data.edge_attr is not None:\n",
        "                    bond_types = {}\n",
        "                    for bond in data.edge_attr:\n",
        "                        bond_type = int(bond[0])\n",
        "                        bond_types[bond_type] = bond_types.get(bond_type, 0) + 1\n",
        "                    mol_info[\"bond_type_counts\"] = bond_types\n",
        "        except:\n",
        "            # If calculation fails, continue without these properties\n",
        "            pass\n",
        "\n",
        "        molecule_data.append(mol_info)\n",
        "\n",
        "    # Save both embeddings and molecule data\n",
        "    with open(filepath, 'wb') as f:\n",
        "        pickle.dump({\n",
        "            'embeddings': embeddings,\n",
        "            'molecule_data': molecule_data,\n",
        "            'graph_properties': True  # Flag to indicate enhanced properties are stored\n",
        "        }, f)\n",
        "\n",
        "    print(f\"Saved embeddings and molecule data with graph properties to {filepath}\")\n",
        "\n",
        "\n",
        "def save_embeddings(embeddings, labels, filepath):\n",
        "    \"\"\"Save embeddings and corresponding labels\"\"\"\n",
        "    with open(filepath, 'wb') as f:\n",
        "        pickle.dump({\n",
        "            'embeddings': embeddings,\n",
        "            'labels': labels\n",
        "        }, f)\n",
        "\n",
        "def save_encoder(encoder, save_path, info=None):\n",
        "    \"\"\"Save encoder model for downstream tasks\"\"\"\n",
        "    save_dict = {\n",
        "        'encoder_state_dict': encoder.state_dict(),\n",
        "        'model_info': info or {}\n",
        "    }\n",
        "    torch.save(save_dict, save_path)\n",
        "\n",
        "def load_encoder(model_path, device='cpu'):\n",
        "    \"\"\"Load saved encoder model\"\"\"\n",
        "    checkpoint = torch.load(model_path, map_location=device)\n",
        "    encoder = GraphDiscriminator(\n",
        "        node_dim=checkpoint['model_info'].get('node_dim'),\n",
        "        edge_dim=checkpoint['model_info'].get('edge_dim'),\n",
        "        hidden_dim=checkpoint['model_info'].get('hidden_dim', 128),\n",
        "        output_dim=checkpoint['model_info'].get('output_dim', 128)\n",
        "    )\n",
        "    encoder.load_state_dict(checkpoint['encoder_state_dict'])\n",
        "    return encoder\n",
        "\n",
        "def train_gan_cl(train_loader, config, dataset, device='cuda',\n",
        "                save_dir='./checkpoints',\n",
        "                embedding_dir='./embeddings'):\n",
        "    \"\"\"Main training function for GAN-CL with embedding storage for bias analysis\"\"\"\n",
        "\n",
        "    # Create directories\n",
        "    os.makedirs(save_dir, exist_ok=True)\n",
        "    os.makedirs(embedding_dir, exist_ok=True)\n",
        "    metadata_dir = os.path.join(embedding_dir, 'metadata')\n",
        "    os.makedirs(metadata_dir, exist_ok=True)\n",
        "    encoder_dir = os.path.join(save_dir, 'encoders')\n",
        "    os.makedirs(encoder_dir, exist_ok=True)\n",
        "\n",
        "    # Extract and save molecule metadata (once, before training)\n",
        "    timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')\n",
        "    metadata = extract_molecule_metadata(dataset)\n",
        "    with open(os.path.join(metadata_dir, f'molecule_metadata_{timestamp}.pkl'), 'wb') as f:\n",
        "        pickle.dump(metadata, f)\n",
        "\n",
        "    # Save molecule indices for consistent order\n",
        "    molecule_indices = list(range(len(metadata)))\n",
        "\n",
        "    # Initialize model\n",
        "    model = MolecularGANCL(config).to(device)\n",
        "\n",
        "    # Get pre-training embeddings before any training\n",
        "    print(\"Extracting pre-training embeddings...\")\n",
        "    model.eval()\n",
        "    pre_training_embeddings = []\n",
        "    with torch.no_grad():\n",
        "        for batch in tqdm(train_loader, desc=\"Pre-training embeddings\"):\n",
        "            batch = batch.to(device)\n",
        "            embeddings = model.get_embeddings(batch)\n",
        "            pre_training_embeddings.append(embeddings.cpu())\n",
        "\n",
        "    pre_training_embeddings = torch.cat(pre_training_embeddings, dim=0).numpy()\n",
        "\n",
        "    # Save pre-training embeddings\n",
        "    pre_training_info = {\n",
        "        \"stage\": \"pre\",\n",
        "        \"epoch\": 0,\n",
        "        \"timestamp\": timestamp,\n",
        "        \"loss_values\": {\"contrastive\": 0, \"adversarial\": 0, \"similarity\": 0, \"total\": 0}\n",
        "    }\n",
        "\n",
        "    save_embedding_file(\n",
        "        pre_training_embeddings,\n",
        "        molecule_indices,\n",
        "        pre_training_info,\n",
        "        config,\n",
        "        os.path.join(embedding_dir, f'pre_training_embeddings_{timestamp}.pkl')\n",
        "    )\n",
        "\n",
        "    model.train()\n",
        "\n",
        "    # Rest of your training code remains the same...\n",
        "    # Initialize optimizers\n",
        "    optimizer_encoder = torch.optim.Adam(model.encoder.parameters(), lr=3e-4)\n",
        "    optimizer_generator = torch.optim.Adam(model.generator.parameters(), lr=1e-4)\n",
        "\n",
        "    # Save initial model info\n",
        "    model_info = {\n",
        "        'node_dim': config.node_dim,\n",
        "        'edge_dim': config.edge_dim,\n",
        "        'hidden_dim': config.hidden_dim,\n",
        "        'output_dim': config.output_dim,\n",
        "        'training_config': config.__dict__\n",
        "    }\n",
        "\n",
        "    # Training phases as before...\n",
        "    best_loss = float('inf')\n",
        "\n",
        "    # Training metrics\n",
        "    metrics = {\n",
        "        'contrastive_losses': [],\n",
        "        'adversarial_losses': [],\n",
        "        'similarity_losses': [],\n",
        "        'total_losses': []\n",
        "    }\n",
        "\n",
        "    # Training phases\n",
        "    print(\"Phase 1: Pretraining Contrastive Learning...\")\n",
        "    pretrain_epochs = 10\n",
        "    for epoch in range(pretrain_epochs):\n",
        "        contrastive_epoch_loss = 0\n",
        "\n",
        "        for batch in tqdm(train_loader, desc=f'Pretrain Epoch {epoch+1}/{pretrain_epochs}'):\n",
        "            batch = batch.to(device)\n",
        "\n",
        "            # Forward pass (without generator)\n",
        "            query_emb = model.encoder(batch)\n",
        "            with torch.no_grad():\n",
        "                key_emb = model.momentum_encoder(batch)\n",
        "\n",
        "            # Compute contrastive loss\n",
        "            contrastive_loss = model.memory_queue.compute_contrastive_loss(\n",
        "                query_emb, key_emb, model.config.temperature\n",
        "            )\n",
        "\n",
        "            # Update encoder\n",
        "            optimizer_encoder.zero_grad()\n",
        "            contrastive_loss.backward()\n",
        "            optimizer_encoder.step()\n",
        "\n",
        "            # Update momentum encoder\n",
        "            model._momentum_update()\n",
        "\n",
        "            # Update memory queue\n",
        "            model.memory_queue.update_queue(key_emb.detach())\n",
        "\n",
        "            contrastive_epoch_loss += contrastive_loss.item()\n",
        "\n",
        "        avg_loss = contrastive_epoch_loss / len(train_loader)\n",
        "        metrics['contrastive_losses'].append(avg_loss)\n",
        "        print(f'Pretrain Epoch {epoch+1}, Avg Loss: {avg_loss:.4f}')\n",
        "\n",
        "        # Save pretrained checkpoint\n",
        "        if (epoch + 1) % 5 == 0:\n",
        "            torch.save({\n",
        "                'epoch': epoch,\n",
        "                'model_state_dict': model.state_dict(),\n",
        "                'optimizer_encoder_state_dict': optimizer_encoder.state_dict(),\n",
        "                'loss': avg_loss,\n",
        "            }, os.path.join(save_dir, f'pretrain_checkpoint_{epoch+1}.pt'))\n",
        "\n",
        "    print(\"\\nPhase 2: Training GAN-CL...\")\n",
        "    train_epochs = 50\n",
        "#     train_epochs = 10\n",
        "    for epoch in range(train_epochs):\n",
        "        epoch_losses = {\n",
        "            'contrastive': 0,\n",
        "            'adversarial': 0,\n",
        "            'similarity': 0,\n",
        "            'total': 0\n",
        "        }\n",
        "\n",
        "        for batch in tqdm(train_loader, desc=f'Train Epoch {epoch+1}/{train_epochs}'):\n",
        "            batch = batch.to(device)\n",
        "\n",
        "            # Step 1: Train Encoder\n",
        "            optimizer_encoder.zero_grad()\n",
        "\n",
        "            # Get importance scores from generator\n",
        "            with torch.no_grad():\n",
        "                node_scores, edge_scores = model.generator(batch)\n",
        "\n",
        "            # Create perturbed graph\n",
        "            perturbed_data = model.drop_graph_elements(batch, node_scores, edge_scores)\n",
        "\n",
        "            # Get embeddings\n",
        "            query_emb = model.encoder(perturbed_data)\n",
        "            with torch.no_grad():\n",
        "                key_emb = model.momentum_encoder(batch)\n",
        "                original_emb = model.encoder(batch).detach()\n",
        "\n",
        "            # Compute losses for encoder\n",
        "            contrastive_loss = model.memory_queue.compute_contrastive_loss(\n",
        "                query_emb, key_emb, model.config.temperature\n",
        "            )\n",
        "            similarity_loss = F.mse_loss(query_emb, original_emb)\n",
        "\n",
        "            # Total loss for encoder\n",
        "            encoder_loss = contrastive_loss + 0.1 * similarity_loss\n",
        "\n",
        "            # Update encoder\n",
        "            encoder_loss.backward()\n",
        "            optimizer_encoder.step()\n",
        "\n",
        "            # Update momentum encoder\n",
        "            model._momentum_update()\n",
        "\n",
        "            # Step 2: Train Generator\n",
        "            optimizer_generator.zero_grad()\n",
        "\n",
        "            # Get new embeddings for adversarial loss\n",
        "            node_scores, edge_scores = model.generator(batch)\n",
        "            perturbed_data = model.drop_graph_elements(batch, node_scores, edge_scores)\n",
        "\n",
        "            with torch.no_grad():\n",
        "                original_emb = model.encoder(batch)\n",
        "            perturbed_emb = model.encoder(perturbed_data)\n",
        "\n",
        "            # Compute adversarial loss\n",
        "            adversarial_loss = -F.mse_loss(perturbed_emb, original_emb)\n",
        "\n",
        "            # Update generator\n",
        "            adversarial_loss.backward()\n",
        "            optimizer_generator.step()\n",
        "\n",
        "            # Update memory queue\n",
        "            model.memory_queue.update_queue(key_emb.detach())\n",
        "\n",
        "            # Update metrics\n",
        "            epoch_losses['contrastive'] += contrastive_loss.item()\n",
        "            epoch_losses['adversarial'] += adversarial_loss.item()\n",
        "            epoch_losses['similarity'] += similarity_loss.item()\n",
        "            epoch_losses['total'] += encoder_loss.item()\n",
        "\n",
        "        # Average losses\n",
        "        for k in epoch_losses:\n",
        "            epoch_losses[k] /= len(train_loader)\n",
        "            metrics[f'{k}_losses'].append(epoch_losses[k])\n",
        "\n",
        "        print(f'Epoch {epoch+1}, Losses: {epoch_losses}')\n",
        "\n",
        "        # Save checkpoint\n",
        "        if (epoch + 1) % 10 == 0:\n",
        "            torch.save({\n",
        "                'epoch': epoch,\n",
        "                'model_state_dict': model.state_dict(),\n",
        "                'optimizer_encoder_state_dict': optimizer_encoder.state_dict(),\n",
        "                'optimizer_generator_state_dict': optimizer_generator.state_dict(),\n",
        "                'losses': epoch_losses,\n",
        "            }, os.path.join(save_dir, f'gan_cl_checkpoint_{epoch+1}.pt'))\n",
        "\n",
        "    # Extract and save embeddings periodically\n",
        "    if (epoch + 1) % 10 == 0:\n",
        "        model.eval()\n",
        "        checkpoint_embeddings = []\n",
        "\n",
        "        with torch.no_grad():\n",
        "            for batch in train_loader:\n",
        "                batch = batch.to(device)\n",
        "                embeddings = model.get_embeddings(batch)\n",
        "                checkpoint_embeddings.append(embeddings.cpu())\n",
        "\n",
        "        checkpoint_embeddings = torch.cat(checkpoint_embeddings, dim=0).numpy()\n",
        "\n",
        "        # Save checkpoint embeddings with training info\n",
        "        checkpoint_info = {\n",
        "            \"stage\": f\"epoch_{epoch+1}\",\n",
        "            \"epoch\": epoch + 1,\n",
        "            \"timestamp\": datetime.now().strftime('%Y%m%d_%H%M%S'),\n",
        "            \"loss_values\": epoch_losses\n",
        "        }\n",
        "\n",
        "        save_embedding_file(\n",
        "            checkpoint_embeddings,\n",
        "            molecule_indices,\n",
        "            checkpoint_info,\n",
        "            config,\n",
        "            os.path.join(embedding_dir, f'epoch_{epoch+1}_embeddings_{timestamp}.pkl')\n",
        "        )\n",
        "\n",
        "        model.train()\n",
        "\n",
        "        model.train()\n",
        "\n",
        "    # Extract and save post-training embeddings at the end\n",
        "    print(\"Extracting post-training embeddings...\")\n",
        "    model.eval()\n",
        "    post_training_embeddings = []\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for batch in tqdm(train_loader, desc=\"Post-training embeddings\"):\n",
        "            batch = batch.to(device)\n",
        "            embeddings = model.get_embeddings(batch)\n",
        "            post_training_embeddings.append(embeddings.cpu())\n",
        "\n",
        "    post_training_embeddings = torch.cat(post_training_embeddings, dim=0).numpy()\n",
        "\n",
        "    # Save post-training embeddings\n",
        "    post_training_info = {\n",
        "        \"stage\": \"post\",\n",
        "        \"epoch\": train_epochs,\n",
        "        \"timestamp\": datetime.now().strftime('%Y%m%d_%H%M%S'),\n",
        "        \"loss_values\": epoch_losses\n",
        "    }\n",
        "\n",
        "    save_embedding_file(\n",
        "        post_training_embeddings,\n",
        "        molecule_indices,\n",
        "        post_training_info,\n",
        "        config,\n",
        "        os.path.join(embedding_dir, f'post_training_embeddings_{timestamp}.pkl')\n",
        "    )\n",
        "\n",
        "    return model, metrics"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "4dbc0517",
      "metadata": {
        "scrolled": false,
        "id": "4dbc0517"
      },
      "outputs": [],
      "source": [
        "def main():\n",
        "     # Enable anomaly detection during development\n",
        "    torch.autograd.set_detect_anomaly(True)\n",
        "    # Your existing data loading code here\n",
        "    torch.manual_seed(42)\n",
        "    np.random.seed(42)\n",
        "\n",
        "    print(\"Starting data loading...\")\n",
        "    extractor = MolecularFeatureExtractor()\n",
        "#     smiles_file = \"D:\\\\PhD\\\\Chapter3\\\\Unsupervised_GAN_Code\\\\pubchem-41-clean.txt\"\n",
        "    smiles_file = \"D:\\\\PhD\\\\Chapter3\\\\Unsupervised_GAN_Code\\\\pubchem-10m-clean_test10k.txt\"\n",
        "#     smiles_file = \"D:\\\\PhD\\\\Chapter3\\\\Unsupervised_GAN_Code\\\\pubchem-10m-clean_test50k.txt\"\n",
        "\n",
        "    dataset = []\n",
        "    failed_smiles = []\n",
        "\n",
        "    with open(smiles_file, 'r') as f:\n",
        "        for line in f:\n",
        "            smiles = line.strip()\n",
        "            data = extractor.process_molecule(smiles)\n",
        "            if data is not None:\n",
        "                dataset.append(data)\n",
        "            else:\n",
        "                failed_smiles.append(smiles)\n",
        "\n",
        "    print(f\"1. Loaded dataset with {len(dataset)} graphs.\")\n",
        "    print(f\"2. Failed SMILES count: {len(failed_smiles)}\")\n",
        "\n",
        "    if not dataset:\n",
        "        print(\"No valid graphs generated.\")\n",
        "        return None\n",
        "\n",
        "    # Setup training\n",
        "    batch_size = 32\n",
        "    train_loader = DataLoader(dataset, batch_size=batch_size, shuffle=True)\n",
        "    print(f\"3. Created DataLoader with {len(train_loader.dataset)} graphs\")\n",
        "\n",
        "    # Set device\n",
        "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "    print(f\"4. Using device: {device}\")\n",
        "\n",
        "    # Get configuration based on dataset\n",
        "    config = get_model_config(dataset)\n",
        "\n",
        "    # Train model\n",
        "    print(\"5. Starting GAN-CL training...\")\n",
        "    model, metrics = train_gan_cl(\n",
        "        train_loader,\n",
        "        config,\n",
        "        dataset,  # Pass the dataset for metadata extraction\n",
        "        device=device,\n",
        "        save_dir='./checkpoints',\n",
        "        embedding_dir='./embeddings'\n",
        "    )\n",
        "\n",
        "\n",
        "    print(\"6. Training completed!\")\n",
        "\n",
        "    # Extract embeddings for XAI\n",
        "    print(\"7. Extracting final embeddings for XAI...\")\n",
        "    model.eval()\n",
        "    with torch.no_grad():\n",
        "        all_embeddings = []\n",
        "        all_graphs = []\n",
        "\n",
        "        for batch in tqdm(train_loader, desc=\"Extracting embeddings\"):\n",
        "            batch = batch.to(device)\n",
        "            embeddings = model.get_embeddings(batch)\n",
        "            all_embeddings.append(embeddings.cpu())\n",
        "            all_graphs.extend([data for data in batch])\n",
        "\n",
        "    all_embeddings = torch.cat(all_embeddings, dim=0).numpy()\n",
        "\n",
        "    # Save final embeddings and graphs\n",
        "#     final_embedding_path = './embeddings/final_embeddings.pkl'\n",
        "#     final_embedding_path = f'./embeddings/final_embeddings_{timestamp}.pkl'\n",
        "#     save_embeddings(all_embeddings, all_graphs, final_embedding_path)\n",
        "#     print(f\"8. Final embeddings saved to {final_embedding_path}\")\n",
        "\n",
        "    # Update your final embedding saving code\n",
        "    final_embedding_path = f'./embeddings/final_embeddings_molecules_{timestamp}.pkl'\n",
        "    save_embeddings_with_molecules(all_embeddings, dataset, final_embedding_path)\n",
        "    print(f\"8. Final embeddings saved to {final_embedding_path}\")\n",
        "\n",
        "\n",
        "    # Print encoder locations\n",
        "    print(f\"9. Encoders saved in ./checkpoints/encoders/:\")\n",
        "    print(f\"   - Best encoder: best_encoder.pt\")\n",
        "    print(f\"   - Final encoder: final_encoder.pt\")\n",
        "    print(f\"   - Periodic encoders: encoder_epoch_*.pt\")\n",
        "\n",
        "    return model, metrics, all_embeddings, all_graphs\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    model, metrics, embeddings, graphs = main()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "27d928ef",
      "metadata": {
        "id": "27d928ef"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.21"
    },
    "colab": {
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}