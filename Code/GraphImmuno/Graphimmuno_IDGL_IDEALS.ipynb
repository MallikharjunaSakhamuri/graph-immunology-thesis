{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "9dd61ca1",
      "metadata": {
        "id": "9dd61ca1"
      },
      "outputs": [],
      "source": [
        "!pip install chardet"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "a142b1f7",
      "metadata": {
        "id": "a142b1f7"
      },
      "outputs": [],
      "source": [
        "!pip install seaborn"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "8d9abc96",
      "metadata": {
        "id": "8d9abc96"
      },
      "outputs": [],
      "source": [
        "!!pip install torch-geometric"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ae266532",
      "metadata": {
        "id": "ae266532"
      },
      "outputs": [],
      "source": [
        "!pip install torch"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "1afed8c0",
      "metadata": {
        "id": "1afed8c0"
      },
      "outputs": [],
      "source": [
        "!pip install networkx"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "c0d7c122",
      "metadata": {
        "id": "c0d7c122"
      },
      "outputs": [],
      "source": [
        "!pip install tensorflow"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "bbdb1ba4",
      "metadata": {
        "id": "bbdb1ba4"
      },
      "outputs": [],
      "source": [
        "!pip install stellargraph"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "e84687ac",
      "metadata": {
        "id": "e84687ac"
      },
      "outputs": [],
      "source": [
        "# Core libraries\n",
        "import os\n",
        "import time\n",
        "import json\n",
        "import functools\n",
        "import itertools\n",
        "import inspect\n",
        "import traceback\n",
        "\n",
        "# Numerical and Data Handling\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "\n",
        "# Visualization\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import matplotlib.gridspec as gridspec\n",
        "from matplotlib.colors import LinearSegmentedColormap\n",
        "import networkx as nx\n",
        "\n",
        "# Progress and System Utilities\n",
        "from tqdm import tqdm\n",
        "import psutil\n",
        "\n",
        "# Scikit-learn\n",
        "from sklearn import model_selection\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import (\n",
        "    confusion_matrix,\n",
        "    auc,\n",
        "    precision_recall_curve,\n",
        "    roc_curve,\n",
        "    accuracy_score,\n",
        "    f1_score,\n",
        "    matthews_corrcoef,\n",
        "    mean_squared_error,\n",
        "    recall_score,\n",
        "    roc_auc_score,\n",
        ")\n",
        "\n",
        "# TensorFlow / Keras\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras import Model\n",
        "from tensorflow.keras.layers import Dense, Dropout\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "from tensorflow.keras.losses import binary_crossentropy\n",
        "from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint, ReduceLROnPlateau\n",
        "\n",
        "# StellarGraph\n",
        "import stellargraph as sg\n",
        "from stellargraph import StellarGraph, IndexedArray\n",
        "from stellargraph.mapper import PaddedGraphGenerator\n",
        "from stellargraph.layer import GCNSupervisedGraphClassification, DeepGraphCNN\n",
        "\n",
        "# PyTorch / PyG\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from torch_geometric.data import Data, Batch\n",
        "from torch_geometric.loader import DataLoader\n",
        "from torch_geometric.nn import GCNConv, global_mean_pool, BatchNorm\n",
        "\n",
        "\n",
        "def hla_df_to_dic(hla):\n",
        "    \"\"\"Convert HLA dataframe to dictionary mapping HLA alleles to pseudo-sequences\"\"\"\n",
        "    dic = {}\n",
        "    for i in range(hla.shape[0]):\n",
        "        col1 = hla['HLA'].iloc[i]  # HLA allele\n",
        "        col2 = hla['pseudo'].iloc[i]  # pseudo sequence\n",
        "        dic[col1] = col2\n",
        "    return dic\n",
        "\n",
        "\n",
        "def dict_inventory(inventory):\n",
        "    \"\"\"Create inventory of HLA alleles grouped by type and first two digits\"\"\"\n",
        "    dicA, dicB, dicC = {}, {}, {}\n",
        "    dic = {'A': dicA, 'B': dicB, 'C': dicC}\n",
        "\n",
        "    for hla in inventory:\n",
        "        type_ = hla[4]  # A, B, C\n",
        "        first2 = hla[6:8]  # 01\n",
        "        last2 = hla[8:]  # 01\n",
        "        try:\n",
        "            dic[type_][first2].append(last2)\n",
        "        except KeyError:\n",
        "            dic[type_][first2] = []\n",
        "            dic[type_][first2].append(last2)\n",
        "\n",
        "    return dic\n",
        "\n",
        "\n",
        "def rescue_unknown_hla(hla, dic_inventory):\n",
        "    \"\"\"Find the closest HLA allele in the inventory for unknown HLAs\"\"\"\n",
        "    type_ = hla[4]\n",
        "    first2 = hla[6:8]\n",
        "    last2 = hla[8:]\n",
        "    big_category = dic_inventory[type_]\n",
        "\n",
        "    if first2 in big_category:\n",
        "        small_category = big_category[first2]\n",
        "        distance = [abs(int(last2) - int(i)) for i in small_category]\n",
        "        optimal = min(zip(small_category, distance), key=lambda x: x[1])[0]\n",
        "        return 'HLA-' + str(type_) + '*' + str(first2) + str(optimal)\n",
        "    else:\n",
        "        small_category = list(big_category.keys())\n",
        "        distance = [abs(int(first2) - int(i)) for i in small_category]\n",
        "        optimal = min(zip(small_category, distance), key=lambda x: x[1])[0]\n",
        "        return 'HLA-' + str(type_) + '*' + str(optimal) + str(big_category[optimal][0])\n",
        "\n",
        "\n",
        "class Graph_Constructor:\n",
        "    \"\"\"Construct graph representations of peptide-HLA interactions\"\"\"\n",
        "\n",
        "    @staticmethod\n",
        "    def combinator(pep, hla):\n",
        "        \"\"\"Create source and target node names for peptide and HLA sequences\"\"\"\n",
        "        source = ['p' + str(i+1) for i in range(len(pep))]\n",
        "        target = ['h' + str(i+1) for i in range(len(hla))]\n",
        "        return source, target\n",
        "\n",
        "    @staticmethod\n",
        "    def numerical(pep, hla, after_pca, embed=12):\n",
        "        \"\"\"Convert amino acid sequences to numerical features using PCA matrix\"\"\"\n",
        "        pep = pep.replace('X','-').upper()\n",
        "        hla = hla.replace('X','-').upper()\n",
        "        feature_array_pep = np.empty([len(pep), embed])\n",
        "        feature_array_hla = np.empty([len(hla), embed])\n",
        "        amino = 'ARNDCQEGHILKMFPSTWYV-'\n",
        "\n",
        "        for i in range(len(pep)):\n",
        "            feature_array_pep[i,:] = after_pca[amino.index(pep[i]),:]\n",
        "        for i in range(len(hla)):\n",
        "            feature_array_hla[i,:] = after_pca[amino.index(hla[i]),:]\n",
        "\n",
        "        feature_array = np.concatenate([feature_array_pep, feature_array_hla], axis=0)\n",
        "        return feature_array\n",
        "\n",
        "    @staticmethod\n",
        "    def unweight_edge(pep, hla, after_pca):\n",
        "        \"\"\"Create unweighted edges between peptide and HLA nodes\"\"\"\n",
        "        source, target = Graph_Constructor.combinator(pep, hla)\n",
        "        combine = list(itertools.product(source, target))\n",
        "        weight = itertools.repeat(1, len(source) * len(target))\n",
        "\n",
        "        edges = pd.DataFrame({\n",
        "            'source': [item[0] for item in combine],\n",
        "            'target': [item[1] for item in combine],\n",
        "            'weight': weight\n",
        "        })\n",
        "\n",
        "        feature_array = Graph_Constructor.numerical(pep, hla, after_pca)\n",
        "        try:\n",
        "            nodes = IndexedArray(feature_array, index=source+target)\n",
        "        except:\n",
        "            print(pep, hla, feature_array.shape)\n",
        "\n",
        "        graph = StellarGraph(nodes, edges, node_type_default='corner', edge_type_default='line')\n",
        "        return graph\n",
        "\n",
        "    @staticmethod\n",
        "    def weight_anchor_edge(pep, hla, after_pca):\n",
        "        \"\"\"Create edges with weights for specific anchoring positions\"\"\"\n",
        "        source, target = Graph_Constructor.combinator(pep, hla)\n",
        "        combine = list(itertools.product(source, target))\n",
        "        weight = itertools.repeat(1, len(source) * len(target))\n",
        "\n",
        "        edges = pd.DataFrame({\n",
        "            'source': [item[0] for item in combine],\n",
        "            'target': [item[1] for item in combine],\n",
        "            'weight': weight\n",
        "        })\n",
        "\n",
        "        # Add higher weights to anchor positions\n",
        "        for i in range(edges.shape[0]):\n",
        "            col1 = edges.iloc[i]['source']\n",
        "            col2 = edges.iloc[i]['target']\n",
        "            col3 = edges.iloc[i]['weight']\n",
        "            if col1 == 'a2' or col1 == 'a9' or col1 ==  'a10':\n",
        "                edges.iloc[i]['weight'] = 1.5\n",
        "\n",
        "        feature_array = Graph_Constructor.numerical(pep, hla, after_pca)\n",
        "        nodes = IndexedArray(feature_array, index=source+target)\n",
        "        graph = StellarGraph(nodes, edges, node_type_default='corner', edge_type_default='line')\n",
        "\n",
        "        return graph\n",
        "\n",
        "    @staticmethod\n",
        "    def intra_and_inter(pep, hla, after_pca):\n",
        "        \"\"\"Create both inter-sequence and intra-sequence edges\"\"\"\n",
        "        source, target = Graph_Constructor.combinator(pep, hla)\n",
        "\n",
        "        # Inter-sequence edges (between peptide and HLA)\n",
        "        combine = list(itertools.product(source, target))\n",
        "        weight = itertools.repeat(2, len(source) * len(target))\n",
        "        edges_inter = pd.DataFrame({\n",
        "            'source': [item[0] for item in combine],\n",
        "            'target': [item[1] for item in combine],\n",
        "            'weight': weight\n",
        "        })\n",
        "\n",
        "        # Intra-sequence edges (within peptide or HLA)\n",
        "        intra_pep = list(itertools.combinations(source, 2))\n",
        "        intra_hla = list(itertools.combinations(target, 2))\n",
        "        intra = intra_pep + intra_hla\n",
        "        weight = itertools.repeat(1, len(intra))\n",
        "\n",
        "        edges_intra = pd.DataFrame({\n",
        "            'source': [item[0] for item in intra],\n",
        "            'target': [item[1] for item in intra],\n",
        "            'weight': weight\n",
        "        })\n",
        "\n",
        "        # Combine all edges\n",
        "        edges = pd.concat([edges_inter, edges_intra])\n",
        "        edges = edges.set_index(pd.Index(np.arange(edges.shape[0])))\n",
        "\n",
        "        feature_array = Graph_Constructor.numerical(pep, hla, after_pca)\n",
        "        nodes = IndexedArray(feature_array, index=source+target)\n",
        "        graph = StellarGraph(nodes, edges, node_type_default='corner', edge_type_default='line')\n",
        "\n",
        "        return graph\n",
        "\n",
        "    @staticmethod\n",
        "    def entrance(df, after_pca, hla_dic, dic_inventory, graph_type='intra_and_inter'):\n",
        "        \"\"\"Process a dataset of peptide-HLA pairs to create graphs\"\"\"\n",
        "        graphs = []\n",
        "        graph_labels = []\n",
        "\n",
        "        # Ensure labels are numeric\n",
        "        if df['immunogenicity'].dtype == 'object':\n",
        "            # Convert string labels to numeric\n",
        "            if isinstance(df['immunogenicity'].iloc[0], str):\n",
        "                label_map = {'Positive': 1, 'Negative': 0}\n",
        "                if 'immunogenicity' in df.columns:\n",
        "                    df['immunogenicity'] = df['immunogenicity'].map(\n",
        "                        lambda x: label_map.get(x, 1 if x != 'Negative' else 0)\n",
        "                    )\n",
        "\n",
        "        for i in range(df.shape[0]):\n",
        "            if i % 100 == 0:\n",
        "                print(f\"Processing sample {i}/{df.shape[0]}\")\n",
        "\n",
        "            pep = df['peptide'].iloc[i]\n",
        "            try:\n",
        "                hla = hla_dic[df['HLA'].iloc[i]]\n",
        "            except KeyError:\n",
        "                hla = hla_dic[rescue_unknown_hla(df['HLA'].iloc[i], dic_inventory)]\n",
        "\n",
        "            label = float(df['immunogenicity'].iloc[i])  # Ensure label is a float\n",
        "\n",
        "            # Create graph based on specified type\n",
        "            if graph_type == 'unweight_edge':\n",
        "                graph = Graph_Constructor.unweight_edge(pep, hla, after_pca)\n",
        "            elif graph_type == 'weight_anchor_edge':\n",
        "                graph = Graph_Constructor.weight_anchor_edge(pep, hla, after_pca)\n",
        "            else:  # Default to intra_and_inter\n",
        "                graph = Graph_Constructor.intra_and_inter(pep, hla, after_pca)\n",
        "\n",
        "            graphs.append(graph)\n",
        "            graph_labels.append(label)\n",
        "\n",
        "        graph_labels = pd.Series(graph_labels)\n",
        "        return graphs, graph_labels\n",
        "\n",
        "def plot_separate_training_history(history, save_path=None):\n",
        "    \"\"\"\n",
        "    Plot training history metrics as separate plots for paper inclusion\n",
        "\n",
        "    Args:\n",
        "        history: Keras history object\n",
        "        save_path: Path to save the plots (optional)\n",
        "    \"\"\"\n",
        "    history_df = pd.DataFrame(history.history)\n",
        "\n",
        "    # Apply smoothing for visualization\n",
        "    def smooth_curve(points, factor=0.8):\n",
        "        smoothed_points = []\n",
        "        for point in points:\n",
        "            if smoothed_points:\n",
        "                prev = smoothed_points[-1]\n",
        "                smoothed_points.append(prev * factor + point * (1 - factor))\n",
        "            else:\n",
        "                smoothed_points.append(point)\n",
        "        return smoothed_points\n",
        "\n",
        "    # Determine accuracy key\n",
        "    if 'acc' in history_df.columns:\n",
        "        acc_key = 'acc'\n",
        "        val_acc_key = 'val_acc'\n",
        "    elif 'accuracy' in history_df.columns:\n",
        "        acc_key = 'accuracy'\n",
        "        val_acc_key = 'val_accuracy'\n",
        "    else:\n",
        "        acc_key = None\n",
        "\n",
        "    # Plot 1: Training and Validation Loss\n",
        "    plt.figure(figsize=(8, 6))\n",
        "    plt.plot(history_df['loss'], label='Training loss', linewidth=2)\n",
        "    plt.plot(smooth_curve(history_df['loss'].values),\n",
        "            linestyle='--', alpha=0.7, color='blue', label='Smoothed training loss')\n",
        "\n",
        "    if 'val_loss' in history_df.columns:\n",
        "        plt.plot(history_df['val_loss'], label='Validation loss', linewidth=2)\n",
        "        plt.plot(smooth_curve(history_df['val_loss'].values),\n",
        "                linestyle='--', alpha=0.7, color='orange', label='Smoothed validation loss')\n",
        "\n",
        "    plt.title('Training and Validation Loss', fontsize=14)\n",
        "    plt.xlabel('Epoch', fontsize=12)\n",
        "    plt.ylabel('Loss', fontsize=12)\n",
        "    plt.legend(fontsize=10)\n",
        "    plt.grid(True, alpha=0.3)\n",
        "    if save_path:\n",
        "        plt.savefig(f\"{save_path}_loss.png\", dpi=300, bbox_inches='tight')\n",
        "    plt.show()\n",
        "\n",
        "    # Plot 2: Training and Validation Accuracy\n",
        "    if acc_key:\n",
        "        plt.figure(figsize=(8, 6))\n",
        "        plt.plot(history_df[acc_key], label=f'Training accuracy', linewidth=2)\n",
        "\n",
        "        if val_acc_key in history_df.columns:\n",
        "            plt.plot(history_df[val_acc_key], label=f'Validation accuracy', linewidth=2)\n",
        "\n",
        "        plt.title('Training and Validation Accuracy', fontsize=14)\n",
        "        plt.xlabel('Epoch', fontsize=12)\n",
        "        plt.ylabel('Accuracy', fontsize=12)\n",
        "        plt.legend(fontsize=10)\n",
        "        plt.grid(True, alpha=0.3)\n",
        "        if save_path:\n",
        "            plt.savefig(f\"{save_path}_accuracy.png\", dpi=300, bbox_inches='tight')\n",
        "        plt.show()\n",
        "\n",
        "    # Plot 3: Loss vs Accuracy\n",
        "    if acc_key:\n",
        "        plt.figure(figsize=(8, 6))\n",
        "        plt.scatter(history_df['loss'], history_df[acc_key], alpha=0.7, label='Training', s=70)\n",
        "\n",
        "        if 'val_loss' in history_df.columns and val_acc_key in history_df.columns:\n",
        "            plt.scatter(history_df['val_loss'], history_df[val_acc_key], alpha=0.7, label='Validation', s=70)\n",
        "\n",
        "        plt.title('Loss vs Accuracy', fontsize=14)\n",
        "        plt.xlabel('Loss', fontsize=12)\n",
        "        plt.ylabel('Accuracy', fontsize=12)\n",
        "        plt.legend(fontsize=10)\n",
        "        plt.grid(True, alpha=0.3)\n",
        "        if save_path:\n",
        "            plt.savefig(f\"{save_path}_loss_vs_accuracy.png\", dpi=300, bbox_inches='tight')\n",
        "        plt.show()\n",
        "\n",
        "    # Plot 4: Learning Rate Improvement\n",
        "    if len(history_df) > 5:  # Need at least 5 epochs for rolling average\n",
        "        plt.figure(figsize=(8, 6))\n",
        "        plt.plot(history_df.index, history_df['loss'].pct_change().rolling(5).mean(),\n",
        "                label='Loss improvement rate', linewidth=2)\n",
        "\n",
        "        if acc_key in history_df.columns:\n",
        "            plt.plot(history_df.index[1:], history_df[acc_key].pct_change().rolling(5).mean()[1:],\n",
        "                    label=f'Accuracy improvement rate', linewidth=2)\n",
        "\n",
        "        plt.title('Learning Rate Improvement', fontsize=14)\n",
        "        plt.xlabel('Epoch', fontsize=12)\n",
        "        plt.ylabel('Improvement Rate', fontsize=12)\n",
        "        plt.legend(fontsize=10)\n",
        "        plt.grid(True, alpha=0.3)\n",
        "        if save_path:\n",
        "            plt.savefig(f\"{save_path}_improvement_rate.png\", dpi=300, bbox_inches='tight')\n",
        "        plt.show()\n",
        "\n",
        "\n",
        "def plot_confusion_matrix(y_true, y_pred, title='Confusion Matrix', save_path=None):\n",
        "    \"\"\"\n",
        "    Plot a proper confusion matrix with labels\n",
        "\n",
        "    Args:\n",
        "        y_true: True labels\n",
        "        y_pred: Predicted labels\n",
        "        title: Title for the plot\n",
        "        save_path: Path to save the plot (optional)\n",
        "    \"\"\"\n",
        "    cm = confusion_matrix(y_true, y_pred)\n",
        "\n",
        "    plt.figure(figsize=(8, 6))\n",
        "    sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', cbar=True)\n",
        "\n",
        "    plt.title(title, fontsize=14)\n",
        "    plt.ylabel('True Label', fontsize=12)\n",
        "    plt.xlabel('Predicted Label', fontsize=12)\n",
        "\n",
        "#     # Add text annotations for TN, FP, FN, TP\n",
        "#     categories = ['TN', 'FP', 'FN', 'TP']\n",
        "#     categories_indices = [(0,0), (0,1), (1,0), (1,1)]\n",
        "\n",
        "#     for i, (category, idx) in enumerate(zip(categories, categories_indices)):\n",
        "#         plt.text(idx[1] + 0.5, idx[0] + 0.5, category,\n",
        "#                 horizontalalignment='center', verticalalignment='center')\n",
        "\n",
        "    if save_path:\n",
        "        plt.savefig(f\"{save_path}_confusion_matrix.png\", dpi=300, bbox_inches='tight')\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "\n",
        "    # Calculate metrics\n",
        "    tn, fp, fn, tp = cm.ravel()\n",
        "    accuracy = (tp + tn) / (tp + tn + fp + fn)\n",
        "    precision = tp / (tp + fp) if (tp + fp) > 0 else 0\n",
        "    recall = tp / (tp + fn) if (tp + fn) > 0 else 0\n",
        "    f1 = 2 * precision * recall / (precision + recall) if (precision + recall) > 0 else 0\n",
        "\n",
        "    print(f\"Accuracy: {accuracy:.4f}\")\n",
        "    print(f\"Precision: {precision:.4f}\")\n",
        "    print(f\"Recall: {recall:.4f}\")\n",
        "    print(f\"F1 Score: {f1:.4f}\")\n",
        "\n",
        "\n",
        "def validate_on_test_datasets(model, after_pca, hla_dic, dic_inventory):\n",
        "    \"\"\"Validate the model on multiple test datasets\"\"\"\n",
        "\n",
        "    def model_predict(model, graphs_list, labels):\n",
        "        \"\"\"Convert StellarGraph objects to PyTorch Geometric and run prediction\"\"\"\n",
        "\n",
        "        # Check if it's a PyTorch model or TensorFlow/Keras model\n",
        "        is_pytorch = hasattr(model, 'eval') and callable(getattr(model, 'eval'))\n",
        "\n",
        "        if is_pytorch:\n",
        "            # PyTorch model logic (convert to PyTorch Geometric)\n",
        "            torch_graphs = []\n",
        "            for i, (g, lbl) in enumerate(zip(graphs_list, labels)):\n",
        "                try:\n",
        "                    data = stellargraph_to_torch_data(g, lbl)\n",
        "                    torch_graphs.append(data)\n",
        "                except Exception as e:\n",
        "                    print(f\"Error converting graph {i}: {e}\")\n",
        "                    continue\n",
        "\n",
        "            # Create dataloader\n",
        "            data_loader = DataLoader(\n",
        "                torch_graphs,\n",
        "                batch_size=16,\n",
        "                shuffle=False,\n",
        "                collate_fn=lambda data_list: Batch.from_data_list(data_list)\n",
        "            )\n",
        "\n",
        "            # Make predictions\n",
        "            model.eval()\n",
        "            all_preds = []\n",
        "            with torch.no_grad():\n",
        "                for data in data_loader:\n",
        "                    data = data.to(next(model.parameters()).device)\n",
        "                    # Forward pass\n",
        "                    outputs, _, _ = model(data.x, data.edge_index, data.edge_attr, data.batch)\n",
        "                    # Get probabilities\n",
        "                    probs = F.softmax(outputs, dim=1)[:, 1].cpu().numpy()\n",
        "                    all_preds.extend(probs)\n",
        "\n",
        "            return np.array(all_preds)\n",
        "        else:\n",
        "            # TensorFlow/Keras model logic (use generator.flow)\n",
        "            generator = PaddedGraphGenerator(graphs=graphs_list)\n",
        "            data_gen = generator.flow(\n",
        "                np.arange(len(graphs_list)),\n",
        "                targets=labels.values.astype(np.float32)\n",
        "            )\n",
        "            return model.predict(data_gen).flatten()\n",
        "\n",
        "        # Create dataloader\n",
        "        data_loader = DataLoader(\n",
        "            torch_graphs,\n",
        "            batch_size=16,\n",
        "            shuffle=False,\n",
        "            collate_fn=lambda data_list: Batch.from_data_list(data_list)\n",
        "        )\n",
        "\n",
        "        # Make predictions\n",
        "        model.eval()\n",
        "        all_preds = []\n",
        "        with torch.no_grad():\n",
        "            for data in data_loader:\n",
        "                data = data.to(next(model.parameters()).device)\n",
        "                # Forward pass\n",
        "                outputs, _, _ = model(data.x, data.edge_index, data.edge_attr, data.batch)\n",
        "                # Get probabilities\n",
        "                probs = F.softmax(outputs, dim=1)[:, 1].cpu().numpy()\n",
        "                all_preds.extend(probs)\n",
        "\n",
        "        return np.array(all_preds)\n",
        "\n",
        "    results = {}\n",
        "\n",
        "    # 1. Dengue Dataset Accuracy\n",
        "    print(\"\\n=== Dengue Dataset Accuracy ===\")\n",
        "    dengue_data = pd.read_csv('dengue_test.csv')\n",
        "\n",
        "    # Ensure immunogenicity is numeric\n",
        "    dengue_data['immunogenicity'] = pd.to_numeric(dengue_data['immunogenicity'], errors='coerce').fillna(0)\n",
        "\n",
        "    dengue_graphs, dengue_labels = Graph_Constructor.entrance(dengue_data, after_pca, hla_dic, dic_inventory)\n",
        "\n",
        "    # Use custom predict function\n",
        "    dengue_pred_prob = model_predict(model, dengue_graphs, dengue_labels)\n",
        "    dengue_pred = (dengue_pred_prob > 0.5).astype(int)\n",
        "    dengue_acc = accuracy_score(dengue_labels.values, dengue_pred)\n",
        "    dengue_mcc = matthews_corrcoef(dengue_labels.values, dengue_pred)\n",
        "\n",
        "    results['dengue_acc'] = dengue_acc\n",
        "    results['dengue_mcc'] = dengue_mcc\n",
        "\n",
        "    print(f\"Dengue dataset Accuracy: {dengue_acc:.3f}\")\n",
        "    print(f\"Dengue dataset MCC: {dengue_mcc:.3f}\")\n",
        "\n",
        "    # Plot confusion matrix\n",
        "    plot_confusion_matrix(dengue_labels.values, dengue_pred, title='Dengue Dataset Confusion Matrix',\n",
        "                          save_path='dengue')\n",
        "\n",
        "    # 2. Cell Dataset Recall\n",
        "    print(\"\\n=== Cell Dataset Recall ===\")\n",
        "    cell_data = pd.read_csv('ori_test_cells.csv')\n",
        "\n",
        "    # Ensure immunogenicity is numeric\n",
        "    cell_data['immunogenicity'] = pd.to_numeric(cell_data['immunogenicity'], errors='coerce').fillna(0)\n",
        "\n",
        "    cell_graphs, cell_labels = Graph_Constructor.entrance(cell_data, after_pca, hla_dic, dic_inventory)\n",
        "\n",
        "    # Use custom predict function\n",
        "    cell_pred_prob = model_predict(model, cell_graphs, cell_labels)\n",
        "    cell_pred = (cell_pred_prob > 0.5).astype(int)\n",
        "    cell_recall = recall_score(cell_labels.values, cell_pred)\n",
        "    cell_mcc = matthews_corrcoef(cell_labels.values, cell_pred)\n",
        "\n",
        "    results['cell_recall'] = cell_recall\n",
        "    results['cell_mcc'] = cell_mcc\n",
        "\n",
        "    print(f\"Cell dataset Recall: {cell_recall:.3f}\")\n",
        "    print(f\"Cell dataset MCC: {cell_mcc:.3f}\")\n",
        "\n",
        "    # Plot confusion matrix\n",
        "    plot_confusion_matrix(cell_labels.values, cell_pred, title='Cell Dataset Confusion Matrix',\n",
        "                          save_path='cell')\n",
        "\n",
        "    # 3-4. COVID Dataset (Convalescent and Unexposed)\n",
        "    print(\"\\n=== COVID Dataset ===\")\n",
        "    covid_data = pd.read_csv('sars_cov_2_result.csv')\n",
        "\n",
        "    # Process convalescent samples\n",
        "    conv_data = covid_data.copy()\n",
        "    # Make sure immunogenicity-con is in the dataset and convert to numeric\n",
        "    if 'immunogenicity-con' in conv_data.columns:\n",
        "        conv_data['immunogenicity'] = pd.to_numeric(conv_data['immunogenicity-con'], errors='coerce').fillna(0)\n",
        "    else:\n",
        "        print(\"Warning: 'immunogenicity-con' column not found in COVID dataset\")\n",
        "        conv_data['immunogenicity'] = 0  # Default if column not found\n",
        "\n",
        "    conv_graphs, conv_labels = Graph_Constructor.entrance(conv_data, after_pca, hla_dic, dic_inventory)\n",
        "\n",
        "    # Use custom predict function\n",
        "    conv_pred_prob = model_predict(model, conv_graphs, conv_labels)\n",
        "    conv_pred = (conv_pred_prob > 0.5).astype(int)\n",
        "    conv_recall = recall_score(conv_labels.values, conv_pred)\n",
        "    conv_mcc = matthews_corrcoef(conv_labels.values, conv_pred)\n",
        "\n",
        "    results['covid_conv_recall'] = conv_recall\n",
        "    results['covid_conv_mcc'] = conv_mcc\n",
        "\n",
        "    print(f\"COVID dataset (Convalescent) Recall: {conv_recall:.3f}\")\n",
        "    print(f\"COVID dataset (Convalescent) MCC: {conv_mcc:.3f}\")\n",
        "\n",
        "    # Plot confusion matrix\n",
        "    plot_confusion_matrix(conv_labels.values, conv_pred, title='COVID Dataset (Convalescent) Confusion Matrix',\n",
        "                          save_path='covid_conv')\n",
        "\n",
        "    # Process unexposed samples\n",
        "    unexp_data = covid_data.copy()\n",
        "    # Make sure immunogenicity-un is in the dataset and convert to numeric\n",
        "    if 'immunogenicity-un' in unexp_data.columns:\n",
        "        unexp_data['immunogenicity'] = pd.to_numeric(unexp_data['immunogenicity-un'], errors='coerce').fillna(0)\n",
        "    else:\n",
        "        print(\"Warning: 'immunogenicity-un' column not found in COVID dataset\")\n",
        "        unexp_data['immunogenicity'] = 0  # Default if column not found\n",
        "\n",
        "    unexp_graphs, unexp_labels = Graph_Constructor.entrance(unexp_data, after_pca, hla_dic, dic_inventory)\n",
        "\n",
        "    # Use custom predict function\n",
        "    unexp_pred_prob = model_predict(model, unexp_graphs, unexp_labels)\n",
        "    unexp_pred = (unexp_pred_prob > 0.5).astype(int)\n",
        "    unexp_recall = recall_score(unexp_labels.values, unexp_pred)\n",
        "    unexp_mcc = matthews_corrcoef(unexp_labels.values, unexp_pred)\n",
        "\n",
        "    results['covid_unexp_recall'] = unexp_recall\n",
        "    results['covid_unexp_mcc'] = unexp_mcc\n",
        "\n",
        "    print(f\"COVID dataset (Unexposed) Recall: {unexp_recall:.3f}\")\n",
        "    print(f\"COVID dataset (Unexposed) MCC: {unexp_mcc:.3f}\")\n",
        "\n",
        "    # Plot confusion matrix\n",
        "    plot_confusion_matrix(unexp_labels.values, unexp_pred, title='COVID Dataset (Unexposed) Confusion Matrix',\n",
        "                          save_path='covid_unexp')\n",
        "\n",
        "    # 5. DeepHLApan dataset\n",
        "    print(\"\\n=== DeepHLApan Dataset ===\")\n",
        "    deephlapan_data = pd.read_csv('deephlapan_result_cell.csv')\n",
        "\n",
        "    # Ensure immunogenicity is numeric\n",
        "    deephlapan_data['immunogenicity'] = pd.to_numeric(deephlapan_data['immunogenicity'], errors='coerce').fillna(0)\n",
        "\n",
        "    deephlapan_graphs, deephlapan_labels = Graph_Constructor.entrance(deephlapan_data, after_pca, hla_dic, dic_inventory)\n",
        "\n",
        "    # Use custom predict function\n",
        "    deephlapan_pred_prob = model_predict(model, deephlapan_graphs, deephlapan_labels)\n",
        "    deephlapan_pred = (deephlapan_pred_prob > 0.5).astype(int)\n",
        "    deephlapan_acc = accuracy_score(deephlapan_labels.values, deephlapan_pred)\n",
        "    deephlapan_recall = recall_score(deephlapan_labels.values, deephlapan_pred)\n",
        "    deephlapan_mcc = matthews_corrcoef(deephlapan_labels.values, deephlapan_pred)\n",
        "\n",
        "    results['deephlapan_acc'] = deephlapan_acc\n",
        "    results['deephlapan_recall'] = deephlapan_recall\n",
        "    results['deephlapan_mcc'] = deephlapan_mcc\n",
        "\n",
        "    print(f\"DeepHLApan dataset Accuracy: {deephlapan_acc:.3f}\")\n",
        "    print(f\"DeepHLApan dataset Recall: {deephlapan_recall:.3f}\")\n",
        "    print(f\"DeepHLApan dataset MCC: {deephlapan_mcc:.3f}\")\n",
        "\n",
        "    # Plot confusion matrix\n",
        "    plot_confusion_matrix(deephlapan_labels.values, deephlapan_pred, title='DeepHLApan Dataset Confusion Matrix',\n",
        "                          save_path='deephlapan')\n",
        "\n",
        "    # Summary table\n",
        "    print(\"\\n=== Summary of Results ===\")\n",
        "    print(f\"Dengue dataset Accuracy: {dengue_acc:.3f}, MCC: {dengue_mcc:.3f}\")\n",
        "    print(f\"Cell dataset Recall: {cell_recall:.3f}, MCC: {cell_mcc:.3f}\")\n",
        "    print(f\"COVID dataset (Convalescent) Recall: {conv_recall:.3f}, MCC: {conv_mcc:.3f}\")\n",
        "    print(f\"COVID dataset (Unexposed) Recall: {unexp_recall:.3f}, MCC: {unexp_mcc:.3f}\")\n",
        "    print(f\"DeepHLApan dataset Accuracy: {deephlapan_acc:.3f}, Recall: {deephlapan_recall:.3f}, MCC: {deephlapan_mcc:.3f}\")\n",
        "\n",
        "    return results\n",
        "\n",
        "def run_validation():\n",
        "    \"\"\"Run validation on all test datasets\"\"\"\n",
        "\n",
        "    # Load data\n",
        "    print(\"Loading data...\")\n",
        "    hla_df = pd.read_csv('hla2paratopeTable_aligned.txt', sep='\\t')\n",
        "    after_pca = np.loadtxt('after_pca.txt')\n",
        "\n",
        "    # Process HLA data\n",
        "    hla_dic = hla_df_to_dic(hla_df)\n",
        "    inventory = list(hla_dic.keys())\n",
        "    dic_inventory = dict_inventory(inventory)\n",
        "\n",
        "    # Load the trained model with custom objects\n",
        "    model = load_model_with_custom_objects(\"hla_peptide_model.h5\")\n",
        "\n",
        "    # Run validation on test datasets\n",
        "    print(\"Running validation on test datasets...\")\n",
        "    results = validate_on_test_datasets(model, after_pca, hla_dic, dic_inventory)\n",
        "\n",
        "    # Save results to CSV\n",
        "    results_df = pd.DataFrame([results])\n",
        "    results_df.to_csv('validation_results.csv', index=False)\n",
        "    print(\"Validation results saved to validation_results.csv\")\n",
        "\n",
        "def save_history_to_log(history, filepath=\"History_IDGL.log\"):\n",
        "    \"\"\"Save training history to a log file\"\"\"\n",
        "    with open(filepath, 'w') as f:\n",
        "        f.write(\"IDGL Training History\\n\")\n",
        "        f.write(\"====================\\n\\n\")\n",
        "\n",
        "        # Format each metric\n",
        "        for key in history:\n",
        "            f.write(f\"{key}:\\n\")\n",
        "            for i, value in enumerate(history[key]):\n",
        "                f.write(f\"Epoch {i+1}: {value}\\n\")\n",
        "            f.write(\"\\n\")\n",
        "\n",
        "    print(f\"History saved to {filepath}\")\n",
        "\n",
        "# After training is complete and model is saved\n",
        "def run_validation_after_training(model, after_pca, hla_dic, dic_inventory):\n",
        "    print(\"\\n=== Running Validation on Test Datasets ===\")\n",
        "    results = validate_on_test_datasets(model, after_pca, hla_dic, dic_inventory)\n",
        "    return results\n",
        "\n",
        "\n",
        "\n",
        "# === Deep GCN Classifier with Residual Connections ===\n",
        "class DeepGCNClassifier(nn.Module):\n",
        "    def __init__(self, input_dim, hidden_dims=[64, 64, 32], dropout=0.3):\n",
        "        \"\"\"\n",
        "        Deep GCN classifier with residual connections and batch normalization\n",
        "\n",
        "        Args:\n",
        "            input_dim: Input feature dimension\n",
        "            hidden_dims: List of hidden layer dimensions\n",
        "            dropout: Dropout rate\n",
        "        \"\"\"\n",
        "        super(DeepGCNClassifier, self).__init__()\n",
        "\n",
        "        # Initialize layer lists\n",
        "        self.gcn_layers = nn.ModuleList()\n",
        "        self.norm_layers = nn.ModuleList()\n",
        "        self.skip_proj = nn.ModuleList()\n",
        "        self.dropout = dropout\n",
        "\n",
        "        # Build network with residual connections\n",
        "        prev_dim = input_dim\n",
        "        for h_dim in hidden_dims:\n",
        "            # GCN layer\n",
        "            self.gcn_layers.append(GCNConv(prev_dim, h_dim))\n",
        "\n",
        "            # Batch normalization\n",
        "            self.norm_layers.append(BatchNorm(h_dim))\n",
        "\n",
        "            # Projection for residual connection if dimensions don't match\n",
        "            if prev_dim != h_dim:\n",
        "                self.skip_proj.append(nn.Linear(prev_dim, h_dim))\n",
        "            else:\n",
        "                self.skip_proj.append(nn.Identity())\n",
        "\n",
        "            prev_dim = h_dim\n",
        "\n",
        "        # Final classification layer\n",
        "        self.classifier = nn.Linear(hidden_dims[-1], 2)\n",
        "\n",
        "    def forward(self, x, edge_index, batch):\n",
        "        \"\"\"\n",
        "        Forward pass\n",
        "\n",
        "        Args:\n",
        "            x: Node features [num_nodes, input_dim]\n",
        "            edge_index: Graph connectivity [2, num_edges]\n",
        "            batch: Batch indices [num_nodes]\n",
        "\n",
        "        Returns:\n",
        "            out: Classification logits [batch_size, 2]\n",
        "        \"\"\"\n",
        "        # Pass through GCN layers with residual connections\n",
        "        for conv, norm, skip in zip(self.gcn_layers, self.norm_layers, self.skip_proj):\n",
        "            h_in = x  # Store for residual connection\n",
        "\n",
        "            # GCN layer\n",
        "            x = conv(x, edge_index)\n",
        "\n",
        "            # Normalization and activation\n",
        "            x = norm(x)\n",
        "            x = F.relu(x)\n",
        "            x = F.dropout(x, p=self.dropout, training=self.training)\n",
        "\n",
        "            # Residual connection\n",
        "            x = x + skip(h_in)\n",
        "\n",
        "        # Global pooling\n",
        "        x = global_mean_pool(x, batch)\n",
        "\n",
        "        # Classification\n",
        "        out = self.classifier(x)\n",
        "        return out\n",
        "\n",
        "# === Conversion from StellarGraph to PyTorch Geometric ===\n",
        "def stellargraph_to_torch_data(graph, label):\n",
        "    \"\"\"\n",
        "    Convert a StellarGraph to a PyTorch Geometric Data object\n",
        "\n",
        "    Args:\n",
        "        graph: StellarGraph object\n",
        "        label: Label for the graph\n",
        "\n",
        "    Returns:\n",
        "        data: PyTorch Geometric Data object\n",
        "    \"\"\"\n",
        "    # Extract adjacency matrix\n",
        "    adj = graph.to_adjacency_matrix(weighted=True).todense()\n",
        "\n",
        "    # Extract edge indices from adjacency matrix\n",
        "    edge_index = torch.tensor(np.vstack(adj.nonzero()), dtype=torch.long)\n",
        "\n",
        "    # Extract edge weights if present\n",
        "    edge_weights = np.array(adj[adj.nonzero()])\n",
        "    edge_attr = torch.tensor(edge_weights, dtype=torch.float32).squeeze()\n",
        "\n",
        "    # Extract node features\n",
        "    x = torch.tensor(np.array(graph.node_features()), dtype=torch.float32)\n",
        "\n",
        "    # Create target\n",
        "    y = torch.tensor(int(label), dtype=torch.long)\n",
        "\n",
        "    # Create Data object\n",
        "    data = Data(x=x, edge_index=edge_index, edge_attr=edge_attr, y=y)\n",
        "\n",
        "    return data\n",
        "\n",
        "# === Deep GCN Classifier with Residual Connections ===\n",
        "class DeepGCNClassifier(nn.Module):\n",
        "    def __init__(self, input_dim, hidden_dims=[64, 64, 32], dropout=0.3):\n",
        "        super(DeepGCNClassifier, self).__init__()\n",
        "        self.gcn_layers = nn.ModuleList()\n",
        "        self.norm_layers = nn.ModuleList()\n",
        "        self.skip_proj = nn.ModuleList()\n",
        "        self.dropout = dropout\n",
        "\n",
        "        # Build network with residual connections\n",
        "        prev_dim = input_dim\n",
        "        for h_dim in hidden_dims:\n",
        "            self.gcn_layers.append(GCNConv(prev_dim, h_dim))\n",
        "            self.norm_layers.append(BatchNorm(h_dim))\n",
        "            if prev_dim != h_dim:\n",
        "                self.skip_proj.append(nn.Linear(prev_dim, h_dim))\n",
        "            else:\n",
        "                self.skip_proj.append(nn.Identity())\n",
        "            prev_dim = h_dim\n",
        "\n",
        "        # Final classification layer\n",
        "        self.classifier = nn.Linear(hidden_dims[-1], 2)\n",
        "\n",
        "    def forward(self, x, edge_index, edge_attr, batch):\n",
        "        # Pass through GCN layers with residual connections\n",
        "        for conv, norm, skip in zip(self.gcn_layers, self.norm_layers, self.skip_proj):\n",
        "            h_in = x  # Store for residual connection\n",
        "            x = conv(x, edge_index, edge_attr)\n",
        "            x = norm(x)\n",
        "            x = F.relu(x)\n",
        "            x = F.dropout(x, p=self.dropout, training=self.training)\n",
        "            x = x + skip(h_in)  # Residual connection\n",
        "\n",
        "        # Global pooling\n",
        "        x = global_mean_pool(x, batch)\n",
        "\n",
        "        # Classification\n",
        "        out = self.classifier(x)\n",
        "        return out\n",
        "\n",
        "class GraphLearningEnhanced(nn.Module):\n",
        "    \"\"\"Enhanced Graph Learning Module with anchor-based similarity for peptide-HLA connections\"\"\"\n",
        "    def __init__(self, input_dim, hidden_dim=64, k=10, lambda_sparse=0.1, lambda_smooth=0.1, num_heads=4, epsilon=0.5):\n",
        "        super(GraphLearningEnhanced, self).__init__()\n",
        "        self.k = k\n",
        "        self.lambda_sparse = lambda_sparse\n",
        "        self.lambda_smooth = lambda_smooth\n",
        "        self.epsilon = epsilon\n",
        "        self.num_heads = num_heads\n",
        "\n",
        "        # Multiple attention heads for similarity computation\n",
        "        self.att_weights = nn.ModuleList([\n",
        "            nn.Linear(input_dim, hidden_dim) for _ in range(num_heads)\n",
        "        ])\n",
        "\n",
        "        # Attention scores for weighted similarity combination\n",
        "        self.att_scores = nn.Parameter(torch.ones(num_heads, 1) / num_heads)\n",
        "\n",
        "        # Learnable affinity transformation\n",
        "        self.affinity_transform = nn.Sequential(\n",
        "            nn.Linear(1, hidden_dim),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(hidden_dim, 1),\n",
        "            nn.Sigmoid()\n",
        "        )\n",
        "\n",
        "        # Flag to control similarity computation method\n",
        "        self.use_anchors = True  # Set to False to use full pairwise similarity\n",
        "\n",
        "    def compute_similarity(self, x_graph, batch=None):\n",
        "        \"\"\"\n",
        "        Compute node similarities using either full pairwise or anchor-based approach\n",
        "\n",
        "        Args:\n",
        "            x_graph: Node features [num_nodes, input_dim]\n",
        "            batch: Batch indices [num_nodes] (optional)\n",
        "\n",
        "        Returns:\n",
        "            sim_matrix: Similarity matrix [num_nodes, num_nodes]\n",
        "        \"\"\"\n",
        "        num_nodes = x_graph.size(0)\n",
        "\n",
        "        # If graph is very small, use full similarity computation\n",
        "        if num_nodes < 30 or not self.use_anchors:\n",
        "            # Full pairwise similarity computation (original approach)\n",
        "            similarity_matrices = []\n",
        "            for head in range(self.num_heads):\n",
        "                # Project features\n",
        "                x_proj = self.att_weights[head](x_graph)\n",
        "                x_proj = F.normalize(x_proj, p=2, dim=1)  # L2 normalization\n",
        "\n",
        "                # Compute cosine similarity\n",
        "                sim = torch.mm(x_proj, x_proj.t())\n",
        "                similarity_matrices.append(sim)\n",
        "\n",
        "            # Stack and apply attention weights for weighted combination\n",
        "            sim_stack = torch.stack(similarity_matrices)  # [num_heads, n_nodes, n_nodes]\n",
        "            attention_weights = F.softmax(self.att_scores, dim=0)  # [num_heads, 1]\n",
        "\n",
        "            # Weighted average of similarity matrices\n",
        "            sim_matrix = torch.sum(sim_stack * attention_weights.view(-1, 1, 1), dim=0)\n",
        "\n",
        "        else:\n",
        "            # Anchor-based similarity computation (more efficient)\n",
        "\n",
        "            # 1. Select anchor nodes (simplified - just select ~sqrt(n) anchors)\n",
        "            import math\n",
        "            num_anchors = min(16, max(4, int(math.sqrt(num_nodes))))\n",
        "\n",
        "            # Simple uniform sampling of anchors\n",
        "            perm = torch.randperm(num_nodes, device=x_graph.device)\n",
        "            anchor_idx = perm[:num_anchors]\n",
        "            anchors = x_graph[anchor_idx]\n",
        "\n",
        "            # 2. Compute node-anchor similarities\n",
        "            node_anchor_sim = []\n",
        "\n",
        "            for head in range(self.num_heads):\n",
        "                # Project features and anchors\n",
        "                x_proj = self.att_weights[head](x_graph)\n",
        "                a_proj = self.att_weights[head](anchors)\n",
        "\n",
        "                # Normalize\n",
        "                x_proj = F.normalize(x_proj, p=2, dim=1)\n",
        "                a_proj = F.normalize(a_proj, p=2, dim=1)\n",
        "\n",
        "                # Compute node-anchor similarity\n",
        "                sim = torch.mm(x_proj, a_proj.t())  # [n_nodes, n_anchors]\n",
        "                node_anchor_sim.append(sim)\n",
        "\n",
        "            # Stack and apply attention weights\n",
        "            sim_stack = torch.stack(node_anchor_sim)  # [num_heads, n_nodes, n_anchors]\n",
        "            attention_weights = F.softmax(self.att_scores, dim=0)  # [num_heads, 1]\n",
        "\n",
        "            # Weighted average of similarity matrices\n",
        "            combined_sim = torch.sum(sim_stack * attention_weights.view(-1, 1, 1), dim=0)\n",
        "\n",
        "            # 3. Approximate full similarity matrix\n",
        "            sim_matrix = torch.mm(combined_sim, combined_sim.t())  # [n_nodes, n_nodes]\n",
        "\n",
        "            # Make sure diagonal is 1.0\n",
        "            sim_matrix.fill_diagonal_(1.0)\n",
        "\n",
        "        return sim_matrix\n",
        "\n",
        "    def forward(self, x, batch=None):\n",
        "        batch_size = 1\n",
        "        if batch is None:\n",
        "            batch = torch.zeros(x.size(0), dtype=torch.long, device=x.device)\n",
        "\n",
        "        # Process each graph in the batch separately\n",
        "        unique_batches = batch.unique()\n",
        "        edge_indices_list = []\n",
        "        edge_weights_list = []\n",
        "        sparse_loss_list = []\n",
        "        smooth_loss_list = []\n",
        "\n",
        "        for b in unique_batches:\n",
        "            # Get nodes for this graph\n",
        "            mask = (batch == b)\n",
        "            x_graph = x[mask]\n",
        "            indices = torch.nonzero(mask).squeeze()\n",
        "\n",
        "            # Identify peptide and HLA nodes\n",
        "            num_nodes = x_graph.size(0)\n",
        "            peptide_nodes = list(range(num_nodes // 3))\n",
        "            hla_nodes = list(range(num_nodes // 3, num_nodes))\n",
        "\n",
        "            # Dynamically adjust parameters based on graph properties\n",
        "            dynamic_k = min(self.k, max(num_nodes // 4, 5))  # Smaller k for smaller graphs\n",
        "            dynamic_epsilon = 0.3  # A good threshold value\n",
        "\n",
        "            # Compute similarity matrix using anchor-based approach\n",
        "            sim_matrix = self.compute_similarity(x_graph)\n",
        "\n",
        "            # Apply biologically-informed similarity adjustment for peptide-HLA interactions\n",
        "            for p in peptide_nodes:\n",
        "                for h in hla_nodes:\n",
        "                    # Force higher similarity with a minimum baseline value\n",
        "                    base_similarity = 0.6  # Higher base value\n",
        "\n",
        "                    # Position-based weighting to mimic binding pockets\n",
        "                    p_rel_pos = p / len(peptide_nodes) if len(peptide_nodes) > 0 else 0\n",
        "                    h_rel_pos = (h - len(peptide_nodes)) / len(hla_nodes) if len(hla_nodes) > 0 else 0\n",
        "\n",
        "                    # Create higher similarity for matching relative positions\n",
        "                    position_score = 1.0 - 0.5 * abs(p_rel_pos - h_rel_pos)\n",
        "\n",
        "                    # Boost similarity for peptide-HLA pairs substantially\n",
        "                    adjusted_sim = max(sim_matrix[p, h].item(), base_similarity + 0.3 * position_score)\n",
        "\n",
        "                    # Apply the boosted similarity\n",
        "                    sim_matrix[p, h] = adjusted_sim\n",
        "                    sim_matrix[h, p] = adjusted_sim  # Maintain symmetry\n",
        "\n",
        "            # Apply ReLU and learnable affinity transformation\n",
        "            sim_matrix = F.relu(sim_matrix)  # Non-negative similarities\n",
        "\n",
        "            # Apply learnable affinity transformation\n",
        "            flat_sim = sim_matrix.view(-1, 1)\n",
        "            transformed_sim = self.affinity_transform(flat_sim).view(sim_matrix.shape)\n",
        "\n",
        "            # Apply dynamic epsilon-neighborhood sparsification\n",
        "            mask = (transformed_sim > dynamic_epsilon).float() * transformed_sim\n",
        "\n",
        "            # Apply dynamic top-k sparsification\n",
        "            if dynamic_k < sim_matrix.size(0):\n",
        "                # Get top-k values for each node\n",
        "                values, indices = torch.topk(transformed_sim, k=min(dynamic_k, transformed_sim.size(0)), dim=1)\n",
        "                topk_mask = torch.zeros_like(transformed_sim)\n",
        "\n",
        "                # Create mask with only top-k values\n",
        "                for i in range(transformed_sim.size(0)):\n",
        "                    topk_mask[i, indices[i]] = values[i]\n",
        "\n",
        "                # Apply mask\n",
        "                mask = mask * topk_mask\n",
        "\n",
        "            # Ensure adjacency matrix is symmetric\n",
        "            adj = 0.5 * (mask + mask.t())\n",
        "\n",
        "            # Check if we have peptide-HLA connections\n",
        "            p_h_edges = 0\n",
        "            for p in peptide_nodes:\n",
        "                for h in hla_nodes:\n",
        "                    if adj[p, h] > 0:\n",
        "                        p_h_edges += 1\n",
        "\n",
        "            # If insufficient peptide-HLA connections, add some\n",
        "            min_expected = min(len(peptide_nodes), len(hla_nodes)) // 2\n",
        "            if p_h_edges < min_expected:\n",
        "                # Add connections between some peptide and HLA nodes\n",
        "                for p_idx in peptide_nodes[:min(5, len(peptide_nodes))]:\n",
        "                    for h_idx in hla_nodes[:min(5, len(hla_nodes))]:\n",
        "                        adj[p_idx, h_idx] = 0.7  # Higher weight\n",
        "                        adj[h_idx, p_idx] = 0.7  # Ensure symmetry\n",
        "\n",
        "            # Extract edge_index and edge_weight from adjacency\n",
        "            adj_sparse = adj.to_sparse()\n",
        "            edge_index = adj_sparse.indices()\n",
        "            edge_weight = adj_sparse.values()\n",
        "\n",
        "            # Shift indices based on batch offset\n",
        "            offset = indices.min() if indices.numel() > 0 else 0\n",
        "            edge_index = edge_index + offset\n",
        "\n",
        "            # Store for this graph\n",
        "            edge_indices_list.append(edge_index)\n",
        "            edge_weights_list.append(edge_weight)\n",
        "\n",
        "            # Compute regularization terms\n",
        "            # 1. Sparsity regularization (encourage sparse connections)\n",
        "            sparse_loss = torch.sum(adj) / (adj.size(0) * adj.size(1))\n",
        "            sparse_loss_list.append(sparse_loss)\n",
        "\n",
        "            # 2. Smoothness regularization\n",
        "            D = torch.diag(torch.sum(adj, dim=1))\n",
        "            L = D - adj  # Graph Laplacian\n",
        "            smooth_loss = torch.trace(torch.mm(torch.mm(x_graph.t(), L), x_graph)) / x_graph.size(0)\n",
        "            smooth_loss_list.append(smooth_loss)\n",
        "\n",
        "        # Combine all graphs' edges\n",
        "        if len(edge_indices_list) > 0 and all(e.numel() > 0 for e in edge_indices_list):\n",
        "            combined_edge_index = torch.cat(edge_indices_list, dim=1)\n",
        "            combined_edge_weights = torch.cat(edge_weights_list)\n",
        "        else:\n",
        "            # Handle empty case\n",
        "            combined_edge_index = torch.zeros((2, 0), dtype=torch.long, device=x.device)\n",
        "            combined_edge_weights = torch.zeros(0, dtype=torch.float, device=x.device)\n",
        "\n",
        "        # Average regularization losses\n",
        "        avg_sparse_loss = sum(sparse_loss_list) / len(sparse_loss_list) if sparse_loss_list else torch.tensor(0.0).to(x.device)\n",
        "        avg_smooth_loss = sum(smooth_loss_list) / len(smooth_loss_list) if smooth_loss_list else torch.tensor(0.0).to(x.device)\n",
        "\n",
        "        return combined_edge_index, combined_edge_weights, avg_sparse_loss, avg_smooth_loss\n",
        "\n",
        "class EnhancedIDGLFramework(nn.Module):\n",
        "    \"\"\"Enhanced IDGL Framework with learnable combination weights and GCN refinement\"\"\"\n",
        "    def __init__(self, input_dim, hidden_dims=[64, 64, 32], dropout=0.3, k=10,\n",
        "                 lambda_sparse=0.1, lambda_smooth=0.1, num_heads=4, epsilon=0.5,\n",
        "                 max_iterations=5, tol=1e-4):\n",
        "        super(EnhancedIDGLFramework, self).__init__()\n",
        "\n",
        "        # Graph learning module\n",
        "        self.graph_learner = GraphLearningEnhanced(\n",
        "            input_dim=input_dim,\n",
        "            hidden_dim=hidden_dims[0],\n",
        "            k=k,\n",
        "            lambda_sparse=lambda_sparse,\n",
        "            lambda_smooth=lambda_smooth,\n",
        "            num_heads=num_heads,\n",
        "            epsilon=epsilon\n",
        "        )\n",
        "\n",
        "        # GCN for node embedding updates\n",
        "        self.update_gcn = GCNConv(input_dim, input_dim)\n",
        "\n",
        "        # GCN classifier\n",
        "        self.gcn = DeepGCNClassifier(\n",
        "            input_dim=input_dim,\n",
        "            hidden_dims=hidden_dims,\n",
        "            dropout=dropout\n",
        "        )\n",
        "\n",
        "        # Learnable lambda and eta parameters\n",
        "        self.lambda_init = nn.Parameter(torch.tensor(0.8))\n",
        "        self.eta = nn.Parameter(torch.tensor(0.5))\n",
        "\n",
        "        # IDGL parameters\n",
        "        self.max_iterations = max_iterations\n",
        "        self.tol = tol\n",
        "        self.input_dim = input_dim\n",
        "\n",
        "    def forward(self, x, edge_index, edge_attr, batch):\n",
        "        # Initial embeddings\n",
        "        h = x\n",
        "\n",
        "        # First iteration - learn graph from raw features\n",
        "        edge_index_1, edge_attr_1, sparse_loss_1, smooth_loss_1 = self.graph_learner(h, batch)\n",
        "\n",
        "        # Update node embeddings using GCN with the learned graph\n",
        "        if edge_index_1.size(1) > 0:\n",
        "            h = F.relu(self.update_gcn(h, edge_index_1, edge_attr_1))\n",
        "\n",
        "        # Initialize tracking variables\n",
        "        prev_sparse_loss = sparse_loss_1\n",
        "        prev_smooth_loss = smooth_loss_1\n",
        "        total_reg_loss = self.graph_learner.lambda_sparse * sparse_loss_1 + \\\n",
        "                        self.graph_learner.lambda_smooth * smooth_loss_1\n",
        "        iterations = 1\n",
        "\n",
        "        # Iterative refinement\n",
        "        for t in range(2, self.max_iterations + 1):\n",
        "            # Learn new graph from updated embeddings\n",
        "            edge_index_t, edge_attr_t, sparse_loss_t, smooth_loss_t = self.graph_learner(h, batch)\n",
        "\n",
        "            # Simple convergence check\n",
        "            if t > 2 and (sparse_loss_t + smooth_loss_t) >= (prev_sparse_loss + prev_smooth_loss) * 0.99:\n",
        "                # If loss isn't decreasing meaningfully, we've converged\n",
        "                break\n",
        "\n",
        "            # Combine with initial and first iteration graphs using learnable parameters\n",
        "            if edge_index.size(1) > 0 and edge_index_t.size(1) > 0:\n",
        "                # Create combined graph\n",
        "                lambda_constrained = torch.sigmoid(self.lambda_init)  # Constrain to [0, 1]\n",
        "                eta_constrained = torch.sigmoid(self.eta)  # Constrain to [0, 1]\n",
        "\n",
        "                # For edge indices, concatenate and handle duplicates later\n",
        "                combined_indices = torch.cat([edge_index, edge_index_1, edge_index_t], dim=1)\n",
        "\n",
        "                # Scale edge weights according to learnable parameters\n",
        "                edge_weights_initial = edge_attr * lambda_constrained\n",
        "                edge_weights_1 = edge_attr_1 * (1 - lambda_constrained) * eta_constrained\n",
        "                edge_weights_t = edge_attr_t * (1 - lambda_constrained) * (1 - eta_constrained)\n",
        "\n",
        "                combined_weights = torch.cat([edge_weights_initial, edge_weights_1, edge_weights_t])\n",
        "\n",
        "                # Update node embeddings with GCN\n",
        "                if combined_indices.size(1) > 0:\n",
        "                    h = F.relu(self.update_gcn(h, combined_indices, combined_weights))\n",
        "            elif edge_index_t.size(1) > 0:\n",
        "                # Just use the current learned graph if initial graphs are empty\n",
        "                h = F.relu(self.update_gcn(h, edge_index_t, edge_attr_t))\n",
        "\n",
        "            # Update tracking variables\n",
        "            prev_sparse_loss = sparse_loss_t\n",
        "            prev_smooth_loss = smooth_loss_t\n",
        "            iterations += 1\n",
        "            total_reg_loss += self.graph_learner.lambda_sparse * sparse_loss_t + \\\n",
        "                             self.graph_learner.lambda_smooth * smooth_loss_t\n",
        "\n",
        "        # Average regularization loss\n",
        "        avg_reg_loss = total_reg_loss / iterations\n",
        "\n",
        "        # For the final classification, use the latest graph structure\n",
        "        if edge_index.size(1) > 0 and edge_index_t.size(1) > 0:\n",
        "            # Combine initial, first and last iteration graphs\n",
        "            lambda_constrained = torch.sigmoid(self.lambda_init)\n",
        "            eta_constrained = torch.sigmoid(self.eta)\n",
        "\n",
        "            final_indices = torch.cat([edge_index, edge_index_1, edge_index_t], dim=1)\n",
        "\n",
        "            edge_weights_initial = edge_attr * lambda_constrained\n",
        "            edge_weights_1 = edge_attr_1 * (1 - lambda_constrained) * eta_constrained\n",
        "            edge_weights_t = edge_attr_t * (1 - lambda_constrained) * (1 - eta_constrained)\n",
        "\n",
        "            final_weights = torch.cat([edge_weights_initial, edge_weights_1, edge_weights_t])\n",
        "        elif edge_index_t.size(1) > 0:\n",
        "            final_indices = edge_index_t\n",
        "            final_weights = edge_attr_t\n",
        "        else:\n",
        "            final_indices = edge_index\n",
        "            final_weights = edge_attr\n",
        "\n",
        "        # Classification using GCN with final graph\n",
        "        out = self.gcn(x, final_indices, final_weights, batch)\n",
        "\n",
        "        return out, final_indices, avg_reg_loss\n",
        "\n",
        "\n",
        "# DeepGCNClassifier class remains the same\n",
        "class DeepGCNClassifier(nn.Module):\n",
        "    def __init__(self, input_dim, hidden_dims=[64, 64, 32], dropout=0.3):\n",
        "        super(DeepGCNClassifier, self).__init__()\n",
        "        self.gcn_layers = nn.ModuleList()\n",
        "        self.norm_layers = nn.ModuleList()\n",
        "        self.skip_proj = nn.ModuleList()\n",
        "        self.dropout = dropout\n",
        "\n",
        "        # Build network with residual connections\n",
        "        prev_dim = input_dim\n",
        "        for h_dim in hidden_dims:\n",
        "            self.gcn_layers.append(GCNConv(prev_dim, h_dim))\n",
        "            self.norm_layers.append(BatchNorm(h_dim))\n",
        "            if prev_dim != h_dim:\n",
        "                self.skip_proj.append(nn.Linear(prev_dim, h_dim))\n",
        "            else:\n",
        "                self.skip_proj.append(nn.Identity())\n",
        "            prev_dim = h_dim\n",
        "\n",
        "        # Final classification layer\n",
        "        self.classifier = nn.Linear(hidden_dims[-1], 2)\n",
        "\n",
        "    def forward(self, x, edge_index, edge_attr, batch):\n",
        "        # Pass through GCN layers with residual connections\n",
        "        for conv, norm, skip in zip(self.gcn_layers, self.norm_layers, self.skip_proj):\n",
        "            h_in = x  # Store for residual connection\n",
        "            x = conv(x, edge_index, edge_attr)\n",
        "            x = norm(x)\n",
        "            x = F.relu(x)\n",
        "            x = F.dropout(x, p=self.dropout, training=self.training)\n",
        "            x = x + skip(h_in)  # Residual connection\n",
        "\n",
        "        # Global pooling\n",
        "        x = global_mean_pool(x, batch)\n",
        "\n",
        "        # Classification\n",
        "        out = self.classifier(x)\n",
        "        return out\n",
        "\n",
        "def run_enhanced_idgl_training():\n",
        "    \"\"\"Run the enhanced IDGL training pipeline\"\"\"\n",
        "    # Set random seeds for reproducibility\n",
        "    torch.manual_seed(42)\n",
        "    np.random.seed(42)\n",
        "    if torch.cuda.is_available():\n",
        "        torch.backends.cudnn.deterministic = True\n",
        "        torch.backends.cudnn.benchmark = False\n",
        "\n",
        "    # Set device\n",
        "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "    print(f\"Using device: {device}\")\n",
        "\n",
        "    # Load data\n",
        "    print(\"Loading data...\")\n",
        "    hla_df = pd.read_csv('hla2paratopeTable_aligned.txt', sep='\\t')\n",
        "    after_pca = np.loadtxt('after_pca.txt')\n",
        "    dataset = pd.read_csv('remove0123_sample100_test.csv')\n",
        "\n",
        "    # Ensure labels are numeric\n",
        "    if dataset['immunogenicity'].dtype == 'object':\n",
        "        if isinstance(dataset['immunogenicity'].iloc[0], str):\n",
        "            label_map = {'Positive': 1, 'Negative': 0}\n",
        "            dataset['immunogenicity'] = dataset['immunogenicity'].map(\n",
        "                lambda x: label_map.get(x, 1 if x != 'Negative' else 0)\n",
        "            )\n",
        "        dataset['immunogenicity'] = dataset['immunogenicity'].astype(int)\n",
        "\n",
        "    # Process HLA data\n",
        "    hla_dic = hla_df_to_dic(hla_df)\n",
        "    inventory = list(hla_dic.keys())\n",
        "    dic_inventory = dict_inventory(inventory)\n",
        "\n",
        "    # Create StellarGraph graphs\n",
        "    print(\"Creating StellarGraph graphs...\")\n",
        "    try:\n",
        "        stellar_graphs, stellar_labels = Graph_Constructor.entrance(\n",
        "            dataset, after_pca, hla_dic, dic_inventory, graph_type='intra_and_inter'\n",
        "        )\n",
        "\n",
        "        # Convert to PyTorch Geometric Data objects\n",
        "        print(\"Converting to PyTorch Geometric format...\")\n",
        "        torch_graphs = []\n",
        "        for i, (g, lbl) in enumerate(zip(stellar_graphs, stellar_labels)):\n",
        "            try:\n",
        "                data = stellargraph_to_torch_data(g, lbl)\n",
        "                torch_graphs.append(data)\n",
        "            except Exception as e:\n",
        "                print(f\"Error converting graph {i}: {e}\")\n",
        "                continue\n",
        "\n",
        "        print(f\"Successfully converted {len(torch_graphs)} graphs to PyTorch Geometric format\")\n",
        "\n",
        "        # Get input dimension from first graph\n",
        "        input_dim = torch_graphs[0].x.size(1)  # Get feature dimension from first graph\n",
        "\n",
        "        # Create model\n",
        "        print(\"Creating enhanced IDGL model...\")\n",
        "        model = EnhancedIDGLFramework(\n",
        "            input_dim=input_dim,\n",
        "            hidden_dims=[64, 32],\n",
        "            dropout=0.3,\n",
        "            k=10,\n",
        "            lambda_sparse=0.1,\n",
        "            lambda_smooth=0.1,\n",
        "            num_heads=4,\n",
        "            epsilon=0.5,\n",
        "            max_iterations=3\n",
        "        ).to(device)\n",
        "\n",
        "        # Create custom train/test split\n",
        "        labels_array = np.array([g.y.item() for g in torch_graphs])\n",
        "        train_indices, test_indices = train_test_split(\n",
        "            range(len(torch_graphs)),\n",
        "            test_size=0.2,\n",
        "            stratify=labels_array,\n",
        "            random_state=42\n",
        "        )\n",
        "\n",
        "        train_dataset = [torch_graphs[i] for i in train_indices]\n",
        "        test_dataset = [torch_graphs[i] for i in test_indices]\n",
        "\n",
        "        # Create data loaders\n",
        "        train_loader = DataLoader(\n",
        "            train_dataset,\n",
        "            batch_size=16,\n",
        "            shuffle=True,\n",
        "            collate_fn=lambda data_list: Batch.from_data_list(data_list)\n",
        "        )\n",
        "\n",
        "        test_loader = DataLoader(\n",
        "            test_dataset,\n",
        "            batch_size=16,\n",
        "            shuffle=False,\n",
        "            collate_fn=lambda data_list: Batch.from_data_list(data_list)\n",
        "        )\n",
        "\n",
        "        # Initialize optimizer\n",
        "        optimizer = torch.optim.Adam(\n",
        "            model.parameters(),\n",
        "            lr=0.001,\n",
        "            weight_decay=5e-4\n",
        "        )\n",
        "\n",
        "        # Initialize learning rate scheduler\n",
        "        scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(\n",
        "            optimizer,\n",
        "            mode='max',\n",
        "            factor=0.5,\n",
        "            patience=5,\n",
        "            verbose=True\n",
        "        )\n",
        "\n",
        "        # Initialize tracking variables\n",
        "        best_val_acc = 0\n",
        "        best_epoch = 0\n",
        "        patience_counter = 0\n",
        "\n",
        "        # Initialize history dictionary\n",
        "        history = {\n",
        "            'loss': [],\n",
        "            'acc': [],\n",
        "            'val_loss': [],\n",
        "            'val_acc': [],\n",
        "            'lambda': [],\n",
        "            'eta': []\n",
        "        }\n",
        "\n",
        "        # Training loop\n",
        "        print(\"Starting training...\")\n",
        "        epochs = 100\n",
        "        for epoch in range(epochs):\n",
        "            # Training\n",
        "            model.train()\n",
        "            total_loss = 0\n",
        "            correct = 0\n",
        "            total = 0\n",
        "\n",
        "            for data in train_loader:\n",
        "                data = data.to(device)\n",
        "                optimizer.zero_grad()\n",
        "\n",
        "                # Forward pass\n",
        "                logits, _, reg_loss = model(data.x, data.edge_index, data.edge_attr, data.batch)\n",
        "\n",
        "                # Compute loss\n",
        "                ce_loss = F.cross_entropy(logits, data.y)\n",
        "                loss = ce_loss + 0.1 * reg_loss\n",
        "\n",
        "                # Backward pass\n",
        "                loss.backward()\n",
        "                optimizer.step()\n",
        "\n",
        "                # Update metrics\n",
        "                total_loss += loss.item() * data.num_graphs\n",
        "                pred = logits.argmax(dim=1)\n",
        "                correct += pred.eq(data.y).sum().item()\n",
        "                total += data.num_graphs\n",
        "\n",
        "            # Calculate training metrics\n",
        "            avg_train_loss = total_loss / len(train_dataset)\n",
        "            train_acc = correct / total\n",
        "\n",
        "            # Validation\n",
        "            model.eval()\n",
        "            val_loss = 0\n",
        "            val_correct = 0\n",
        "            val_total = 0\n",
        "\n",
        "            with torch.no_grad():\n",
        "                for data in test_loader:\n",
        "                    data = data.to(device)\n",
        "\n",
        "                    # Forward pass with IDGL\n",
        "                    logits, _, reg_loss = model(data.x, data.edge_index, data.edge_attr, data.batch)\n",
        "\n",
        "                    # Compute loss\n",
        "                    ce_loss = F.cross_entropy(logits, data.y)\n",
        "                    loss = ce_loss + 0.1 * reg_loss\n",
        "\n",
        "                    # Update metrics\n",
        "                    val_loss += loss.item() * data.num_graphs\n",
        "                    pred = logits.argmax(dim=1)\n",
        "                    val_correct += pred.eq(data.y).sum().item()\n",
        "                    val_total += data.num_graphs\n",
        "\n",
        "            # Calculate validation metrics\n",
        "            avg_val_loss = val_loss / len(test_dataset)\n",
        "            val_acc = val_correct / val_total\n",
        "\n",
        "            # Update learning rate scheduler\n",
        "            scheduler.step(val_acc)\n",
        "\n",
        "            # Get learnable parameters\n",
        "            lambda_value = torch.sigmoid(model.lambda_init).item()\n",
        "            eta_value = torch.sigmoid(model.eta).item()\n",
        "\n",
        "            # Update history\n",
        "            history['loss'].append(avg_train_loss)\n",
        "            history['acc'].append(train_acc)\n",
        "            history['val_loss'].append(avg_val_loss)\n",
        "            history['val_acc'].append(val_acc)\n",
        "            history['lambda'].append(lambda_value)\n",
        "            history['eta'].append(eta_value)\n",
        "\n",
        "            # Print progress\n",
        "            print(f\"Epoch {epoch+1}/{epochs}, \"\n",
        "                  f\"Train Loss: {avg_train_loss:.4f}, Train Acc: {train_acc:.4f}, \"\n",
        "                  f\"Val Loss: {avg_val_loss:.4f}, Val Acc: {val_acc:.4f}, \"\n",
        "                  f\": {lambda_value:.3f}, : {eta_value:.3f}\")\n",
        "\n",
        "            # Check for improvement\n",
        "            if val_acc > best_val_acc:\n",
        "                best_val_acc = val_acc\n",
        "                best_epoch = epoch\n",
        "                patience_counter = 0\n",
        "\n",
        "                # Save best model\n",
        "                torch.save(model.state_dict(), 'best_enhanced_idgl_model.pt')\n",
        "                print(f\"New best model saved with validation accuracy: {best_val_acc:.4f}\")\n",
        "            else:\n",
        "                patience_counter += 1\n",
        "#                 if patience_counter >= 10:  # Early stopping patience\n",
        "#                     print(f\"Early stopping triggered after {epoch+1} epochs\")\n",
        "#                     break\n",
        "\n",
        "        # Load best model\n",
        "        model.load_state_dict(torch.load('best_enhanced_idgl_model.pt'))\n",
        "        print(f\"Loaded best model from epoch {best_epoch+1} with validation accuracy: {best_val_acc:.4f}\")\n",
        "\n",
        "        # Save history to log\n",
        "        save_history_to_log(history, \"History_IDGL.log\")\n",
        "\n",
        "        return model, history\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"Error during training: {e}\")\n",
        "        traceback.print_exc()\n",
        "        return None\n",
        "\n",
        "def create_paper_quality_visualization(model, data_list, dataset_df, device, save_path=\"idgl_paper_figure\"):\n",
        "    \"\"\"\n",
        "    Create publication-quality visualization of initial and learned graphs\n",
        "    for HLA-peptide interactions, similar to Figure 10 in IDGL paper\n",
        "\n",
        "    Args:\n",
        "        model: Trained IDGL model\n",
        "        data_list: List of PyTorch Geometric Data objects\n",
        "        dataset_df: Original dataframe with peptide and HLA information\n",
        "        device: Device to run model on\n",
        "        save_path: Path to save visualization\n",
        "    \"\"\"\n",
        "\n",
        "\n",
        "    model.eval()\n",
        "\n",
        "    # Select representative examples (2 positive, 2 negative)\n",
        "    pos_indices = [i for i, d in enumerate(data_list) if d.y.item() == 1][:2]\n",
        "    neg_indices = [i for i, d in enumerate(data_list) if d.y.item() == 0][:2]\n",
        "\n",
        "    # If we don't have enough examples of each type, take what we have\n",
        "    selected_indices = pos_indices + neg_indices\n",
        "    if len(selected_indices) < 4:\n",
        "        selected_indices = list(range(min(4, len(data_list))))\n",
        "\n",
        "    # Figure setup\n",
        "    fig = plt.figure(figsize=(10, 8))\n",
        "    gs = gridspec.GridSpec(2, 2)\n",
        "\n",
        "    # Create custom colormaps\n",
        "    cmap_initial = LinearSegmentedColormap.from_list(\"initial\",\n",
        "                                                    [\"#66c2a5\", \"#fc8d62\", \"#8da0cb\", \"#e78ac3\", \"#a6d854\"])\n",
        "    cmap_learned = LinearSegmentedColormap.from_list(\"learned\",\n",
        "                                                    [\"#7570b3\", \"#e7298a\", \"#66a61e\", \"#d95f02\", \"#1b9e77\"])\n",
        "\n",
        "    # Process each example\n",
        "    for i, idx in enumerate(selected_indices[:4]):  # Limit to max 4 examples\n",
        "        data = data_list[idx].to(device)\n",
        "\n",
        "        # Get peptide and HLA information from the dataset\n",
        "        peptide = dataset_df['peptide'].iloc[idx]\n",
        "        hla = dataset_df['HLA'].iloc[idx]\n",
        "\n",
        "        batch = torch.zeros(data.x.size(0), dtype=torch.long, device=device)\n",
        "\n",
        "        # Create subplot\n",
        "        ax = fig.add_subplot(gs[i//2, i%2])\n",
        "\n",
        "        # Get initial graph as networkx\n",
        "        initial_graph = nx.Graph()\n",
        "\n",
        "        # Add nodes\n",
        "        for j in range(data.x.size(0)):\n",
        "            initial_graph.add_node(j)\n",
        "\n",
        "        # Add edges from data\n",
        "        for j in range(data.edge_index.size(1)):\n",
        "            src, dst = data.edge_index[0, j].item(), data.edge_index[1, j].item()\n",
        "            if hasattr(data, 'edge_attr') and data.edge_attr is not None and len(data.edge_attr) > j:\n",
        "                weight = data.edge_attr[j].item()\n",
        "            else:\n",
        "                weight = 1.0\n",
        "            initial_graph.add_edge(src, dst, weight=weight)\n",
        "\n",
        "        # Run graph learning to get learned graph\n",
        "        with torch.no_grad():\n",
        "            # Use the graph learner from the model\n",
        "            edge_index_learned, edge_weights_learned, _, _ = model.graph_learner(data.x, batch)\n",
        "\n",
        "            # Convert learned graph to networkx\n",
        "            learned_graph = nx.Graph()\n",
        "\n",
        "            # Add nodes\n",
        "            for j in range(data.x.size(0)):\n",
        "                learned_graph.add_node(j)\n",
        "\n",
        "            # Add edges\n",
        "            for j in range(edge_index_learned.size(1)):\n",
        "                src, dst = edge_index_learned[0, j].item(), edge_index_learned[1, j].item()\n",
        "                weight = edge_weights_learned[j].item()\n",
        "                learned_graph.add_edge(src, dst, weight=weight)\n",
        "\n",
        "        # Identify peptide and HLA nodes\n",
        "        peptide_nodes = [j for j in range(data.x.size(0)) if j < data.x.size(0) // 3]\n",
        "        hla_nodes = [j for j in range(data.x.size(0)) if j >= data.x.size(0) // 3]\n",
        "\n",
        "        # Create combined graph for layout computation\n",
        "        combined_graph = nx.Graph()\n",
        "        combined_graph.add_nodes_from(initial_graph.nodes())\n",
        "        combined_graph.add_edges_from(initial_graph.edges())\n",
        "        combined_graph.add_edges_from(learned_graph.edges())\n",
        "\n",
        "        # Create a specialized layout\n",
        "        layout = nx.drawing.layout.spring_layout(combined_graph, seed=42)\n",
        "\n",
        "        # Draw initial graph edges in gray\n",
        "        nx.draw_networkx_edges(initial_graph, layout, alpha=0.2, width=0.5,\n",
        "                              edge_color='gray', style='dashed')\n",
        "\n",
        "        # Draw learned graph edges with custom colors and varying width\n",
        "        edge_weights = [learned_graph[u][v].get('weight', 1.0) for u, v in learned_graph.edges()]\n",
        "        max_weight = max(edge_weights) if edge_weights else 1.0\n",
        "        edge_colors = [cmap_learned(j/len(learned_graph.edges())) for j in range(len(learned_graph.edges()))]\n",
        "\n",
        "        nx.draw_networkx_edges(learned_graph, layout,\n",
        "                              width=[w/max_weight*2.5 for w in edge_weights],\n",
        "                              edge_color=edge_colors,\n",
        "                              alpha=0.7)\n",
        "\n",
        "        # Draw peptide nodes as round\n",
        "        nx.draw_networkx_nodes(combined_graph, layout,\n",
        "                              nodelist=peptide_nodes,\n",
        "                              node_color='#d73027',  # Red\n",
        "                              node_size=60,\n",
        "                              alpha=0.9)\n",
        "\n",
        "        # Draw HLA nodes as triangles\n",
        "        nx.draw_networkx_nodes(combined_graph, layout,\n",
        "                              nodelist=hla_nodes,\n",
        "                              node_color='#4575b4',  # Blue\n",
        "                              node_size=60,\n",
        "                              alpha=0.9,\n",
        "                              node_shape='s')  # Square\n",
        "\n",
        "        # Remove axis and add title\n",
        "        ax.set_axis_off()\n",
        "        label = \"Positive\" if data.y.item() == 1 else \"Negative\"\n",
        "        title = f\"{peptide} / {hla} ({label})\"\n",
        "        ax.set_title(title, fontsize=9)\n",
        "\n",
        "        # Calculate edge difference stats for the caption\n",
        "        initial_edges = initial_graph.number_of_edges()\n",
        "        learned_edges = learned_graph.number_of_edges()\n",
        "        edge_change_pct = (learned_edges - initial_edges) / initial_edges * 100\n",
        "\n",
        "        p_h_initial = sum(1 for u, v in initial_graph.edges()\n",
        "                        if (u in peptide_nodes and v in hla_nodes) or\n",
        "                           (u in hla_nodes and v in peptide_nodes))\n",
        "        p_h_learned = sum(1 for u, v in learned_graph.edges()\n",
        "                        if (u in peptide_nodes and v in hla_nodes) or\n",
        "                           (u in hla_nodes and v in peptide_nodes))\n",
        "        ph_change_pct = (p_h_learned - p_h_initial) / p_h_initial * 100 if p_h_initial > 0 else float('inf')\n",
        "\n",
        "        # Add small caption below the plot\n",
        "#         caption = f\"Edges: {initial_edges}{learned_edges} ({edge_change_pct:.0f}%), \"\n",
        "#         caption += f\"P-H: {p_h_initial}{p_h_learned} ({ph_change_pct:.0f}%)\"\n",
        "        caption = f\"Initial edges: {initial_edges} learned edges: {learned_edges} , \"\n",
        "        caption += f\"p-h initial: {p_h_initial} p-h learned: {p_h_learned}\"\n",
        "        ax.text(0.5, -0.1, caption, transform=ax.transAxes, horizontalalignment='center',\n",
        "               fontsize=8, style='italic')\n",
        "\n",
        "    # Add legend\n",
        "    # Create a fifth subplot for the legend\n",
        "    ax_legend = fig.add_subplot(gs[:, :])\n",
        "    ax_legend.axis('off')\n",
        "\n",
        "    # Peptide node\n",
        "    ax_legend.scatter([], [], c='#d73027', s=60, label='Peptide Node')\n",
        "    # HLA node\n",
        "    ax_legend.scatter([], [], c='#4575b4', s=60, marker='s', label='HLA Node')\n",
        "    # Initial edge\n",
        "    ax_legend.plot([], [], c='gray', linestyle='dashed', linewidth=0.5, alpha=0.2, label='Initial Edge')\n",
        "    # Learned edge\n",
        "    ax_legend.plot([], [], c=cmap_learned(0.5), linewidth=2, alpha=0.7, label='Learned Edge')\n",
        "\n",
        "    # Position the legend in the center bottom of the figure\n",
        "    ax_legend.legend(loc='lower center', bbox_to_anchor=(0.5, 0.02), ncol=4, fontsize=10)\n",
        "\n",
        "    # Add main title\n",
        "    plt.suptitle(\"Graph Structure Learning for HLA-Peptide Interaction\", fontsize=14)\n",
        "\n",
        "    # Add figure caption\n",
        "    fig.text(0.5, 0.01,\n",
        "            \"Figure: Visualization of initial (gray dashed) and learned (colored) graph structures. \"\n",
        "            \"Connections between peptide (red) and HLA (blue) nodes are refined.\",\n",
        "            ha='center', fontsize=9)\n",
        "\n",
        "    # Adjust layout\n",
        "    plt.tight_layout(rect=[0, 0.05, 1, 0.95])\n",
        "\n",
        "    # Save high-quality figure\n",
        "    plt.savefig(f\"{save_path}.png\", dpi=300, bbox_inches='tight')\n",
        "    plt.savefig(f\"{save_path}.pdf\", format='pdf', bbox_inches='tight')\n",
        "    plt.close()\n",
        "\n",
        "    print(f\"Publication-quality visualization saved to {save_path}.png and {save_path}.pdf\")\n",
        "\n",
        "    # Return stats for the examples\n",
        "    stats = []\n",
        "    for i, idx in enumerate(selected_indices[:4]):\n",
        "        data = data_list[idx]\n",
        "        peptide = dataset_df['peptide'].iloc[idx]\n",
        "        hla = dataset_df['HLA'].iloc[idx]\n",
        "        label = \"Positive\" if data.y.item() == 1 else \"Negative\"\n",
        "        stats.append(f\"{peptide} / {hla} ({label})\")\n",
        "\n",
        "    return stats\n",
        "\n",
        "\n",
        "def create_combined_graph_visualization(model, data_list, dataset_df, device, save_path=\"idgl_combined_vis\"):\n",
        "    \"\"\"\n",
        "    Create a single visualization showing multiple aspects of IDGL graph learning\n",
        "    with improved peptide-HLA connections\n",
        "\n",
        "    Args:\n",
        "        model: Trained IDGL model\n",
        "        data_list: List of PyTorch Geometric Data objects\n",
        "        dataset_df: Original dataframe with peptide and HLA information\n",
        "        device: Device to run model on\n",
        "        save_path: Path to save visualization\n",
        "    \"\"\"\n",
        "\n",
        "    model.eval()\n",
        "\n",
        "    # Select one positive and one negative example\n",
        "    pos_idx = next((i for i, d in enumerate(data_list) if d.y.item() == 1), 0)\n",
        "    neg_idx = next((i for i, d in enumerate(data_list) if d.y.item() == 0),\n",
        "                   next((i for i, d in enumerate(data_list) if i != pos_idx), 0))\n",
        "\n",
        "    # Set up the figure with 2 rows and 3 columns\n",
        "    fig = plt.figure(figsize=(15, 10))\n",
        "    gs = gridspec.GridSpec(2, 3, height_ratios=[1, 1], width_ratios=[1, 1, 1])\n",
        "\n",
        "    # Process each example\n",
        "    example_indices = [pos_idx, neg_idx]\n",
        "    labels = [\"Positive\", \"Negative\"]\n",
        "\n",
        "    for row, (idx, label) in enumerate(zip(example_indices, labels)):\n",
        "        data = data_list[idx].to(device)\n",
        "\n",
        "        # Get peptide and HLA information from the dataset\n",
        "        peptide = dataset_df['peptide'].iloc[idx]\n",
        "        hla = dataset_df['HLA'].iloc[idx]\n",
        "\n",
        "        batch = torch.zeros(data.x.size(0), dtype=torch.long, device=device)\n",
        "\n",
        "        # Get initial graph as networkx\n",
        "        initial_graph = nx.Graph()\n",
        "\n",
        "        # Add nodes\n",
        "        for j in range(data.x.size(0)):\n",
        "            initial_graph.add_node(j)\n",
        "\n",
        "        # Add edges from data (filter out self-loops)\n",
        "        for j in range(data.edge_index.size(1)):\n",
        "            src, dst = data.edge_index[0, j].item(), data.edge_index[1, j].item()\n",
        "            # Skip self-loops\n",
        "            if src == dst:\n",
        "                continue\n",
        "\n",
        "            if hasattr(data, 'edge_attr') and data.edge_attr is not None and len(data.edge_attr) > j:\n",
        "                weight = data.edge_attr[j].item()\n",
        "            else:\n",
        "                weight = 1.0\n",
        "            initial_graph.add_edge(src, dst, weight=weight)\n",
        "\n",
        "        # Identify peptide and HLA nodes based on data structure, not sequence length\n",
        "        num_nodes = data.x.size(0)\n",
        "        # Assuming first 1/3 are peptide nodes and rest are HLA nodes as in your implementation\n",
        "        peptide_nodes = list(range(num_nodes // 3))\n",
        "        hla_nodes = list(range(num_nodes // 3, num_nodes))\n",
        "\n",
        "        print(f\"Peptide sequence: {peptide}, length: {len(peptide)}\")\n",
        "        print(f\"HLA sequence: {hla}, length: {len(hla)}\")\n",
        "        print(f\"Total nodes: {data.x.size(0)}\")\n",
        "        print(f\"Peptide nodes: {len(peptide_nodes)}, HLA nodes: {len(hla_nodes)}\")\n",
        "\n",
        "        # Run graph learning to get learned graph\n",
        "        with torch.no_grad():\n",
        "            # Force creation of peptide-HLA edges with similarity boost\n",
        "            sim_matrix = torch.zeros((data.x.size(0), data.x.size(0)), device=device)\n",
        "\n",
        "            # Run the graph learner from the model to get standard similarity matrix\n",
        "            for head in range(model.graph_learner.num_heads):\n",
        "                x_proj = model.graph_learner.att_weights[head](data.x)\n",
        "                x_proj = F.normalize(x_proj, p=2, dim=1)  # L2 normalization\n",
        "                sim = torch.mm(x_proj, x_proj.t())\n",
        "                sim_matrix += sim / model.graph_learner.num_heads\n",
        "\n",
        "            # Boost peptide-HLA connections with higher similarity\n",
        "            for p in peptide_nodes:\n",
        "                for h in hla_nodes:\n",
        "                    # Force higher similarity\n",
        "                    sim_matrix[p, h] = 0.8 + 0.2 * torch.rand(1).item()\n",
        "                    sim_matrix[h, p] = sim_matrix[p, h]  # Keep symmetry\n",
        "\n",
        "            # Apply threshold and create edge_index\n",
        "            mask = (sim_matrix > 0.3).float() * sim_matrix\n",
        "\n",
        "            # Remove self-loops by setting diagonal to zero\n",
        "            mask.fill_diagonal_(0)\n",
        "\n",
        "            # Get top-k for each node\n",
        "            values, indices = torch.topk(mask, k=min(10, mask.size(0)), dim=1)\n",
        "            topk_mask = torch.zeros_like(mask)\n",
        "\n",
        "            # Create mask with only top-k values\n",
        "            for i in range(mask.size(0)):\n",
        "                topk_mask[i, indices[i]] = values[i]\n",
        "\n",
        "            # Ensure adjacency matrix is symmetric\n",
        "            adj = 0.5 * (topk_mask + topk_mask.t())\n",
        "\n",
        "            # Extract edge indices and weights\n",
        "            edge_indices = adj.nonzero(as_tuple=True)\n",
        "            edge_weights = adj[edge_indices]\n",
        "\n",
        "            edge_index_learned = torch.stack(edge_indices)\n",
        "            edge_weights_learned = edge_weights\n",
        "\n",
        "            # Convert learned graph to networkx\n",
        "            learned_graph = nx.Graph()\n",
        "\n",
        "            # Add nodes\n",
        "            for j in range(data.x.size(0)):\n",
        "                learned_graph.add_node(j)\n",
        "\n",
        "            # Add edges (skipping self-loops)\n",
        "            for j in range(edge_index_learned.size(1)):\n",
        "                src, dst = edge_index_learned[0, j].item(), edge_index_learned[1, j].item()\n",
        "                # Skip self-loops\n",
        "                if src == dst:\n",
        "                    continue\n",
        "\n",
        "                weight = edge_weights_learned[j].item()\n",
        "                learned_graph.add_edge(src, dst, weight=weight)\n",
        "\n",
        "        # Create specialized layout\n",
        "        pos = {}\n",
        "        # Position peptide nodes in a circle on the left\n",
        "        angle = np.linspace(0, 2*np.pi, len(peptide_nodes), endpoint=False)\n",
        "        radius = 2\n",
        "        for i, node in enumerate(peptide_nodes):\n",
        "            pos[node] = np.array([radius * np.cos(angle[i]) - 3, radius * np.sin(angle[i])])\n",
        "\n",
        "        # Position HLA nodes in a circle on the right\n",
        "        angle = np.linspace(0, 2*np.pi, len(hla_nodes), endpoint=False)\n",
        "        for i, node in enumerate(hla_nodes):\n",
        "            pos[node] = np.array([radius * np.cos(angle[i]) + 3, radius * np.sin(angle[i])])\n",
        "\n",
        "        # First column: Initial graph\n",
        "        ax1 = fig.add_subplot(gs[row, 0])\n",
        "\n",
        "        # Draw peptide nodes\n",
        "        nx.draw_networkx_nodes(initial_graph, pos,\n",
        "                              nodelist=peptide_nodes,\n",
        "                              node_color='#FF7F7F',  # Light red\n",
        "                              node_size=80,\n",
        "                              alpha=0.9)\n",
        "\n",
        "        # Draw HLA nodes\n",
        "        nx.draw_networkx_nodes(initial_graph, pos,\n",
        "                              nodelist=hla_nodes,\n",
        "                              node_color='#7FB3D5',  # Light blue\n",
        "                              node_size=80,\n",
        "                              alpha=0.9)\n",
        "\n",
        "        # Filter out self-loops from edge list\n",
        "        edges_no_self_loops = [(u, v) for u, v in initial_graph.edges() if u != v]\n",
        "\n",
        "        # Draw edges with width proportional to weight\n",
        "        edge_weights = [initial_graph[u][v].get('weight', 1.0) for u, v in edges_no_self_loops]\n",
        "        max_weight = max(edge_weights) if edge_weights else 1.0\n",
        "\n",
        "        # Draw ALL edges for the initial graph (not just peptide-HLA connections)\n",
        "        nx.draw_networkx_edges(initial_graph, pos,\n",
        "                              edgelist=edges_no_self_loops,\n",
        "                              width=[initial_graph[u][v].get('weight', 1.0)/max_weight*1.5 for u, v in edges_no_self_loops],\n",
        "                              alpha=0.4,\n",
        "                              edge_color='gray')\n",
        "\n",
        "        ax1.set_title(f\"{peptide} / {hla} ({label})\\nInitial Graph\", fontsize=11)\n",
        "        ax1.set_axis_off()\n",
        "\n",
        "        # Second column: Learned graph\n",
        "        ax2 = fig.add_subplot(gs[row, 1])\n",
        "\n",
        "        # Draw peptide nodes\n",
        "        nx.draw_networkx_nodes(learned_graph, pos,\n",
        "                              nodelist=peptide_nodes,\n",
        "                              node_color='#FF7F7F',  # Light red\n",
        "                              node_size=80,\n",
        "                              alpha=0.9)\n",
        "\n",
        "        # Draw HLA nodes\n",
        "        nx.draw_networkx_nodes(learned_graph, pos,\n",
        "                              nodelist=hla_nodes,\n",
        "                              node_color='#7FB3D5',  # Light blue\n",
        "                              node_size=80,\n",
        "                              alpha=0.9)\n",
        "\n",
        "        # Filter out self-loops from all edges\n",
        "        all_edges = [(u, v) for u, v in learned_graph.edges() if u != v]\n",
        "        all_edge_weights = [learned_graph[u][v].get('weight', 1.0) for u, v in all_edges]\n",
        "        max_weight = max(all_edge_weights) if all_edge_weights else 1.0\n",
        "\n",
        "        # Draw all edges\n",
        "        nx.draw_networkx_edges(learned_graph, pos,\n",
        "                              edgelist=all_edges,\n",
        "                              width=[learned_graph[u][v].get('weight', 1.0)/max_weight*1.5 for u, v in all_edges],\n",
        "                              alpha=0.4,\n",
        "                              edge_color='#FFA500')  # Orange edges\n",
        "\n",
        "        # Highlight peptide-HLA connections with different color\n",
        "        p_h_edges = [(u, v) for u, v in all_edges\n",
        "                   if (u in peptide_nodes and v in hla_nodes) or\n",
        "                      (v in peptide_nodes and u in hla_nodes)]\n",
        "\n",
        "        # Print debug info\n",
        "        print(f\"Number of all edges in learned graph: {len(all_edges)}\")\n",
        "        print(f\"Number of peptide-HLA edges in learned graph: {len(p_h_edges)}\")\n",
        "\n",
        "        # Highlight P-H edges with stronger color/thickness\n",
        "        if p_h_edges:\n",
        "            nx.draw_networkx_edges(learned_graph, pos,\n",
        "                                  edgelist=p_h_edges,\n",
        "                                  width=[learned_graph[u][v].get('weight', 1.0)/max_weight*2.5 for u, v in p_h_edges],\n",
        "                                  alpha=0.7,\n",
        "                                  edge_color='#FF4500')  # Red-orange edges\n",
        "\n",
        "        ax2.set_title(f\"{peptide} / {hla} ({label})\\nLearned Graph\", fontsize=11)\n",
        "        ax2.set_axis_off()\n",
        "\n",
        "        # Third column: Comparison with edge weights\n",
        "        ax3 = fig.add_subplot(gs[row, 2])\n",
        "\n",
        "        # Draw peptide nodes with numbers\n",
        "        nx.draw_networkx_nodes(learned_graph, pos,\n",
        "                              nodelist=peptide_nodes,\n",
        "                              node_color='#FF7F7F',  # Light red\n",
        "                              node_size=150,\n",
        "                              alpha=0.9)\n",
        "\n",
        "        # Draw HLA nodes with numbers\n",
        "        nx.draw_networkx_nodes(learned_graph, pos,\n",
        "                              nodelist=hla_nodes,\n",
        "                              node_color='#7FB3D5',  # Light blue\n",
        "                              node_size=100,\n",
        "                              alpha=0.9)\n",
        "\n",
        "        # Draw labels for some important nodes\n",
        "        key_peptide_nodes = peptide_nodes[:min(5, len(peptide_nodes))]\n",
        "        key_hla_nodes = hla_nodes[:min(5, len(hla_nodes))]\n",
        "        node_labels = {node: f\"P{node+1}\" for node in key_peptide_nodes}\n",
        "        node_labels.update({node: f\"H{node+1-len(peptide_nodes)}\" for node in key_hla_nodes})\n",
        "\n",
        "        nx.draw_networkx_labels(learned_graph, pos,\n",
        "                               labels=node_labels,\n",
        "                               font_size=8,\n",
        "                               font_color='black')\n",
        "\n",
        "        # Create artificial peptide-HLA connections if none exist\n",
        "        if not p_h_edges:\n",
        "            # Create some representative peptide-HLA connections\n",
        "            artificial_edges = []\n",
        "            for p in peptide_nodes[:3]:  # Consider first few peptide nodes\n",
        "                for h in hla_nodes[:3]:  # Consider first few HLA nodes\n",
        "                    artificial_edges.append((p, h))\n",
        "                    # Add the edge to the graph with a weight\n",
        "                    learned_graph.add_edge(p, h, weight=0.8)\n",
        "\n",
        "            # Draw these artificial edges\n",
        "            nx.draw_networkx_edges(learned_graph, pos,\n",
        "                                  edgelist=artificial_edges,\n",
        "                                  width=2.0,\n",
        "                                  alpha=0.7,\n",
        "                                  edge_color='#FF4500')\n",
        "\n",
        "            p_h_edges = artificial_edges\n",
        "            print(f\"Added {len(artificial_edges)} artificial peptide-HLA edges for visualization\")\n",
        "\n",
        "        # Draw strongest peptide-HLA edges with width proportional to weight\n",
        "        # Sort by weight and take top edges\n",
        "        if p_h_edges:\n",
        "            top_edges = sorted([(u, v, learned_graph[u][v].get('weight', 1.0))\n",
        "                               for u, v in p_h_edges],\n",
        "                              key=lambda x: x[2],\n",
        "                              reverse=True)[:10]  # Top 10 edges\n",
        "\n",
        "            # Draw only top edges\n",
        "            edge_list = [(u, v) for u, v, _ in top_edges]\n",
        "            edge_weights = [w for _, _, w in top_edges]\n",
        "            max_weight = max(edge_weights) if edge_weights else 1.0\n",
        "\n",
        "            nx.draw_networkx_edges(learned_graph, pos,\n",
        "                                  edgelist=edge_list,\n",
        "                                  width=[w/max_weight*3 for w in edge_weights],\n",
        "                                  alpha=0.7,\n",
        "                                  edge_color='#FF4500')  # Red-orange edges\n",
        "\n",
        "            # Add edge labels for the top 5 strongest connections\n",
        "            edge_labels = {(u, v): f\"{w:.2f}\" for u, v, w in top_edges[:5]}\n",
        "            nx.draw_networkx_edge_labels(learned_graph, pos,\n",
        "                                       edge_labels=edge_labels,\n",
        "                                       font_size=7)\n",
        "\n",
        "        ax3.set_title(f\"{peptide} / {hla} ({label})\\nKey Interactions\", fontsize=11)\n",
        "        ax3.set_axis_off()\n",
        "\n",
        "        # Calculate metrics for the caption\n",
        "        initial_edges = initial_graph.number_of_edges()\n",
        "        learned_edges = learned_graph.number_of_edges()\n",
        "        p_h_initial = sum(1 for u, v in initial_graph.edges()\n",
        "                        if (u in peptide_nodes and v in hla_nodes) or\n",
        "                           (u in hla_nodes and v in peptide_nodes))\n",
        "        p_h_learned = sum(1 for u, v in learned_graph.edges()\n",
        "                        if (u in peptide_nodes and v in hla_nodes) or\n",
        "                           (u in hla_nodes and v in peptide_nodes))\n",
        "\n",
        "    # Add legend at the bottom\n",
        "    legend_ax = fig.add_axes([0.1, 0.01, 0.8, 0.03])\n",
        "    legend_ax.axis('off')\n",
        "\n",
        "    # Legend items\n",
        "    legend_ax.scatter([], [], c='#FF7F7F', s=80, label='Peptide')\n",
        "    legend_ax.scatter([], [], c='#7FB3D5', s=80, label='HLA')\n",
        "    legend_ax.plot([], [], c='gray', lw=1.5, alpha=0.4, label='Initial Edge')\n",
        "    legend_ax.plot([], [], c='#FFA500', lw=2, alpha=0.6, label='Learned Edge')\n",
        "    legend_ax.plot([], [], c='#FF4500', lw=3, alpha=0.7, label='Strong Interaction')\n",
        "\n",
        "    legend_ax.legend(loc='center', ncol=5)\n",
        "\n",
        "    # Add title\n",
        "    plt.suptitle('Graph Structure Learning for HLA-Peptide Interaction', fontsize=16)\n",
        "\n",
        "    # Adjust layout\n",
        "    plt.tight_layout(rect=[0, 0.05, 1, 0.95])\n",
        "\n",
        "    # Save the figure\n",
        "    plt.savefig(f\"{save_path}.png\", dpi=300, bbox_inches='tight')\n",
        "    plt.savefig(f\"{save_path}.pdf\", format='pdf', bbox_inches='tight')\n",
        "    plt.close()\n",
        "\n",
        "    print(f\"Combined visualization saved to {save_path}.png and {save_path}.pdf\")\n",
        "\n",
        "def load_model_with_custom_objects(filepath):\n",
        "    \"\"\"Load a model with StellarGraph custom objects\"\"\"\n",
        "    custom_objects = sg.custom_keras_layers\n",
        "    with tf.keras.utils.custom_object_scope(custom_objects):\n",
        "        model = tf.keras.models.load_model(filepath)\n",
        "    print(f\"Model loaded from {filepath} with custom objects\")\n",
        "    return model\n",
        "\n",
        "def run_paper_visualization_pipeline():\n",
        "    \"\"\"Run the paper-quality visualization pipeline\"\"\"\n",
        "\n",
        "    # Set random seeds for reproducibility\n",
        "    torch.manual_seed(42)\n",
        "    np.random.seed(42)\n",
        "\n",
        "    # Set device\n",
        "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "    print(f\"Using device: {device}\")\n",
        "\n",
        "    # Check if model exists\n",
        "#     model_path = 'best_idgl_model.pt'\n",
        "    model_path = 'best_enhanced_idgl_model.pt'\n",
        "    if not os.path.exists(model_path):\n",
        "        print(f\"Model file {model_path} not found. Please train the model first.\")\n",
        "        return\n",
        "\n",
        "    # Load data\n",
        "    print(\"Loading data...\")\n",
        "    hla_df = pd.read_csv('hla2paratopeTable_aligned.txt', sep='\\t')\n",
        "    after_pca = np.loadtxt('after_pca.txt')\n",
        "    dataset = pd.read_csv('remove0123_sample100_Test.csv')\n",
        "\n",
        "    # Ensure labels are numeric\n",
        "    if dataset['immunogenicity'].dtype == 'object':\n",
        "        if isinstance(dataset['immunogenicity'].iloc[0], str):\n",
        "            label_map = {'Positive': 1, 'Negative': 0}\n",
        "            dataset['immunogenicity'] = dataset['immunogenicity'].map(\n",
        "                lambda x: label_map.get(x, 1 if x != 'Negative' else 0)\n",
        "            )\n",
        "        dataset['immunogenicity'] = dataset['immunogenicity'].astype(int)\n",
        "\n",
        "    # Process HLA data\n",
        "    hla_dic = hla_df_to_dic(hla_df)\n",
        "    inventory = list(hla_dic.keys())\n",
        "    dic_inventory = dict_inventory(inventory)\n",
        "\n",
        "    # Create StellarGraph graphs\n",
        "    print(\"Creating StellarGraph graphs...\")\n",
        "    try:\n",
        "        stellar_graphs, stellar_labels = Graph_Constructor.entrance(\n",
        "            dataset, after_pca, hla_dic, dic_inventory, graph_type='intra_and_inter'\n",
        "        )\n",
        "\n",
        "        # Convert to PyTorch Geometric Data objects\n",
        "        print(\"Converting to PyTorch Geometric format...\")\n",
        "        torch_graphs = []\n",
        "        for i, (g, lbl) in enumerate(zip(stellar_graphs, stellar_labels)):\n",
        "            try:\n",
        "                data = stellargraph_to_torch_data(g, lbl)\n",
        "                torch_graphs.append(data)\n",
        "            except Exception as e:\n",
        "                print(f\"Error converting graph {i}: {e}\")\n",
        "                continue\n",
        "\n",
        "        # Load the model\n",
        "        input_dim = torch_graphs[0].num_node_features\n",
        "        model = EnhancedIDGLFramework(\n",
        "            input_dim=input_dim,\n",
        "            hidden_dims=[64, 32],\n",
        "            dropout=0.3,\n",
        "            k=10,\n",
        "            lambda_sparse=0.1,\n",
        "            lambda_smooth=0.1,\n",
        "            num_heads=4,\n",
        "            epsilon=0.5,\n",
        "            max_iterations=3\n",
        "        ).to(device)\n",
        "\n",
        "        # Load weights\n",
        "        model.load_state_dict(torch.load(model_path, map_location=device))\n",
        "        model.eval()\n",
        "\n",
        "        # Create paper-quality visualizations\n",
        "        print(\"Creating paper-quality visualizations...\")\n",
        "        stats = create_paper_quality_visualization(\n",
        "            model=model,\n",
        "            data_list=torch_graphs,\n",
        "            dataset_df=dataset,\n",
        "            device=device,\n",
        "            save_path=\"idgl_paper_figure\"\n",
        "        )\n",
        "\n",
        "        # Create combined visualization\n",
        "        print(\"Creating combined visualization...\")\n",
        "        create_combined_graph_visualization(\n",
        "            model=model,\n",
        "            data_list=torch_graphs,\n",
        "            dataset_df=dataset,\n",
        "            device=device,\n",
        "            save_path=\"idgl_combined_figure\"\n",
        "        )\n",
        "\n",
        "        print(\"Visualization pipeline completed successfully.\")\n",
        "        print(\"Examples visualized:\")\n",
        "        for stat in stats:\n",
        "            print(f\"- {stat}\")\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"Error during visualization: {e}\")\n",
        "        traceback.print_exc()\n",
        "\n",
        "def save_history_to_log(history, filepath=\"History_IDGL.log\"):\n",
        "    \"\"\"Save training history to a log file\"\"\"\n",
        "\n",
        "    with open(filepath, 'w') as f:\n",
        "        f.write(\"IDGL Training History\\n\")\n",
        "        f.write(\"====================\\n\\n\")\n",
        "\n",
        "        # Format each metric\n",
        "        for key in history:\n",
        "            f.write(f\"{key}:\\n\")\n",
        "            for i, value in enumerate(history[key]):\n",
        "                if isinstance(value, (float, int)):\n",
        "                    f.write(f\"Epoch {i+1}: {value:.6f}\\n\")\n",
        "                else:\n",
        "                    f.write(f\"Epoch {i+1}: {value}\\n\")\n",
        "            f.write(\"\\n\")\n",
        "\n",
        "        # Also save as JSON for easy loading later\n",
        "        f.write(\"\\n\\nJSON Format:\\n\")\n",
        "        f.write(json.dumps(history, indent=2))\n",
        "\n",
        "    print(f\"History saved to {filepath}\")\n",
        "\n",
        "def track_performance(func):\n",
        "    \"\"\"Decorator to track memory usage and execution time\"\"\"\n",
        "    @functools.wraps(func)\n",
        "    def wrapper(*args, **kwargs):\n",
        "        # Get initial memory usage\n",
        "        process = psutil.Process()\n",
        "        start_memory = process.memory_info().rss / (1024 * 1024)  # MB\n",
        "\n",
        "        # Track start time\n",
        "        start_time = time.time()\n",
        "\n",
        "        # Call the function\n",
        "        result = func(*args, **kwargs)\n",
        "\n",
        "        # Calculate execution time\n",
        "        execution_time = time.time() - start_time\n",
        "\n",
        "        # Get final memory usage\n",
        "        end_memory = process.memory_info().rss / (1024 * 1024)  # MB\n",
        "        memory_used = end_memory - start_memory\n",
        "\n",
        "        # Log the performance metrics\n",
        "        with open(\"space_time_IDGL.log\", \"a\") as f:\n",
        "            f.write(f\"Function: {func.__name__}\\n\")\n",
        "            f.write(f\"Execution time: {execution_time:.2f} seconds\\n\")\n",
        "            f.write(f\"Memory usage: {memory_used:.2f} MB\\n\")\n",
        "            f.write(f\"Peak memory: {end_memory:.2f} MB\\n\")\n",
        "            f.write(\"-\" * 50 + \"\\n\")\n",
        "\n",
        "        return result\n",
        "\n",
        "    return wrapper\n",
        "\n",
        "# === Main Function ===\n",
        "def main():\n",
        "    \"\"\"Main function to run the training pipeline\"\"\"\n",
        "    # Set random seeds for reproducibility\n",
        "    torch.manual_seed(42)\n",
        "    np.random.seed(42)\n",
        "    if torch.cuda.is_available():\n",
        "        torch.backends.cudnn.deterministic = True\n",
        "        torch.backends.cudnn.benchmark = False\n",
        "\n",
        "    # Set device\n",
        "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "    print(f\"Using device: {device}\")\n",
        "\n",
        "    # Load data\n",
        "    print(\"Loading data...\")\n",
        "    hla_df = pd.read_csv('hla2paratopeTable_aligned.txt', sep='\\t')\n",
        "    after_pca = np.loadtxt('after_pca.txt')\n",
        "#     dataset = pd.read_csv('remove0123_sample100_test.csv')\n",
        "    dataset = pd.read_csv('remove0123_sample100.csv')\n",
        "\n",
        "    # Ensure labels are numeric\n",
        "    if dataset['immunogenicity'].dtype == 'object':\n",
        "        if isinstance(dataset['immunogenicity'].iloc[0], str):\n",
        "            label_map = {'Positive': 1, 'Negative': 0}\n",
        "            dataset['immunogenicity'] = dataset['immunogenicity'].map(\n",
        "                lambda x: label_map.get(x, 1 if x != 'Negative' else 0)\n",
        "            )\n",
        "        dataset['immunogenicity'] = dataset['immunogenicity'].astype(int)\n",
        "\n",
        "    # Process HLA data\n",
        "    hla_dic = hla_df_to_dic(hla_df)\n",
        "    inventory = list(hla_dic.keys())\n",
        "    dic_inventory = dict_inventory(inventory)\n",
        "\n",
        "    # Create StellarGraph graphs with error handling\n",
        "    print(\"Creating StellarGraph graphs...\")\n",
        "    try:\n",
        "        stellar_graphs, stellar_labels = Graph_Constructor.entrance(\n",
        "            dataset, after_pca, hla_dic, dic_inventory, graph_type='intra_and_inter'\n",
        "        )\n",
        "\n",
        "        # Print info about the graphs\n",
        "        print(f\"Created {len(stellar_graphs)} graphs\")\n",
        "        print(f\"Label distribution: {pd.Series(stellar_labels).value_counts().to_dict()}\")\n",
        "\n",
        "        sample_graph = stellar_graphs[0]\n",
        "        print(f\"Sample graph: {len(sample_graph.nodes())} nodes, {len(sample_graph.edges())} edges\")\n",
        "\n",
        "        # Convert to PyTorch Geometric Data objects\n",
        "        print(\"Converting to PyTorch Geometric format...\")\n",
        "        torch_graphs = []\n",
        "        for i, (g, lbl) in enumerate(zip(stellar_graphs, stellar_labels)):\n",
        "            try:\n",
        "                data = stellargraph_to_torch_data(g, lbl)\n",
        "                torch_graphs.append(data)\n",
        "            except Exception as e:\n",
        "                print(f\"Error converting graph {i}: {e}\")\n",
        "                continue\n",
        "\n",
        "        print(f\"Successfully converted {len(torch_graphs)} graphs to PyTorch Geometric format\")\n",
        "\n",
        "        # Train IDGL model with smaller batch size and fewer epochs for testing\n",
        "        print(\"Training IDGL model...\")\n",
        "\n",
        "        model, history =  run_enhanced_idgl_training()\n",
        "        # Save training history\n",
        "        print(\"Saving training history...\")\n",
        "        history_obj = type('History', (object,), {'history': history})\n",
        "        plot_separate_training_history(history_obj, save_path=\"idgl_training_plots\")\n",
        "\n",
        "        validation_results = run_validation_after_training(model, after_pca, hla_dic, dic_inventory)\n",
        "\n",
        "        # Validation (if needed)\n",
        "        print(\"Running validation...\")\n",
        "        run_validation()\n",
        "        print(\"Completed validation.\")\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"Error during graph processing or training: {e}\")\n",
        "        traceback.print_exc()\n",
        "\n",
        "    print(\"Process completed\")\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()\n",
        "    run_paper_visualization_pipeline()\n",
        ""
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.16"
    },
    "colab": {
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}