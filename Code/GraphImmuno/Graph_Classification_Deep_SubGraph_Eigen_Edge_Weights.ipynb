{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "J8t2CE-XWf5r"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import numpy as np"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yPiflVrPWosg"
      },
      "outputs": [],
      "source": [
        "!pip install stellargraph"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hnW3b7hzYlnd"
      },
      "outputs": [],
      "source": [
        "!pip install chardet"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OAInvHTmFhal"
      },
      "outputs": [],
      "source": [
        "import stellargraph as sg\n",
        "from stellargraph.mapper import PaddedGraphGenerator\n",
        "from stellargraph.layer import GCNSupervisedGraphClassification\n",
        "from stellargraph import StellarGraph"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9rkZgJjAFiqY"
      },
      "outputs": [],
      "source": [
        "from sklearn import model_selection\n",
        "from sklearn.metrics import confusion_matrix,auc,precision_recall_curve,roc_curve\n",
        "from tensorflow.keras import Model\n",
        "from tensorflow.keras.layers import Dense\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "from tensorflow.keras.losses import binary_crossentropy,mean_squared_error\n",
        "from tensorflow.keras.callbacks import EarlyStopping\n",
        "import tensorflow as tf\n",
        "from stellargraph import IndexedArray"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5KrVFrSEFlXS"
      },
      "outputs": [],
      "source": [
        "import itertools\n",
        "import matplotlib.pyplot as plt"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "roaJbWysFm8A"
      },
      "outputs": [],
      "source": [
        "def read_para(path):\n",
        "    df = pd.read_csv(path,sep='\\t',header=None)\n",
        "    dic = {}\n",
        "    for i in range(df.shape[0]):\n",
        "        hla = df[0].iloc[i]\n",
        "        paratope = df[1].iloc[i]\n",
        "        try:\n",
        "            dic[hla] = paratope\n",
        "        except KeyError:\n",
        "            dic[hla] = []\n",
        "            dic[hla].append(paratope)\n",
        "    return dic"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "iJPLF2XGFoWk"
      },
      "outputs": [],
      "source": [
        "def dict_inventory(inventory):\n",
        "    dicA, dicB, dicC = {}, {}, {}\n",
        "    dic = {'A': dicA, 'B': dicB, 'C': dicC}\n",
        "\n",
        "    for hla in inventory:\n",
        "        type_ = hla[4]  # A,B,C\n",
        "        first2 = hla[6:8]  # 01\n",
        "        last2 = hla[8:]  # 01\n",
        "        try:\n",
        "            dic[type_][first2].append(last2)\n",
        "        except KeyError:\n",
        "            dic[type_][first2] = []\n",
        "            dic[type_][first2].append(last2)\n",
        "\n",
        "    return dic"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LNlXP2GHFrMK"
      },
      "outputs": [],
      "source": [
        "def rescue_unknown_hla(hla, dic_inventory):\n",
        "    type_ = hla[4]\n",
        "    first2 = hla[6:8]\n",
        "    last2 = hla[8:]\n",
        "    big_category = dic_inventory[type_]\n",
        "    #print(hla)\n",
        "    if not big_category.get(first2) == None:\n",
        "        small_category = big_category.get(first2)\n",
        "        distance = [abs(int(last2) - int(i)) for i in small_category]\n",
        "        optimal = min(zip(small_category, distance), key=lambda x: x[1])[0]\n",
        "        return 'HLA-' + str(type_) + '*' + str(first2) + str(optimal)\n",
        "    else:\n",
        "        small_category = list(big_category.keys())\n",
        "        distance = [abs(int(first2) - int(i)) for i in small_category]\n",
        "        optimal = min(zip(small_category, distance), key=lambda x: x[1])[0]\n",
        "        return 'HLA-' + str(type_) + '*' + str(optimal) + str(big_category[optimal][0])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lTDefDkrFtZc"
      },
      "outputs": [],
      "source": [
        "class Graph_Constructor():\n",
        "\n",
        "    @staticmethod\n",
        "    def combinator(pep,hla):\n",
        "        source = ['p' + str(i+1) for i in range(len(pep))]\n",
        "        target = ['h' + str(i+1) for i in range(len(hla))]\n",
        "        return source,target\n",
        "\n",
        "    @staticmethod\n",
        "    def numerical(pep,hla,after_pca,embed=12):   # after_pca [21,12]\n",
        "        pep = pep.replace('X','-').upper()\n",
        "        hla = hla.replace('X','-').upper()\n",
        "        feature_array_pep = np.empty([len(pep),embed])\n",
        "        feature_array_hla = np.empty([len(hla),embed])\n",
        "        amino = 'ARNDCQEGHILKMFPSTWYV-'\n",
        "        for i in range(len(pep)):\n",
        "            feature_array_pep[i,:] = after_pca[amino.index(pep[i]),:]\n",
        "        for i in range(len(hla)):\n",
        "            feature_array_hla[i,:] = after_pca[amino.index(hla[i]),:]\n",
        "        feature_array = np.concatenate([feature_array_pep,feature_array_hla],axis=0)\n",
        "        #print(feature_array_pep.shape,feature_array_hla.shape,feature_array.shape)\n",
        "        return feature_array\n",
        "\n",
        "\n",
        "\n",
        "    # define the function to convert the weight column\n",
        "    @staticmethod\n",
        "    def DHT_numerical(weight, feature_array):\n",
        "        if weight[0] == 'p':\n",
        "            idx = int(weight[1:]) - 1\n",
        "            return feature_array[idx]\n",
        "        elif weight[0] == 'h':\n",
        "            idx = int(weight[1:]) + 8\n",
        "            return feature_array[idx]\n",
        "        else:\n",
        "            return None\n",
        "\n",
        "    # define the function to Split the features values(orginal node features)\n",
        "    @staticmethod\n",
        "    def Split_Weight(row):\n",
        "        #return pd.Series(row['weight'].strip('[]').split())\n",
        "        return pd.Series(row['weight'].tolist())\n",
        "\n",
        "\n",
        "\n",
        "    @staticmethod\n",
        "    def DHT_numerical_notused(Edges_df,feature_array):   # after_pca [21,12]\n",
        "        amino = 'ARNDCQEGHILKMFPSTWYV-'\n",
        "\n",
        "        print(\"type of feature_array[1]:\",type(feature_array[1]))\n",
        "\n",
        "        for i in range(len(Edges_df)):\n",
        "            weight_col = Edges_df.iloc[i]['weight']\n",
        "            first_letter = weight_col[0]\n",
        "            number = re.findall('\\d+', weight_col)[0]\n",
        "            #print(\"in DHT Numerical edges weight is : \",i,weight_col)\n",
        "            #print(\"in DHT Numerical edges weight1: \",i,first_letter)\n",
        "            #print(\"in DHT Numerical edges weight2: \",i,number)\n",
        "            #print(\"type of number:\",type(number))\n",
        "            #print(\"type of first_letter:\",type(first_letter))\n",
        "\n",
        "\n",
        "            if (first_letter =='p'):\n",
        "                print(\"In p\")\n",
        "                Edges_df.at[i,'weight'] = 1 #feature_array[1]\n",
        "\n",
        "            elif (first_letter =='h'):\n",
        "                print(\"In h\")\n",
        "                Edges_df.at[i,'weight'] = 2 #feature_array[1+9]\n",
        "\n",
        "        #Edges_df.to_csv('D:\\\\PhD\\\\GraphImmuno_NewResults\\\\data\\\\DHT_numerical.txt', sep='\\t', index=False)\n",
        "        return None\n",
        "\n",
        "\n",
        "    @staticmethod\n",
        "    def unweight_edge(pep,hla,after_pca):\n",
        "        source,target = Graph_Constructor.combinator(pep,hla)\n",
        "        combine = list(itertools.product(source,target))\n",
        "        weight = itertools.repeat(1,len(source)*len(target))\n",
        "        edges = pd.DataFrame({'source':[item[0] for item in combine],'target':[item[1] for item in combine],'weight':weight})\n",
        "        feature_array = Graph_Constructor.numerical(pep,hla,after_pca)\n",
        "        try:nodes = IndexedArray(feature_array,index=source+target)\n",
        "        except: print(pep,hla,feature_array.shape)\n",
        "        graph = StellarGraph(nodes,edges,node_type_default='corner',edge_type_default='line')\n",
        "        return graph\n",
        "\n",
        "\n",
        "    @staticmethod\n",
        "    def Update_Edges_new(pep,hla,edges):\n",
        "\n",
        "      pep = pep.replace('X','-').upper()\n",
        "      hla = hla.replace('X','-').upper()\n",
        "\n",
        "      amino = 'ARNDCQEGHILKMFPSTWYV-'\n",
        "\n",
        "      edges_weights = pd.read_csv(r'D:\\PhD\\New_Graph_Model\\data\\Edge_weights_pca_array_new.txt',sep=' ',header = None)\n",
        "\n",
        "      # create a dictionary to map amino acid characters to their position in the amino acid string\n",
        "      amino_dict = {}\n",
        "      for i, c in enumerate(amino):\n",
        "        amino_dict[c] = i\n",
        "\n",
        "      # loop through the rows of the edges dataframe\n",
        "      for index, row in edges.iterrows():\n",
        "            # get the source and target nodes from the row\n",
        "            source = row['source']\n",
        "            target = row['target']\n",
        "\n",
        "\n",
        "            # check if source is a peptide or HLA allele\n",
        "            if source[0] == 'p':\n",
        "            # get the position of the amino acid in the peptide sequence\n",
        "                pos = int(source[1:]) - 1\n",
        "\n",
        "                # get the amino acid character at that position\n",
        "                amino_char = pep[pos]\n",
        "\n",
        "                # get the index of the amino acid character in the amino acid string\n",
        "                amino_pos = amino_dict[amino_char]\n",
        "\n",
        "            elif source[0] == 'h':\n",
        "                # get the position of the amino acid in the HLA allele\n",
        "                pos = int(source[1:]) - 1\n",
        "\n",
        "                # get the amino acid character at that position\n",
        "                amino_char = hla[pos]\n",
        "\n",
        "                # get the index of the amino acid character in the amino acid string\n",
        "                amino_pos = amino_dict[amino_char]\n",
        "\n",
        "            # check if target is a peptide or HLA allele\n",
        "            if target[0] == 'p':\n",
        "                # get the position of the amino acid in the peptide sequence\n",
        "                pos = int(target[1:]) - 1\n",
        "\n",
        "                # get the amino acid character at that position\n",
        "                amino_char = pep[pos]\n",
        "\n",
        "                # get the index of the amino acid character in the amino acid string\n",
        "                amino_pos2 = amino_dict[amino_char]\n",
        "\n",
        "            elif target[0] == 'h':\n",
        "                # get the position of the amino acid in the HLA allele\n",
        "                pos = int(target[1:]) - 1\n",
        "\n",
        "                # get the amino acid character at that position\n",
        "                amino_char = hla[pos]\n",
        "\n",
        "                # get the index of the amino acid character in the amino acid string\n",
        "                amino_pos2 = amino_dict[amino_char]\n",
        "\n",
        "                # get the weight value from the weights dataframe\n",
        "                weight = edges_weights.iloc[amino_pos, amino_pos2]\n",
        "\n",
        "            # update the weight column in the edges dataframe\n",
        "            #edges.at[index, 'weight'] = weight\n",
        "            edges['weight'] = edges['weight'].astype(float)\n",
        "            edges.at[index, 'weight'] = (weight)\n",
        "\n",
        "      # Check if source[0] and target[0] are same, if not update weight accordingly\n",
        "      for index, row in edges.iterrows():\n",
        "        source, target = row['source'], row['target']\n",
        "        weight = row['weight']\n",
        "        mean = np.mean(edges['weight'])\n",
        "        std = np.std(edges['weight'])\n",
        "\n",
        "        if source[0] != target[0] and (weight > (mean + std) or weight < (mean - std)):\n",
        "            edges.at[index, 'weight'] = -1\n",
        "\n",
        "      return edges\n",
        "\n",
        "\n",
        "    @staticmethod\n",
        "    def Update_Edges(pep,hla,edges):\n",
        "      #print(\"edges\",edges)\n",
        "\n",
        "      #pep = \"KEHVFFSEY\"\n",
        "      #hla = \"MMMMYYTMMKKRKYREISNT-ENNTT--YIYDYTKWVVQDYLSRY-\"\n",
        "\n",
        "      pep = pep.replace('X','-').upper()\n",
        "      hla = hla.replace('X','-').upper()\n",
        "\n",
        "      print(\"pep is :\",pep)\n",
        "      print(\"hla is :\",hla)\n",
        "      print(\"edges\",edges)\n",
        "      edges.to_csv('D:\\\\PhD\\\\New_Graph_Model\\\\orginal_edges.txt', sep='\\t', index=False)\n",
        "\n",
        "\n",
        "      #edges_weights = pd.read_csv('Edge_Weights_V3.txt',sep='\\t',header = None)\n",
        "      #edges_weights = pd.read_csv('D:\\\\PhD\\\\GraphImmuno_NewResults\\\\data\\\\Edge_Weights_V3.txt',sep='\\t',header = None)\n",
        "      edges_weights = pd.read_csv(r'D:\\PhD\\New_Graph_Model\\data\\Edge_weights_pca_array_new.txt',sep=' ',header = None)\n",
        "      print(\"edges_weights size\",edges_weights.shape)\n",
        "      print(\"edges_weights\",edges_weights)\n",
        "\n",
        "      amino = 'ARNDCQEGHILKMFPSTWYV-'\n",
        "\n",
        "      # add a new row and column with values zero\n",
        "      #edges_weights = np.pad(edges_weights, [(0,1),(0,1)], mode='constant')\n",
        "\n",
        "      # calculate the mean and standard deviation\n",
        "      mean = np.mean(edges_weights)\n",
        "      std = np.std(edges_weights)\n",
        "\n",
        "      edges['weight'] = edges['weight'].astype(float)\n",
        "\n",
        "      for i in range(0, len(edges)):\n",
        "\n",
        "        if ((edges.iloc[i]['source'][0] == \"p\") and (edges.iloc[i]['target'][0] == \"p\")):\n",
        "          edges.at[i,'weight'] = edges_weights[amino.index(pep[(int([edges.iloc[i]['source'][1:]][0])-1)])][amino.index(pep[(int([edges.iloc[i]['target'][1:]][0])-1)])]\n",
        "\n",
        "        elif ((edges.iloc[i]['source'][0] == \"p\") and (edges.iloc[i]['target'][0] == \"h\")):\n",
        "          edges.at[i,'weight'] = edges_weights[amino.index(pep[(int([edges.iloc[i]['source'][1:]][0])-1)])][amino.index(hla[(int([edges.iloc[i]['target'][1:]][0])-1)])]\n",
        "          if ((edges.at[i,'weight'] > (mean + std)) or (edges.at[i,'weight'] < ((mean - std)))):\n",
        "            edges.at[i,'weight'] = -1\n",
        "\n",
        "        elif ((edges.iloc[i]['source'][0] == \"h\") and (edges.iloc[i]['target'][0] == \"p\")):\n",
        "          edges.at[i,'weight'] = edges_weights[amino.index(hla[(int([edges.iloc[i]['source'][1:]][0])-1)])][amino.index(pep[(int([edges.iloc[i]['target'][1:]][0])-1)])]\n",
        "          if ((edges.at[i,'weight'] > (mean + std)) or (edges.at[i,'weight'] < ((mean - std)))):\n",
        "            edges.at[i,'weight'] = -1\n",
        "\n",
        "        elif ((edges.iloc[i]['source'][0] == \"h\") and (edges.iloc[i]['target'][0] == \"h\")):\n",
        "          edges.at[i,'weight'] = edges_weights[amino.index(hla[(int([edges.iloc[i]['source'][1:]][0])-1)])][amino.index(hla[(int([edges.iloc[i]['target'][1:]][0])-1)])]\n",
        "\n",
        "      edges['weight'] = edges['weight'].astype(float)\n",
        "\n",
        "      return edges\n",
        "\n",
        "\n",
        "    #Dual Hyper transformation\n",
        "    @staticmethod\n",
        "    def DHT_new(source,target,edges,feature_array):\n",
        "\n",
        "      new_nodeslist= edges.index.tolist()\n",
        "      New_weights = edges[\"weight\"].tolist()\n",
        "\n",
        "\n",
        "\n",
        "      Edges_df = pd.DataFrame(columns=['source', 'target', 'weight'])\n",
        "\n",
        "      # Reset the index to a named column\n",
        "      edges = edges.rename_axis('index').reset_index()\n",
        "\n",
        "\n",
        "      Edges_df = pd.DataFrame(columns=['source', 'target', 'weight'])\n",
        "\n",
        "\n",
        "      # Updating the nodes edges by refering to edges_dataframe\n",
        "      for i in range(len(edges)):\n",
        "            for j in range(i+1, len(edges)):\n",
        "                if edges.loc[i, 'source'] == edges.loc[j, 'source']:\n",
        "                    weight = edges.loc[i, 'source']\n",
        "                    Edges_df.loc[len(Edges_df)] = [edges.loc[i, 'index'], edges.loc[j, 'index'], weight]\n",
        "                elif edges.loc[i, 'source'] == edges.loc[j, 'target']:\n",
        "                    weight = edges.loc[i, 'source']\n",
        "                    Edges_df.loc[len(Edges_df)] = [edges.loc[i, 'index'], edges.loc[j, 'index'], weight]\n",
        "                elif edges.loc[i, 'target'] == edges.loc[j, 'source']:\n",
        "                    weight = edges.loc[i, 'target']\n",
        "                    Edges_df.loc[len(Edges_df)] = [edges.loc[i, 'index'], edges.loc[j, 'index'], weight]\n",
        "                elif edges.loc[i, 'target'] == edges.loc[j, 'target']:\n",
        "                    weight = edges.loc[i, 'target']\n",
        "                    Edges_df.loc[len(Edges_df)] = [edges.loc[i, 'index'], edges.loc[j, 'index'], weight]\n",
        "\n",
        "\n",
        "      #Edges_df.to_csv('D:\\\\PhD\\\\New_Graph_Model\\\\edges_df_values.txt', sep='\\t', index=False)\n",
        "\n",
        "      # replace empty values with NaN\n",
        "      Edges_df = Edges_df.replace(r'^\\s*$', np.nan, regex=True)\n",
        "\n",
        "      # drop rows where 'weight' column has null or empty values\n",
        "      Edges_df = Edges_df.dropna(subset=['weight'])\n",
        "\n",
        "\n",
        "      new_nodeslist= pd.concat([Edges_df['source'], Edges_df['target']]).unique().tolist()\n",
        "      New_nodes = pd.DataFrame(    {\"x\": New_weights}, index=new_nodeslist)\n",
        "\n",
        "\n",
        "      # apply the function to the weight column\n",
        "      Edges_df['weight'] = Edges_df['weight'].apply(Graph_Constructor.DHT_numerical, feature_array=feature_array)\n",
        "\n",
        "\n",
        "      # Apply the function to the \"weight\" column to get a new DataFrame\n",
        "      new_df = Edges_df.apply(Graph_Constructor.Split_Weight, axis=1)\n",
        "\n",
        "      # Rename the columns in the new DataFrame\n",
        "      new_df.columns = ['weight_'+str(i) for i in range(new_df.shape[1])]\n",
        "\n",
        "      # Concatenate the new DataFrame with the original DataFrame\n",
        "      Edges_df = pd.concat([Edges_df, new_df], axis=1)\n",
        "\n",
        "      # Drop the original \"weight\" column\n",
        "      Edges_df.drop('weight', axis=1, inplace=True)\n",
        "\n",
        "      Edges_df.to_csv('D:\\\\PhD\\\\GraphImmuno_NewResults\\\\data\\\\Edges_df_values_New.txt', sep='\\t', index=False)\n",
        "\n",
        "      return New_nodes,Edges_df\n",
        "\n",
        "\n",
        "\n",
        "    #Dual Hyper transformation\n",
        "    @staticmethod\n",
        "    def DHT(source,target,edges,feature_array):\n",
        "\n",
        "      new_nodeslist= edges.index.tolist()\n",
        "      New_weights = edges[\"weight\"].tolist()\n",
        "\n",
        "      #print(\"new_nodes in DHT\", new_nodeslist)\n",
        "      #print(\"New_weights\", New_weights)\n",
        "      #print(\"Existing edges : \", edges)\n",
        "\n",
        "      #feature_weights = np.array(New_weights)\n",
        "      #feature_weights_new = np.zeros((feature_weights.shape[0], 2))\n",
        "\n",
        "      #for i in range(feature_weights_new.shape[0]):\n",
        "        #feature_weights_new[i:, 0] = New_weights[i]\n",
        "\n",
        "\n",
        "      #print(\"feature_weights_new\",feature_weights_new)\n",
        "      #print('feature_array Type',np.shape(feature_weights_new))\n",
        "      #print('index Type',np.shape(new_nodeslist))\n",
        "\n",
        "\n",
        "      # Finding the pairs\n",
        "      new_substuff = []\n",
        "      for L in range(1):\n",
        "        for subset in itertools.combinations(new_nodeslist, 2):\n",
        "          if len(subset) == 2:\n",
        "            new_substuff.append(subset)\n",
        "\n",
        "      # Converting the pairs to data frames\n",
        "      Edges_df = pd.DataFrame(new_substuff)\n",
        "      Edges_df.columns = [\"Source\", \"Target\"]\n",
        "      Edges_df['Edge'] = ''\n",
        "\n",
        "      #print(Edges_df.tail(5))\n",
        "      #print(len(Edges_df))\n",
        "\n",
        "      # Reset the index to a named column\n",
        "      edges = edges.rename_axis('index').reset_index()\n",
        "\n",
        "      # export dataframe to a text file\n",
        "      #edges.to_csv('D:\\\\PhD\\\\GraphImmuno_NewResults\\\\data\\\\Existing_edges.txt', sep='\\t', index=False)\n",
        "\n",
        "      # Updating the edges by referingto edges dataframe\n",
        "      for i in range(len(Edges_df)):\n",
        "\n",
        "\n",
        "        if((edges.loc[edges['index'] == Edges_df.iloc[i]['Source']]['source']).values == (edges.loc[edges['index'] == Edges_df.iloc[i]['Target']]['source']).values):\n",
        "          Edges_df.at[i,'Edge'] = edges.loc[edges['index'] == Edges_df.iloc[i]['Source']]['source'].values[0]\n",
        "\n",
        "        if((edges.loc[edges['index'] == Edges_df.iloc[i]['Source']]['target']).values == (edges.loc[edges['index'] == Edges_df.iloc[i]['Target']]['target']).values):\n",
        "          Edges_df.at[i,'Edge'] = edges.loc[edges['index'] == Edges_df.iloc[i]['Source']]['target'].values[0]\n",
        "\n",
        "        if((edges.loc[edges['index'] == Edges_df.iloc[i]['Source']]['source']).values == (edges.loc[edges['index'] == Edges_df.iloc[i]['Target']]['target']).values):\n",
        "          Edges_df.at[i,'Edge'] = edges.loc[edges['index'] == Edges_df.iloc[i]['Source']]['source'].values[0]\n",
        "\n",
        "        if((edges.loc[edges['index'] == Edges_df.iloc[i]['Source']]['target']).values == (edges.loc[edges['index'] == Edges_df.iloc[i]['Target']]['source']).values):\n",
        "          Edges_df.at[i,'Edge'] = edges.loc[edges['index'] == Edges_df.iloc[i]['Source']]['target'].values[0]\n",
        "\n",
        "\n",
        "      Edges_df.rename(columns={'Source': 'source', 'Target': 'target', 'Edge': 'weight'}, inplace=True)\n",
        "\n",
        "      #print('Edges_df Type1',(Edges_df))\n",
        "      #Edges_df.to_csv('D:\\\\PhD\\\\GraphImmuno_NewResults\\\\data\\\\Edges_df_before.txt', sep='\\t', index=False)\n",
        "\n",
        "\n",
        "      # replace empty values with NaN\n",
        "      Edges_df = Edges_df.replace(r'^\\s*$', np.nan, regex=True)\n",
        "\n",
        "      # drop rows where 'weight' column has null or empty values\n",
        "      Edges_df = Edges_df.dropna(subset=['weight'])\n",
        "\n",
        "\n",
        "      #print(\"Edges_df\",Edges_df)\n",
        "      #Edges_df.to_csv('D:\\\\PhD\\\\GraphImmuno_NewResults\\\\data\\\\Edges_df_After.txt', sep='\\t', index=False)\n",
        "\n",
        "      new_nodeslist= pd.concat([Edges_df['source'], Edges_df['target']]).unique().tolist()\n",
        "      #New_nodes = IndexedArray(feature_weights_new, index=new_nodeslist)\n",
        "\n",
        "      # Creating a the node df using the nodes ;ist as index and weihts as column.\n",
        "      #feature_weights_new_list = [sublst[0] for sublst in feature_weights_new.tolist()]\n",
        "      #New_nodes_df = pd.DataFrame(    {\"x\": feature_weights_new_list}, index=new_nodeslist)\n",
        "      New_nodes = pd.DataFrame(    {\"x\": New_weights}, index=new_nodeslist)\n",
        "\n",
        "\n",
        "      # apply the function to the weight column\n",
        "      #Graph_Constructor.DHT_numerical(Edges_df,feature_array)\n",
        "      #Edges_df['weight'] = Edges_df['weight'].apply(Graph_Constructor.DHT_numerical)\n",
        "      Edges_df['weight'] = Edges_df['weight'].apply(Graph_Constructor.DHT_numerical, feature_array=feature_array)\n",
        "\n",
        "\n",
        "      # Apply the function to the \"weight\" column to get a new DataFrame\n",
        "      new_df = Edges_df.apply(Graph_Constructor.Split_Weight, axis=1)\n",
        "\n",
        "      # Rename the columns in the new DataFrame\n",
        "      new_df.columns = ['weight_'+str(i) for i in range(new_df.shape[1])]\n",
        "\n",
        "      # Concatenate the new DataFrame with the original DataFrame\n",
        "      Edges_df = pd.concat([Edges_df, new_df], axis=1)\n",
        "\n",
        "      # Drop the original \"weight\" column\n",
        "      Edges_df.drop('weight', axis=1, inplace=True)\n",
        "\n",
        "\n",
        "      Edges_df.to_csv('D:\\\\PhD\\\\GraphImmuno_NewResults\\\\data\\\\Edges_df_values.txt', sep='\\t', index=False)\n",
        "      #print(\"The new nodes are : \", new_nodeslist)\n",
        "\n",
        "      return New_nodes,Edges_df\n",
        "      #return None\n",
        "\n",
        "\n",
        "\n",
        "    @staticmethod\n",
        "    def weight_anchor_edge(pep,hla,after_pca):\n",
        "        source, target = Graph_Constructor.combinator(pep, hla)\n",
        "        combine = list(itertools.product(source, target))\n",
        "        weight = itertools.repeat(1, len(source) * len(target))\n",
        "        edges = pd.DataFrame({'source': [item[0] for item in combine], 'target': [item[1] for item in combine], 'weight': weight})\n",
        "        for i in range(edges.shape[0]):\n",
        "            col1 = edges.iloc[i]['source']\n",
        "            col2 = edges.iloc[i]['target']\n",
        "            col3 = edges.iloc[i]['weight']\n",
        "            if col1 == 'a2' or col1 == 'a9' or col1 ==  'a10':\n",
        "                edges.iloc[i]['weight'] = 1.5\n",
        "        feature_array = Graph_Constructor.numerical(pep, hla, after_pca)\n",
        "        nodes = IndexedArray(feature_array, index=source + target)\n",
        "        graph = StellarGraph(nodes, edges, node_type_default='corner', edge_type_default='line')\n",
        "        return graph\n",
        "\n",
        "    @staticmethod\n",
        "    def intra_and_inter(pep,hla,after_pca):\n",
        "        source, target = Graph_Constructor.combinator(pep, hla)\n",
        "        combine = list(itertools.product(source, target))\n",
        "        weight = itertools.repeat(2, len(source) * len(target))\n",
        "        edges_inter = pd.DataFrame({'source': [item[0] for item in combine], 'target': [item[1] for item in combine], 'weight': weight})\n",
        "\n",
        "        ## Consinderonly only linear relationships in peptides(pep and hla)\n",
        "        #intra_pep = list(itertools.combinations(source,2))\n",
        "        #intra_hla = list(itertools.combinations(target,2))\n",
        "        intra_pep = [(source[i], source[i+1]) for i in range(len(source)-1)]\n",
        "        intra_hla = [(target[i], target[i+1]) for i in range(len(target)-1)]\n",
        "\n",
        "        intra = intra_pep + intra_hla\n",
        "        #print(\"intra\",intra)\n",
        "        weight = itertools.repeat(1,len(intra))\n",
        "        edges_intra = pd.DataFrame({'source':[item[0] for item in intra],'target':[item[1] for item in intra],'weight':weight})\n",
        "        edges = pd.concat([edges_inter,edges_intra])\n",
        "        edges = edges.set_index(pd.Index(np.arange(edges.shape[0])))\n",
        "\n",
        "        # Updating Edge weights\n",
        "        #Graph_Constructor.Update_Edges(pep,hla,edges)\n",
        "        Graph_Constructor.Update_Edges_new(pep,hla,edges)\n",
        "\n",
        "        #Removing the edges\n",
        "        edges = edges[edges['weight'] != -1.00]\n",
        "        Edge_Weight_List.append(edges)\n",
        "\n",
        "        feature_array = Graph_Constructor.numerical(pep, hla, after_pca)\n",
        "        nodes = IndexedArray(feature_array, index=source + target)\n",
        "\n",
        "\n",
        "\n",
        "        # Dual Hyper Transfer\n",
        "        #New_nodes,new_Edges = Graph_Constructor.DHT(source,target,edges,feature_array)\n",
        "        #New_nodes,new_Edges = Graph_Constructor.DHT_new(source,target,edges,feature_array)\n",
        "\n",
        "        # Orginal Graph\n",
        "        graph = StellarGraph(nodes, edges, node_type_default='corner', edge_type_default='line')\n",
        "\n",
        "        #new_Edges.to_csv('D:\\\\PhD\\\\New_Graph_Model\\\\new_Edges.txt', sep='\\t', index=False)\n",
        "        #print('type(new_Edges[weight]:',type(new_Edges['weight']))\n",
        "        #edges.to_csv('D:\\\\PhD\\\\New_Graph_Model\\\\edges.txt', sep='\\t', index=False)\n",
        "        #print('type(edges[weight]:',type(edges['weight']))\n",
        "\n",
        "\n",
        "        # Creating DHT graphs\n",
        "        #graph = StellarGraph(New_nodes, new_Edges, node_type_default='corner', edge_type_default='line')\n",
        "        #print(\"New_nodes\",New_nodes)\n",
        "        #print(\"New_nodes type : \",type(New_nodes))\n",
        "        #print(\"new_Edges\",new_Edges)\n",
        "        #print(\"new_Edges type : \",type(new_Edges))\n",
        "\n",
        "        #print('Nodes Type',type(nodes))\n",
        "        #print('feature_array',feature_array)\n",
        "        #print('feature_array size',feature_array.shape)\n",
        "        #print('index', source + target)\n",
        "        #print('index len', len(source + target))\n",
        "        #print('Edges Type',type(edges))\n",
        "\n",
        "        return graph\n",
        "\n",
        "    @staticmethod\n",
        "    def entrance(df,after_pca,hla_dic,dic_inventory):\n",
        "        graphs = []\n",
        "        graph_labels = []\n",
        "        for i in range(df.shape[0]):\n",
        "            print(i)\n",
        "            pep = df['peptide'].iloc[i]\n",
        "            try:\n",
        "                hla = hla_dic[df['HLA'].iloc[i]]\n",
        "            except KeyError:\n",
        "                hla = hla_dic[rescue_unknown_hla(df['HLA'].iloc[i],dic_inventory)]\n",
        "            label = df['immunogenicity'].iloc[i]\n",
        "            #if label != 'Negative': label = 0\n",
        "            #else: label = 1\n",
        "            #graph = Graph_Constructor.unweight_edge(pep,hla,after_pca)\n",
        "            #graph = Graph_Constructor.unweight_edge(pep,hla,after_pca)\n",
        "            graph = Graph_Constructor.intra_and_inter(pep,hla,after_pca)\n",
        "            graphs.append(graph)\n",
        "            graph_labels.append(label)\n",
        "        graph_labels = pd.Series(graph_labels)\n",
        "        return graphs,graph_labels"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "seeKiegpFv4a"
      },
      "outputs": [],
      "source": [
        "def train_fold(model, train_gen, test_gen, es, epochs):\n",
        "    history = model.fit(\n",
        "        train_gen, epochs=epochs, validation_data=test_gen, verbose=2, callbacks=[es],)\n",
        "    # calculate performance on the test data and return along with history\n",
        "    test_metrics = model.evaluate(test_gen, verbose=0)\n",
        "    test_acc = test_metrics[model.metrics_names.index(\"acc\")]\n",
        "    return history, test_acc"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1umwI2WPFxbz"
      },
      "outputs": [],
      "source": [
        "def get_generators(train_index, test_index, graph_labels, batch_size):\n",
        "    train_gen = generator.flow(\n",
        "        train_index, targets=graph_labels.iloc[train_index].values, batch_size=batch_size)\n",
        "    test_gen = generator.flow(\n",
        "        test_index, targets=graph_labels.iloc[test_index].values, batch_size=batch_size)\n",
        "\n",
        "    return train_gen, test_gen"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MOpd2LEkFzaS"
      },
      "outputs": [],
      "source": [
        "def draw_ROC(y_true,y_pred):\n",
        "\n",
        "    fpr,tpr,_ = roc_curve(y_true,y_pred,pos_label=1)\n",
        "    area_mine = auc(fpr,tpr)\n",
        "    fig = plt.figure()\n",
        "    lw = 2\n",
        "    plt.plot(fpr, tpr, color='darkorange',\n",
        "            lw=lw, label='ROC curve (area = %0.2f)' % area_mine)\n",
        "    plt.plot([0, 1], [0, 1], color='navy', lw=lw, linestyle='--')\n",
        "    plt.xlim([0.0, 1.0])\n",
        "    plt.ylim([0.0, 1.05])\n",
        "    plt.xlabel('False Positive Rate')\n",
        "    plt.ylabel('True Positive Rate')\n",
        "    plt.title('Receiver operating characteristic example')\n",
        "    plt.legend(loc=\"lower right\")\n",
        "    plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3KqFSmalF3kw"
      },
      "outputs": [],
      "source": [
        "def draw_PR(y_true,y_pred):\n",
        "    precision,recall,_ = precision_recall_curve(y_true,y_pred,pos_label=1)\n",
        "    area_PR = auc(recall,precision)\n",
        "    baseline = np.sum(np.array(y_true) == 1) / len(y_true)\n",
        "\n",
        "    plt.figure()\n",
        "    lw = 2\n",
        "    plt.plot(recall,precision, color='darkorange',\n",
        "            lw=lw, label='PR curve (area = %0.2f)' % area_PR)\n",
        "    plt.plot([0, 1], [baseline, baseline], color='navy', lw=lw, linestyle='--')\n",
        "    plt.xlim([0.0, 1.0])\n",
        "    plt.ylim([0.0, 1.05])\n",
        "    plt.xlabel('Recall')\n",
        "    plt.ylabel('Precision')\n",
        "    plt.title('PR curve example')\n",
        "    plt.legend(loc=\"lower right\")\n",
        "    plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rtIw24_XF6PY"
      },
      "outputs": [],
      "source": [
        "def draw_history(history):\n",
        "    plt.subplot(211)\n",
        "    plt.title('Loss')\n",
        "    plt.plot(history.history['loss'], label='train')\n",
        "    plt.plot(history.history['val_loss'], label='validation')\n",
        "    plt.legend()\n",
        "    # plot accuracy during training\n",
        "    plt.subplot(212)\n",
        "    plt.title('Accuracy')\n",
        "    plt.plot(history.history['acc'], label='train')\n",
        "    plt.plot(history.history['val_acc'], label='validation')\n",
        "    plt.legend()\n",
        "    plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RdiCZWNnGjkl"
      },
      "outputs": [],
      "source": [
        "def hla_df_to_dic(hla):\n",
        "    dic = {}\n",
        "    for i in range(hla.shape[0]):\n",
        "        col1 = hla['HLA'].iloc[i]  # HLA allele\n",
        "        col2 = hla['pseudo'].iloc[i]  # pseudo sequence\n",
        "        dic[col1] = col2\n",
        "    return dic"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "p2gs8Z53Glcq"
      },
      "outputs": [],
      "source": [
        "def retain_910(ori):\n",
        "    cond = []\n",
        "    for i in range(ori.shape[0]):\n",
        "        peptide = ori['peptide'].iloc[i]\n",
        "        if len(peptide) == 9 or len(peptide) == 10:\n",
        "            cond.append(True)\n",
        "        else:\n",
        "            cond.append(False)\n",
        "    data = ori.loc[cond]\n",
        "    data = data.set_index(pd.Index(np.arange(data.shape[0])))\n",
        "    return data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mnTsnL1LGnwn"
      },
      "outputs": [],
      "source": [
        "    #ori_train = pd.read_csv('remove0123_sample100.csv')\n",
        "    #ori_train = pd.read_csv('remove0123_sample100_Test.csv')\n",
        "    #hla = pd.read_csv('hla2paratopeTable_aligned.txt',sep='\\t')\n",
        "    #after_pca = np.loadtxt('after_pca.txt')\n",
        "    Edge_Weight_List = []\n",
        "\n",
        "    #ori_train = pd.read_csv('D:\\\\PhD\\Subgraph_Sampling_SLOT_GNN\\\\Data\\\\remove0123_sample100_Test_Copy2.csv')\n",
        "    ori_train = pd.read_csv('D:\\\\PhD\\Subgraph_Sampling_SLOT_GNN\\\\Data\\\\remove0123_sample100.csv')\n",
        "    hla = pd.read_csv('D:\\\\PhD\\\\GraphImmuno_NewResults\\\\data\\\\hla2paratopeTable_aligned.txt',sep='\\t')\n",
        "    after_pca = np.loadtxt('D:\\\\PhD\\\\GraphImmuno_NewResults\\\\data\\\\after_pca.txt')\n",
        "\n",
        "    hla_dic = hla_df_to_dic(hla)\n",
        "    inventory = list(hla_dic.keys())\n",
        "    dic_inventory = dict_inventory(inventory)\n",
        "\n",
        "    ori_train['immunogenicity'], ori_train['potential'] = ori_train['potential'], ori_train['immunogenicity']"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DI9y8gWrGpbP"
      },
      "outputs": [],
      "source": [
        "graphs, graph_labels = Graph_Constructor.entrance(ori_train, after_pca, hla_dic, dic_inventory)\n",
        "generator = PaddedGraphGenerator(graphs=graphs)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CK_vXBimYlnh"
      },
      "outputs": [],
      "source": [
        "#Sub Graph preparation\n",
        "import random\n",
        "import stellargraph as sg\n",
        "\n",
        "subgraphs = []\n",
        "subgraph_labels = []"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "335DlFuuYlni"
      },
      "outputs": [],
      "source": [
        "import networkx as nx\n",
        "import matplotlib.pyplot as plt"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4PXYtjkbYlni"
      },
      "outputs": [],
      "source": [
        "for i in range(len(graphs)):\n",
        "    # Convert the nodes to a list\n",
        "    all_nodes = list(graphs[i].nodes())\n",
        "\n",
        "    # Compute eigenvector centrality for nodes using the graph structure\n",
        "    #eigenvector_centrality = nx.eigenvector_centrality_numpy(graphs[i].to_networkx(), weight=None)  # Replace 'None' with edge weights if applicable\n",
        "    eigenvector_centrality = nx.eigenvector_centrality_numpy(graphs[i].to_networkx(), weight='weight')\n",
        "\n",
        "    # Create the adjacency matrix\n",
        "    adj_matrix = np.array(graphs[i].to_adjacency_matrix().toarray())\n",
        "\n",
        "    # Set the damping factor for PageRank\n",
        "    damping_factor = 0.85\n",
        "\n",
        "    # Initialize PageRank scores\n",
        "    num_nodes = len(adj_matrix)\n",
        "    pagerank = np.ones(num_nodes) / num_nodes\n",
        "\n",
        "    # Maximum number of iterations and convergence threshold for PageRank\n",
        "    max_iterations = 100\n",
        "    threshold = 1e-6\n",
        "\n",
        "    # Perform PageRank iterations\n",
        "    for _ in range(max_iterations):\n",
        "        new_pagerank = np.zeros(num_nodes)\n",
        "\n",
        "        for k in range(num_nodes):\n",
        "            for j in range(num_nodes):\n",
        "                if adj_matrix[j, k] != 0:\n",
        "                    # Contribution to new PageRank from node j\n",
        "                    new_pagerank[k] += damping_factor * pagerank[j] / np.sum(adj_matrix[j])\n",
        "\n",
        "        # Handling teleportation and damping factor\n",
        "        new_pagerank += (1 - damping_factor) / num_nodes\n",
        "\n",
        "        # Check for convergence\n",
        "        if np.sum(np.abs(new_pagerank - pagerank)) < threshold:\n",
        "            break\n",
        "\n",
        "        pagerank = new_pagerank\n",
        "\n",
        "\n",
        "    # Create a dictionary of node names and their PageRank scores\n",
        "    pagerank_scores = {all_nodes[l]: pagerank[l] for l in range(num_nodes)}\n",
        "\n",
        "    # Combine custom centrality scores and PageRank scores\n",
        "    combined_scores = {\n",
        "        node: eigenvector_centrality[node] + pagerank_scores.get(node, 0.0)\n",
        "        #for node in graphs[i].nodes()\n",
        "        for node in all_nodes\n",
        "    }\n",
        "\n",
        "    # Find the most influential node based on the combined scores\n",
        "    most_influential_node = max(combined_scores, key=combined_scores.get)\n",
        "    start_node = most_influential_node\n",
        "\n",
        "    # Print the most significant node and its degree centrality value\n",
        "    print(\"Most Significant Node:\",i, start_node)\n",
        "    #print(\"Degree Centrality:\", combined_scores)\n",
        "\n",
        "    # Initialize the subgraph nodes with the start node\n",
        "    subgraph_nodes = set([start_node])\n",
        "\n",
        "    # Perform BFS traversal starting from the randomly chosen node\n",
        "    queue = [start_node]\n",
        "    max_hops = 2  # Adjust max_hops as needed\n",
        "\n",
        "    # Perform BFS traversal starting from the randomly chosen node\n",
        "    while queue and max_hops > 0:\n",
        "        max_hops -= 1\n",
        "        current_node = queue.pop(0)\n",
        "        neighbors = graphs[i].neighbors(current_node)\n",
        "        for neighbor in neighbors:\n",
        "            if neighbor not in subgraph_nodes:\n",
        "                subgraph_nodes.add(neighbor)\n",
        "                queue.append(neighbor)\n",
        "\n",
        "\n",
        "    # Create a subgraph from the selected nodes\n",
        "    subgraph = graphs[i].subgraph(list(subgraph_nodes))\n",
        "\n",
        "    # Get the edges of the subgraph\n",
        "    subgraph_edges = list(subgraph.edges())\n",
        "\n",
        "    # Initialize a list to store subgraph edge features\n",
        "    subgraph_edge_features = []\n",
        "\n",
        "    # Iterate through subgraph edges and get their features\n",
        "    for edge in subgraph_edges:\n",
        "        source, target = edge\n",
        "        # Check if the edge exists in the main graph\n",
        "        if edge in graphs[i].edges():\n",
        "            # Get the index of the edge in the main graph\n",
        "            edge_index = list(graphs[i].edges()).index(edge)\n",
        "            # Get edge features using the index\n",
        "            edge_features = list(graphs[i].edge_features())[edge_index]\n",
        "            #subgraph_edge_features.append([source, target] + edge_features)\n",
        "            subgraph_edge_features.append([source, target] + list(edge_features))\n",
        "\n",
        "\n",
        "    # Create a DataFrame for subgraph edges with columns 'source', 'target', and edge feature values\n",
        "    subgraph_edges_df = pd.DataFrame(subgraph_edge_features, columns=['source', 'target'] + [f'edge_feature_{i}' for i in range(len(edge_features))])\n",
        "\n",
        "\n",
        "    # Get the index positions of subgraph nodes in the original graph\n",
        "    node_positions = [all_nodes.index(node) for node in subgraph_nodes]\n",
        "\n",
        "    # Get node features using node positions\n",
        "    node_features = graphs[i].node_features()[node_positions]\n",
        "\n",
        "    # Create a DataFrame for subgraph nodes with selected node features\n",
        "    subgraph_nodes_df = pd.DataFrame(node_features, index=list(subgraph_nodes))\n",
        "\n",
        "    subgraph = StellarGraph(subgraph_nodes_df,subgraph_edges_df,  node_type_default='corner', edge_type_default='line')\n",
        "\n",
        "    subgraphs.append(subgraph)\n",
        "\n",
        "#subgraph_labels = graph_labels"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Uj53-GJuGtJm"
      },
      "outputs": [],
      "source": [
        "gc_model = GCNSupervisedGraphClassification(\n",
        "            layer_sizes=[64, 64],\n",
        "            activations=[\"relu\", \"relu\"],\n",
        "            generator=generator,\n",
        "            dropout=0.2, )\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "AHhJWl8gGvYQ"
      },
      "outputs": [],
      "source": [
        "x_inp, x_out = gc_model.in_out_tensors()\n",
        "predictions = Dense(units=32, activation=\"relu\")(x_out)\n",
        "predictions = Dense(units=16, activation=\"relu\")(predictions)\n",
        "predictions = Dense(units=1, activation=\"sigmoid\")(predictions)\n",
        "model = Model(inputs=x_inp, outputs=predictions)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2IoWaQK0GzC1"
      },
      "outputs": [],
      "source": [
        "model.compile(optimizer=Adam(0.001), loss=mean_squared_error)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Vmd43k_yG2_w"
      },
      "outputs": [],
      "source": [
        "train_index, test_index = model_selection.train_test_split(ori_train.index, train_size=0.5)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RcPH8B1GG_GU"
      },
      "outputs": [],
      "source": [
        "def get_generators1(train_index, test_index, graph_labels):\n",
        "    train_gen = generator.flow(\n",
        "        train_index, targets=graph_labels.iloc[train_index].values)\n",
        "    test_gen = generator.flow(\n",
        "        test_index, targets=graph_labels.iloc[test_index].values)\n",
        "\n",
        "    return train_gen, test_gen"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vhOmhvEVHJqJ"
      },
      "outputs": [],
      "source": [
        "!pip install networkx"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4IPSjk5NHU4S"
      },
      "outputs": [],
      "source": [
        "import networkx as nx\n",
        "import matplotlib.pyplot as plt"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pCtpbuXOHRn9"
      },
      "outputs": [],
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "x_train, x_test, y_train, y_test = train_test_split(subgraphs, graph_labels, test_size = 0.33, random_state = 0)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-rjf5i3xHJCQ"
      },
      "outputs": [],
      "source": [
        "import networkx as nx\n",
        "import matplotlib.pyplot as plt"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6zJkeWZKHaG9"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "import stellargraph as sg\n",
        "from stellargraph.mapper import PaddedGraphGenerator\n",
        "from stellargraph.layer import DeepGraphCNN\n",
        "from stellargraph import StellarGraph\n",
        "\n",
        "from stellargraph import datasets\n",
        "\n",
        "from sklearn import model_selection\n",
        "from IPython.display import display, HTML\n",
        "\n",
        "from tensorflow.keras import Model\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "from tensorflow.keras.layers import Dense, Conv1D, MaxPool1D, Dropout, Flatten\n",
        "from tensorflow.keras.losses import binary_crossentropy\n",
        "import tensorflow as tf"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "R-1AHKf6HglI"
      },
      "outputs": [],
      "source": [
        "summary = pd.DataFrame(\n",
        "    [(g.number_of_nodes(), g.number_of_edges()) for g in graphs],\n",
        "    columns=[\"nodes\", \"edges\"],\n",
        ")\n",
        "summary.describe().round(1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2FerzH9CYlnk"
      },
      "outputs": [],
      "source": [
        "summary = pd.DataFrame(\n",
        "    [(g.number_of_nodes(), g.number_of_edges()) for g in subgraphs],\n",
        "    columns=[\"nodes\", \"edges\"],\n",
        ")\n",
        "summary.describe().round(1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GafZpCJrHhDn"
      },
      "outputs": [],
      "source": [
        "graph_labels.value_counts().to_frame()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QFur10p5HmCT"
      },
      "outputs": [],
      "source": [
        "graph_labels = pd.get_dummies(graph_labels, drop_first=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "X3b5uIFgML3y"
      },
      "outputs": [],
      "source": [
        "ori_train1 = ori_train"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "svTOiezNJY5x"
      },
      "outputs": [],
      "source": [
        "ori_train1.loc[ori_train1[\"potential\"] != \"Negative\", \"potential\"] = 'Positive'\n",
        "ori_train1['potential'] = ori_train1['potential'].astype('category')\n",
        "ori_train1['potential'] = pd.factorize(ori_train1['potential'])[0]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SdD-1j6JJeP-"
      },
      "outputs": [],
      "source": [
        "graph_labels = ori_train1['potential']"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NS8wGMx8LEdJ"
      },
      "outputs": [],
      "source": [
        "graph_labels.value_counts().to_frame()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "miczOwrlHpll"
      },
      "outputs": [],
      "source": [
        "generator = PaddedGraphGenerator(graphs=subgraphs)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8z7bBviaVAyn"
      },
      "source": [
        "Deep CGN"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YeTbT8cdHrnZ"
      },
      "outputs": [],
      "source": [
        "k = 35  # the number of rows for the output tensor\n",
        "layer_sizes = [32, 32, 32, 1]\n",
        "\n",
        "dgcnn_model = DeepGraphCNN(\n",
        "    layer_sizes=layer_sizes,\n",
        "    activations=[\"tanh\", \"tanh\", \"tanh\", \"tanh\"],\n",
        "    k=k,\n",
        "    bias=False,\n",
        "    generator=generator,\n",
        ")\n",
        "x_inp, x_out = dgcnn_model.in_out_tensors()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-pbiy8mNIHdD"
      },
      "outputs": [],
      "source": [
        "x_out = Conv1D(filters=16, kernel_size=sum(layer_sizes), strides=sum(layer_sizes))(x_out)\n",
        "x_out = MaxPool1D(pool_size=2)(x_out)\n",
        "\n",
        "x_out = Conv1D(filters=32, kernel_size=5, strides=1)(x_out)\n",
        "\n",
        "x_out = Flatten()(x_out)\n",
        "\n",
        "x_out = Dense(units=128, activation=\"relu\")(x_out)\n",
        "x_out = Dropout(rate=0.5)(x_out)\n",
        "\n",
        "predictions = Dense(units=1, activation=\"sigmoid\")(x_out)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "q4_aR-93IKoE"
      },
      "outputs": [],
      "source": [
        "model = Model(inputs=x_inp, outputs=predictions)\n",
        "\n",
        "model.compile(\n",
        "    optimizer=Adam(lr=0.0001), loss=binary_crossentropy, metrics=[\"acc\"],\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "I_MInNMbINkD"
      },
      "outputs": [],
      "source": [
        "train_graphs, test_graphs = model_selection.train_test_split(\n",
        "    graph_labels, train_size=0.9, test_size=None, stratify=graph_labels,\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VD3ggoDCIQhc"
      },
      "outputs": [],
      "source": [
        "gen = PaddedGraphGenerator(graphs=subgraphs)\n",
        "\n",
        "train_gen = gen.flow(\n",
        "    list(train_graphs.index - 1),\n",
        "    targets=train_graphs.values,\n",
        "    batch_size=50,\n",
        "    symmetric_normalization=False,\n",
        ")\n",
        "\n",
        "test_gen = gen.flow(\n",
        "    list(test_graphs.index - 1),\n",
        "    targets=test_graphs.values,\n",
        "    batch_size=1,\n",
        "    symmetric_normalization=False,\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Wf2Tk94oITx_"
      },
      "outputs": [],
      "source": [
        "epochs = 100"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XsQEP7w9IUUZ"
      },
      "outputs": [],
      "source": [
        "history = model.fit(\n",
        "    train_gen, epochs=epochs, verbose=1, validation_data=test_gen, shuffle=True,\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qivFvRgcIYaN"
      },
      "outputs": [],
      "source": [
        "sg.utils.plot_history(history)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QyMiv_CTI7o5"
      },
      "outputs": [],
      "source": [
        "test_metrics = model.evaluate(test_gen)\n",
        "print(\"\\nTest Set Metrics:\")\n",
        "for name, val in zip(model.metrics_names, test_metrics):\n",
        "    print(\"\\t{}: {:0.4f}\".format(name, val))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IyNNFNeDHjvx"
      },
      "outputs": [],
      "source": [
        "plt.figure(figsize=(6,4))\n",
        "#plt.tight_layout()\n",
        "#plt.yscale('log')\n",
        "plt.xlabel('Episodes')\n",
        "plt.ylabel('Loss')\n",
        "\n",
        "#create smooth line chart\n",
        "plt.plot(history.history[\"loss\"],  label='Training Loss',marker='1',linewidth=0.7)\n",
        "ax = plt.gca()\n",
        "\n",
        "plt.plot(history.history[\"val_loss\"], label='Validation Loss',marker='.',markevery=1, linewidth=0.7)\n",
        "#plt.plot(f, y1moothed , label='Actor/Critic-FSO/RF',marker='*',linewidth=0.7)\n",
        "handles, labels = ax.get_legend_handles_labels()\n",
        "ax.legend(handles, labels)\n",
        "ax.grid(False)\n",
        "#plt.fill_between(d, 1, c['Loss']+1, color='blue', alpha=.25)\n",
        "%config InlineBackend.figure_format = 'svg'\n",
        "#plt.savefig('/content/content1/LossvsEpisodeAll.svg')\n",
        "#plt.ylim([0, 1])\n",
        "plt.show()\n",
        "#y_smooth = interpolate.interp1d(a['Loss'],b)\n",
        "#plt.plot(make_interp_spline(b, a['Loss']), c='g', marker='.')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "t9NlMMflYlnt"
      },
      "outputs": [],
      "source": [
        "plt.figure(figsize=(6,4))\n",
        "#plt.tight_layout()\n",
        "#plt.yscale('log')\n",
        "plt.xlabel('Episodes')\n",
        "plt.ylabel('Accuracy')\n",
        "\n",
        "#create smooth line chart\n",
        "plt.plot(history.history[\"acc\"],  label='Training Accuracy',marker='1',linewidth=0.7)\n",
        "ax = plt.gca()\n",
        "\n",
        "plt.plot(history.history[\"val_acc\"], label='Validation Accuracy',marker='.',markevery=1, linewidth=0.7)\n",
        "#plt.plot(f, y1moothed , label='Actor/Critic-FSO/RF',marker='*',linewidth=0.7)\n",
        "handles, labels = ax.get_legend_handles_labels()\n",
        "ax.legend(handles, labels)\n",
        "ax.grid(False)\n",
        "#plt.fill_between(d, 1, c['Loss']+1, color='blue', alpha=.25)\n",
        "%config InlineBackend.figure_format = 'svg'\n",
        "#plt.savefig('/content/content1/LossvsEpisodeAll.svg')\n",
        "#plt.ylim([0, 1])\n",
        "plt.show()\n",
        "#y_smooth = interpolate.interp1d(a['Loss'],b)\n",
        "#plt.plot(make_interp_spline(b, a['Loss']), c='g', marker='.')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MTbeVZBsYlnt"
      },
      "outputs": [],
      "source": [
        "graphs[1].node_features()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NLgdur3BYlnt"
      },
      "outputs": [],
      "source": [
        "graphs[1].edge_features()"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.16"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}