{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "359bf9d2",
      "metadata": {
        "id": "359bf9d2"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "import pickle\n",
        "from tqdm import tqdm\n",
        "from datetime import datetime\n",
        "from typing import Tuple, List, Optional, Dict\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from torch.optim.lr_scheduler import ReduceLROnPlateau\n",
        "from torch_geometric.data import Dataset, Data, DataLoader\n",
        "from torch_geometric.nn import (\n",
        "    GCNConv,\n",
        "    GINConv,\n",
        "    global_add_pool,\n",
        "    global_mean_pool,\n",
        "    global_max_pool,\n",
        "    MessagePassing\n",
        ")\n",
        "\n",
        "import deepchem as dc\n",
        "from deepchem.molnet import load_qm7\n",
        "\n",
        "from rdkit import Chem\n",
        "from rdkit.Chem import (\n",
        "    RemoveHs,\n",
        "    AllChem,\n",
        "    Descriptors,\n",
        "    ChemicalFeatures,\n",
        "    Draw\n",
        ")\n",
        "from rdkit import RDLogger\n",
        "\n",
        "import dalex as dx\n",
        "import plotly.graph_objects as go\n",
        "from plotly.subplots import make_subplots\n",
        "\n",
        "from sklearn.metrics import (\n",
        "    mean_squared_error,\n",
        "    r2_score,\n",
        "    mean_absolute_error\n",
        ")\n",
        "\n",
        "from tqdm import tqdm  # For progress bars\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import dalex as dx\n",
        "from rdkit import Chem\n",
        "from rdkit.Chem import Descriptors\n",
        "\n",
        "import traceback\n",
        "\n",
        "# Suppress RDKit warnings\n",
        "RDLogger.DisableLog('rdApp.warning')\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "7f98ef7e",
      "metadata": {
        "id": "7f98ef7e"
      },
      "outputs": [],
      "source": [
        "class MolecularFeatureExtractor:\n",
        "    def __init__(self):\n",
        "        self.atom_list = list(range(1, 119))\n",
        "        self.chirality_list = [\n",
        "            Chem.rdchem.ChiralType.CHI_UNSPECIFIED,\n",
        "            Chem.rdchem.ChiralType.CHI_TETRAHEDRAL_CW,\n",
        "            Chem.rdchem.ChiralType.CHI_TETRAHEDRAL_CCW,\n",
        "            Chem.rdchem.ChiralType.CHI_OTHER\n",
        "        ]\n",
        "        self.bond_list = [\n",
        "            Chem.rdchem.BondType.SINGLE,\n",
        "            Chem.rdchem.BondType.DOUBLE,\n",
        "            Chem.rdchem.BondType.TRIPLE,\n",
        "            Chem.rdchem.BondType.AROMATIC,\n",
        "            Chem.rdchem.BondType.DATIVE\n",
        "        ]\n",
        "        self.bonddir_list = [\n",
        "            Chem.rdchem.BondDir.NONE,\n",
        "            Chem.rdchem.BondDir.ENDUPRIGHT,\n",
        "            Chem.rdchem.BondDir.ENDDOWNRIGHT\n",
        "        ]\n",
        "\n",
        "    def calc_atom_features(self, atom: Chem.Atom) -> Tuple[list, list]:\n",
        "        \"\"\"Calculate atom features with better error handling\"\"\"\n",
        "        try:\n",
        "            # Basic features\n",
        "            atom_feat = [\n",
        "                self.atom_list.index(atom.GetAtomicNum()),\n",
        "                self.chirality_list.index(atom.GetChiralTag())\n",
        "            ]\n",
        "\n",
        "            # Physical features with error handling\n",
        "            phys_feat = []\n",
        "\n",
        "            # Molecular weight contribution\n",
        "            try:\n",
        "                contrib_mw = Descriptors.ExactMolWt(Chem.MolFromSmiles(f'[{atom.GetSymbol()}]'))\n",
        "                phys_feat.append(contrib_mw)\n",
        "            except:\n",
        "                phys_feat.append(0.0)\n",
        "\n",
        "            # LogP contribution\n",
        "            try:\n",
        "                contrib_logp = Descriptors.MolLogP(Chem.MolFromSmiles(f'[{atom.GetSymbol()}]'))\n",
        "                phys_feat.append(contrib_logp)\n",
        "            except:\n",
        "                phys_feat.append(0.0)\n",
        "\n",
        "            # Add other physical properties\n",
        "            phys_feat.extend([\n",
        "                atom.GetFormalCharge(),\n",
        "                int(atom.GetHybridization()),\n",
        "                int(atom.GetIsAromatic()),\n",
        "                atom.GetTotalNumHs(),\n",
        "                atom.GetTotalValence(),\n",
        "                atom.GetDegree()\n",
        "            ])\n",
        "\n",
        "            return atom_feat, phys_feat\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"Error calculating atom features: {e}\")\n",
        "            return [0, 0], [0.0] * 9\n",
        "\n",
        "    def get_atom_features(self, mol: Chem.Mol) -> Tuple[torch.Tensor, torch.Tensor]:\n",
        "        \"\"\"Extract atom features for the whole molecule\"\"\"\n",
        "        atom_feats = []\n",
        "        phys_feats = []\n",
        "\n",
        "        if mol is None:\n",
        "            return torch.tensor([[0, 0]], dtype=torch.long), torch.tensor([[0.0] * 9], dtype=torch.float)\n",
        "\n",
        "        for atom in mol.GetAtoms():\n",
        "            atom_feat, phys_feat = self.calc_atom_features(atom)\n",
        "            atom_feats.append(atom_feat)\n",
        "            phys_feats.append(phys_feat)\n",
        "\n",
        "        x = torch.tensor(atom_feats, dtype=torch.long)\n",
        "        phys = torch.tensor(phys_feats, dtype=torch.float)\n",
        "\n",
        "        return x, phys\n",
        "\n",
        "    def remove_unbonded_hydrogens(mol):\n",
        "        params = Chem.RemoveHsParameters()\n",
        "        params.removeDegreeZero = True\n",
        "        mol = Chem.RemoveHs(mol, params)\n",
        "        return mol\n",
        "\n",
        "\n",
        "    def get_bond_features(self, mol: Chem.Mol) -> Tuple[torch.Tensor, torch.Tensor]:\n",
        "        \"\"\"Extract bond features with better error handling\"\"\"\n",
        "        if mol is None:\n",
        "            return torch.tensor([[0], [0]], dtype=torch.long), torch.tensor([[0.0] * 5], dtype=torch.float)\n",
        "\n",
        "        row, col, edge_feat = [], [], []\n",
        "\n",
        "        for bond in mol.GetBonds():\n",
        "            try:\n",
        "                start, end = bond.GetBeginAtomIdx(), bond.GetEndAtomIdx()\n",
        "\n",
        "                # Add edges in both directions\n",
        "                row += [start, end]\n",
        "                col += [end, start]\n",
        "\n",
        "                # Bond features\n",
        "                bond_type = self.bond_list.index(bond.GetBondType())\n",
        "                bond_dir = self.bonddir_list.index(bond.GetBondDir())\n",
        "\n",
        "                # Calculate additional properties\n",
        "                feat = [\n",
        "                    bond_type,\n",
        "                    bond_dir,\n",
        "                    int(bond.GetIsConjugated()),\n",
        "                    int(self._is_rotatable(bond)),\n",
        "                    self._get_bond_length(mol, start, end)\n",
        "                ]\n",
        "\n",
        "                edge_feat.extend([feat, feat])\n",
        "\n",
        "            except Exception as e:\n",
        "                print(f\"Error processing bond: {e}\")\n",
        "                continue\n",
        "\n",
        "        if not row:  # If no valid bonds were processed\n",
        "            return torch.tensor([[0], [0]], dtype=torch.long), torch.tensor([[0.0] * 5], dtype=torch.float)\n",
        "\n",
        "        edge_index = torch.tensor([row, col], dtype=torch.long)\n",
        "        edge_attr = torch.tensor(edge_feat, dtype=torch.float)\n",
        "\n",
        "        return edge_index, edge_attr\n",
        "\n",
        "    def _is_rotatable(self, bond: Chem.Bond) -> bool:\n",
        "        \"\"\"Check if bond is rotatable\"\"\"\n",
        "        return (bond.GetBondType() == Chem.rdchem.BondType.SINGLE and\n",
        "                not bond.IsInRing() and\n",
        "                len(bond.GetBeginAtom().GetNeighbors()) > 1 and\n",
        "                len(bond.GetEndAtom().GetNeighbors()) > 1)\n",
        "\n",
        "    def _get_bond_length(self, mol: Chem.Mol, start: int, end: int) -> float:\n",
        "        \"\"\"Get bond length with error handling\"\"\"\n",
        "        try:\n",
        "            conf = mol.GetConformer()\n",
        "            if conf.Is3D():\n",
        "                return Chem.rdMolTransforms.GetBondLength(conf, start, end)\n",
        "        except:\n",
        "            pass\n",
        "        return 0.0\n",
        "\n",
        "    def process_molecule(self, smiles: str) -> Data:\n",
        "        \"\"\"Process SMILES string to graph data\"\"\"\n",
        "        try:\n",
        "            mol = Chem.MolFromSmiles(smiles)\n",
        "            if mol is None:\n",
        "                print(f\"Invalid SMILES: {smiles}\")\n",
        "                return None  # Skip invalid molecules\n",
        "            mol = RemoveHs(mol)\n",
        "\n",
        "            # Add explicit hydrogens\n",
        "            mol = Chem.AddHs(mol, addCoords=True)\n",
        "\n",
        "            # Sanitize molecule\n",
        "            Chem.SanitizeMol(mol)\n",
        "\n",
        "            # Check if the molecule has atoms\n",
        "            if mol.GetNumAtoms() == 0:\n",
        "                print(\"Molecule has no atoms, skipping.\")\n",
        "                return None\n",
        "\n",
        "            # Generate 3D coordinates\n",
        "            if not mol.GetNumConformers():\n",
        "                status = AllChem.EmbedMolecule(mol, AllChem.ETKDG())\n",
        "                if status != 0:\n",
        "                    print(\"Failed to generate 3D conformer\")\n",
        "                    return None  # Skip failed molecules\n",
        "\n",
        "                # Try MMFF or UFF optimization\n",
        "                try:\n",
        "                    AllChem.MMFFOptimizeMolecule(mol)\n",
        "                except:\n",
        "                    AllChem.UFFOptimizeMolecule(mol)\n",
        "\n",
        "            # Extract features\n",
        "            x_cat, x_phys = self.get_atom_features(mol)\n",
        "            edge_index, edge_attr = self.get_bond_features(mol)\n",
        "\n",
        "            data = Data(\n",
        "                x_cat=x_cat,\n",
        "                x_phys=x_phys,\n",
        "                edge_index=edge_index,\n",
        "                edge_attr=edge_attr,\n",
        "                num_nodes=x_cat.size(0)\n",
        "            )\n",
        "\n",
        "            # Store the original SMILES string\n",
        "            data._store.smiles = smiles\n",
        "\n",
        "            return data\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"Error processing molecule {smiles}: {e}\")\n",
        "            return None\n",
        ""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "c7fb9684",
      "metadata": {
        "id": "c7fb9684"
      },
      "outputs": [],
      "source": [
        "class GraphDiscriminator(nn.Module):\n",
        "    \"\"\"Reimplementation of original discriminator architecture\"\"\"\n",
        "    def __init__(self, node_dim: int, edge_dim: int, hidden_dim: int = 128, output_dim: int = 128):\n",
        "        super().__init__()\n",
        "\n",
        "        # Feature encoding\n",
        "        self.node_encoder = nn.Sequential(\n",
        "            nn.Linear(node_dim, hidden_dim),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(hidden_dim, hidden_dim)\n",
        "        )\n",
        "\n",
        "        self.edge_encoder = nn.Sequential(\n",
        "            nn.Linear(edge_dim, hidden_dim),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(hidden_dim, hidden_dim)\n",
        "        )\n",
        "\n",
        "        # Graph convolution layers\n",
        "        self.conv1 = GCNConv(hidden_dim, hidden_dim)\n",
        "        self.conv2 = GCNConv(hidden_dim, hidden_dim)\n",
        "        self.conv3 = GCNConv(hidden_dim, output_dim)\n",
        "\n",
        "        # Projection head\n",
        "        self.projection = nn.Sequential(\n",
        "            nn.Linear(output_dim, hidden_dim),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(hidden_dim, output_dim)\n",
        "        )\n",
        "\n",
        "    def forward(self, data):\n",
        "        x = torch.cat([data.x_cat.float(), data.x_phys], dim=-1)\n",
        "        edge_index = data.edge_index\n",
        "        edge_attr = data.edge_attr.float()\n",
        "        batch = data.batch\n",
        "\n",
        "        # Initial feature encoding\n",
        "        x = self.node_encoder(x)\n",
        "        edge_attr = self.edge_encoder(edge_attr)\n",
        "\n",
        "        # Graph convolutions\n",
        "        x = F.relu(self.conv1(x, edge_index))\n",
        "        x = F.relu(self.conv2(x, edge_index))\n",
        "        x = self.conv3(x, edge_index)\n",
        "\n",
        "        # Global pooling\n",
        "        x = global_mean_pool(x, batch)\n",
        "\n",
        "        # Projection\n",
        "        x = self.projection(x)\n",
        "\n",
        "        return x\n",
        "\n",
        "# Load Encoder\n",
        "def load_encoder(model_path, device='cpu'):\n",
        "    \"\"\"Load trained encoder\"\"\"\n",
        "    checkpoint = torch.load(model_path, map_location=device)\n",
        "    encoder = GraphDiscriminator(\n",
        "        node_dim=checkpoint['model_info'].get('node_dim'),\n",
        "        edge_dim=checkpoint['model_info'].get('edge_dim'),\n",
        "        hidden_dim=checkpoint['model_info'].get('hidden_dim', 128),\n",
        "        output_dim=checkpoint['model_info'].get('output_dim', 128)\n",
        "    )\n",
        "    encoder.load_state_dict(checkpoint['encoder_state_dict'])\n",
        "    encoder.eval()\n",
        "    return encoder.to(device)\n",
        "\n",
        "# Load Embeddings\n",
        "def load_embeddings(filepath):\n",
        "    \"\"\"Load embeddings and labels\"\"\"\n",
        "    with open(filepath, 'rb') as f:\n",
        "        data = pickle.load(f)\n",
        "    return data['embeddings'], data['labels']\n",
        "\n",
        "# Paths from your saved model\n",
        "encoder_path = '../checkpoints/encoders/final_encoder_20250216_111050.pt'\n",
        "embedding_path = '../embeddings/final_embeddings_20250216_111005.pkl'\n",
        "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "\n",
        "# Load encoder and embeddings\n",
        "encoder = load_encoder(encoder_path, device)\n",
        "embeddings, graph_data = load_embeddings(embedding_path)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "4fe4a1f2",
      "metadata": {
        "id": "4fe4a1f2"
      },
      "outputs": [],
      "source": [
        "class GINLayer(torch.nn.Module):\n",
        "    def __init__(self, in_channels: int, out_channels: int, edge_dim: int):\n",
        "        super().__init__()\n",
        "        self.mlp = torch.nn.Sequential(\n",
        "            torch.nn.Linear(in_channels + edge_dim, out_channels),\n",
        "            torch.nn.BatchNorm1d(out_channels),\n",
        "            torch.nn.ReLU(),\n",
        "            torch.nn.Linear(out_channels, out_channels)\n",
        "        )\n",
        "        self.edge_encoder = torch.nn.Linear(edge_dim, edge_dim)\n",
        "\n",
        "    def forward(self, x, edge_index, edge_attr):\n",
        "        edge_embedding = self.edge_encoder(edge_attr)\n",
        "        row, col = edge_index\n",
        "        out = torch.cat([x[row], edge_embedding], dim=1)\n",
        "        out = self.mlp(out)\n",
        "\n",
        "        # Using PyTorch's native scatter_add\n",
        "        output = torch.zeros_like(x)\n",
        "        output.scatter_add_(0, col.unsqueeze(-1).expand(-1, out.size(-1)), out)\n",
        "        return output\n",
        "\n",
        "class HybridGNNRegressor(torch.nn.Module):\n",
        "    def __init__(self, encoder, node_dim: int, edge_dim: int, hidden_dim: int = 128,\n",
        "                 num_layers: int = 3, num_tasks: int = 1, dropout: float = 0.3):\n",
        "        super().__init__()\n",
        "        self.encoder = encoder\n",
        "        self.encoder.eval()  # Freeze encoder\n",
        "\n",
        "        # Initial node embedding (same as before)\n",
        "        self.node_embedding = torch.nn.Sequential(\n",
        "            torch.nn.Linear(node_dim, hidden_dim),\n",
        "            torch.nn.BatchNorm1d(hidden_dim),\n",
        "            torch.nn.ReLU(),\n",
        "            torch.nn.Dropout(dropout)\n",
        "        )\n",
        "\n",
        "        # GIN layers (same as before)\n",
        "        self.gin_layers = torch.nn.ModuleList()\n",
        "        for _ in range(num_layers):\n",
        "            gin_layer = GINLayer(hidden_dim, hidden_dim, edge_dim)\n",
        "            self.gin_layers.append(gin_layer)\n",
        "\n",
        "        # Pooling attention (same as before)\n",
        "        self.pool_attention = torch.nn.Sequential(\n",
        "            torch.nn.Linear(hidden_dim, 3),\n",
        "            torch.nn.Softmax(dim=1)\n",
        "        )\n",
        "\n",
        "        # Combine pretrained embeddings with GNN output (same as before)\n",
        "        encoder_out_dim = encoder.projection[-1].out_features\n",
        "        self.combination_layer = torch.nn.Sequential(\n",
        "            torch.nn.Linear(hidden_dim + encoder_out_dim, hidden_dim),\n",
        "            torch.nn.ReLU(),\n",
        "            torch.nn.Dropout(dropout)\n",
        "        )\n",
        "\n",
        "        # Regression head - modified for regression output\n",
        "        self.regression_head = torch.nn.Sequential(\n",
        "            torch.nn.Linear(hidden_dim, hidden_dim // 2),\n",
        "            torch.nn.ReLU(),\n",
        "            torch.nn.Dropout(dropout),\n",
        "            torch.nn.Linear(hidden_dim // 2, hidden_dim // 4),\n",
        "            torch.nn.ReLU(),\n",
        "            torch.nn.Linear(hidden_dim // 4, num_tasks)  # No activation for regression\n",
        "        )\n",
        "\n",
        "    def forward(self, data):\n",
        "        # Forward pass remains the same until the final layer\n",
        "        with torch.no_grad():\n",
        "            pretrained_emb = self.encoder(data)\n",
        "\n",
        "        x = torch.cat([data.x_cat.float(), data.x_phys], dim=-1)\n",
        "        x = self.node_embedding(x)\n",
        "        edge_index = data.edge_index\n",
        "        edge_attr = data.edge_attr\n",
        "        batch = data.batch\n",
        "\n",
        "        for gin_layer in self.gin_layers:\n",
        "            x_new = gin_layer(x, edge_index, edge_attr)\n",
        "            x = x + x_new\n",
        "\n",
        "        pool_attention = self.pool_attention(x)\n",
        "\n",
        "        x_mean = global_mean_pool(x * pool_attention[:, 0:1], batch)\n",
        "        x_max = global_max_pool(x * pool_attention[:, 1:2], batch)\n",
        "        x_sum = global_add_pool(x * pool_attention[:, 2:3], batch)\n",
        "\n",
        "        x_pooled = x_mean + x_max + x_sum\n",
        "\n",
        "        combined = self.combination_layer(\n",
        "            torch.cat([x_pooled, pretrained_emb], dim=1)\n",
        "        )\n",
        "\n",
        "        # Regression output\n",
        "        return self.regression_head(combined)\n",
        "\n",
        "def train_hybrid_model_regression(model, train_loader, val_loader, device,\n",
        "                                num_epochs=100, lr=1e-3, weight_decay=1e-4):\n",
        "    print(\"Initializing training...\")\n",
        "    print(f\"Number of training batches: {len(train_loader)}\")\n",
        "    print(f\"Number of validation batches: {len(val_loader)}\")\n",
        "\n",
        "    optimizer = torch.optim.AdamW(model.parameters(), lr=lr, weight_decay=weight_decay)\n",
        "    scheduler = ReduceLROnPlateau(optimizer, mode='min', factor=0.5,\n",
        "                                patience=5, verbose=True)\n",
        "\n",
        "    # Use MSE Loss for regression\n",
        "    criterion = torch.nn.MSELoss()\n",
        "    best_val_rmse = float('inf')\n",
        "    best_model = None\n",
        "    patience = 10\n",
        "    patience_counter = 0\n",
        "\n",
        "    print(\"\\nStarting training loop...\")\n",
        "    for epoch in range(num_epochs):\n",
        "        # Training\n",
        "        model.train()\n",
        "        train_loss = 0\n",
        "        train_predictions = []\n",
        "        train_targets = []\n",
        "\n",
        "        for batch_idx, batch in enumerate(train_loader):\n",
        "            batch = batch.to(device)\n",
        "            optimizer.zero_grad()\n",
        "\n",
        "            try:\n",
        "                outputs = model(batch)\n",
        "                targets = batch.y.view(-1, 1)\n",
        "                loss = criterion(outputs, targets)\n",
        "\n",
        "                loss.backward()\n",
        "                torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n",
        "                optimizer.step()\n",
        "\n",
        "                train_loss += loss.item()\n",
        "                train_predictions.extend(outputs.detach().cpu().numpy())\n",
        "                train_targets.extend(targets.cpu().numpy())\n",
        "\n",
        "            except Exception as e:\n",
        "                print(f\"Error in batch {batch_idx}: {str(e)}\")\n",
        "                continue\n",
        "\n",
        "        # Validation\n",
        "        model.eval()\n",
        "        val_loss = 0\n",
        "        val_predictions = []\n",
        "        val_targets = []\n",
        "\n",
        "        with torch.no_grad():\n",
        "            for batch in val_loader:\n",
        "                batch = batch.to(device)\n",
        "                try:\n",
        "                    outputs = model(batch)\n",
        "                    targets = batch.y.view(-1, 1)\n",
        "                    loss = criterion(outputs, targets)\n",
        "\n",
        "                    val_loss += loss.item()\n",
        "                    val_predictions.extend(outputs.cpu().numpy())\n",
        "                    val_targets.extend(targets.cpu().numpy())\n",
        "\n",
        "                except Exception as e:\n",
        "                    print(f\"Error in validation: {str(e)}\")\n",
        "                    continue\n",
        "\n",
        "        # Calculate metrics\n",
        "        try:\n",
        "            train_rmse = np.sqrt(mean_squared_error(train_targets, train_predictions))\n",
        "            train_r2 = r2_score(train_targets, train_predictions)\n",
        "            train_mae = mean_absolute_error(train_targets, train_predictions)\n",
        "\n",
        "            val_rmse = np.sqrt(mean_squared_error(val_targets, val_predictions))\n",
        "            val_r2 = r2_score(val_targets, val_predictions)\n",
        "            val_mae = mean_absolute_error(val_targets, val_predictions)\n",
        "\n",
        "            print(f'\\nEpoch {epoch+1}/{num_epochs}:')\n",
        "            print(f'Train Loss: {train_loss/len(train_loader):.4f}')\n",
        "            print(f'Train RMSE: {train_rmse:.4f}')\n",
        "            print(f'Train R2: {train_r2:.4f}')\n",
        "            print(f'Train MAE: {train_mae:.4f}')\n",
        "            print(f'Val Loss: {val_loss/len(val_loader):.4f}')\n",
        "            print(f'Val RMSE: {val_rmse:.4f}')\n",
        "            print(f'Val R2: {val_r2:.4f}')\n",
        "            print(f'Val MAE: {val_mae:.4f}')\n",
        "            print(f'Learning Rate: {optimizer.param_groups[0][\"lr\"]:.6f}')\n",
        "\n",
        "            # Save best model based on validation RMSE\n",
        "            if val_rmse < best_val_rmse:\n",
        "                best_val_rmse = val_rmse\n",
        "                best_model = {\n",
        "                    'state_dict': model.state_dict(),\n",
        "                    'val_rmse': val_rmse,\n",
        "                    'epoch': epoch\n",
        "                }\n",
        "                patience_counter = 0\n",
        "                print(\"New best model saved!\")\n",
        "            else:\n",
        "                patience_counter += 1\n",
        "                if patience_counter >= patience:\n",
        "                    print(f'Early stopping triggered after epoch {epoch+1}')\n",
        "                    break\n",
        "\n",
        "            # Update learning rate based on validation RMSE\n",
        "            scheduler.step(val_rmse)\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"Error calculating metrics: {str(e)}\")\n",
        "            continue\n",
        "\n",
        "    return best_model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "3ba9991a",
      "metadata": {
        "id": "3ba9991a"
      },
      "outputs": [],
      "source": [
        "class QM7Dataset(Dataset):\n",
        "    \"\"\"Custom PyTorch Geometric dataset for ClinTox with better error handling\"\"\"\n",
        "    def __init__(self, smiles_list, labels, feature_extractor):\n",
        "        super().__init__()\n",
        "        self.smiles_list = []\n",
        "        self.labels = []\n",
        "        self.feature_extractor = feature_extractor\n",
        "        self.processed_data = []\n",
        "\n",
        "        # Process all molecules\n",
        "        print(\"Processing molecules...\")\n",
        "        for idx, smiles in enumerate(tqdm(smiles_list)):\n",
        "            data = self.feature_extractor.process_molecule(smiles)\n",
        "            if data is not None:\n",
        "                self.processed_data.append(data)\n",
        "                self.smiles_list.append(smiles)\n",
        "                self.labels.append(labels[idx])\n",
        "\n",
        "        # Convert labels to tensor\n",
        "        self.labels = torch.tensor(self.labels, dtype=torch.float)\n",
        "        print(f\"Successfully processed {len(self.processed_data)} out of {len(smiles_list)} molecules\")\n",
        "\n",
        "    def len(self):\n",
        "        return len(self.processed_data)\n",
        "\n",
        "    def get(self, idx):\n",
        "        data = self.processed_data[idx]\n",
        "        data.y = self.labels[idx].view(1, -1)  # Reshape to [1, num_tasks]\n",
        "        # Store SMILES as a property of the Data object\n",
        "        data._store.smiles = self.smiles_list[idx]\n",
        "\n",
        "        return data\n",
        "\n",
        "class RegressionHead(nn.Module):\n",
        "    \"\"\"Enhanced regression head with residual connections\"\"\"\n",
        "    def __init__(self, input_dim: int, hidden_dims: List[int] = [256, 128, 64],\n",
        "                 num_tasks: int = 1, dropout_rate: float = 0.3):\n",
        "        super().__init__()\n",
        "\n",
        "        self.input_bn = nn.BatchNorm1d(input_dim)\n",
        "\n",
        "        # Shared layers with residual connections\n",
        "        self.shared_layers = nn.ModuleList()\n",
        "        current_dim = input_dim\n",
        "\n",
        "        for hidden_dim in hidden_dims:\n",
        "            self.shared_layers.append(nn.Sequential(\n",
        "                nn.Linear(current_dim, hidden_dim),\n",
        "                nn.BatchNorm1d(hidden_dim),\n",
        "                nn.ReLU(),\n",
        "                nn.Dropout(dropout_rate)\n",
        "            ))\n",
        "            if current_dim == hidden_dim:  # Add residual connection\n",
        "                self.shared_layers.append(lambda x: x)\n",
        "            current_dim = hidden_dim\n",
        "\n",
        "        # Regression-specific layers\n",
        "        self.regressor = nn.Sequential(\n",
        "            nn.Linear(hidden_dims[-1], hidden_dims[-1] // 2),\n",
        "            nn.BatchNorm1d(hidden_dims[-1] // 2),\n",
        "            nn.ReLU(),\n",
        "            nn.Dropout(dropout_rate),\n",
        "            nn.Linear(hidden_dims[-1] // 2, num_tasks)  # Direct regression output\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.input_bn(x)\n",
        "\n",
        "        # Shared feature extraction\n",
        "        for layer in self.shared_layers:\n",
        "            if isinstance(layer, nn.Sequential):\n",
        "                x = x + layer(x) if x.size() == layer(x).size() else layer(x)\n",
        "            else:\n",
        "                x = layer(x)\n",
        "\n",
        "        # Regression prediction\n",
        "        return self.regressor(x)\n",
        "\n",
        "def load_pretrained_encoder(encoder_path: str, device: str = 'cuda'):\n",
        "    \"\"\"Load the pretrained encoder\"\"\"\n",
        "    checkpoint = torch.load(encoder_path, map_location=device)\n",
        "    model_info = checkpoint['model_info']\n",
        "\n",
        "    encoder = GraphDiscriminator(\n",
        "        node_dim=model_info['node_dim'],\n",
        "        edge_dim=model_info['edge_dim'],\n",
        "        hidden_dim=model_info['hidden_dim'],\n",
        "        output_dim=model_info['output_dim']\n",
        "    ).to(device)\n",
        "\n",
        "    encoder.load_state_dict(checkpoint['encoder_state_dict'])\n",
        "    return encoder, model_info\n",
        "\n",
        "def train_regressor_with_scheduler(encoder, regression_head, train_loader, val_loader,\n",
        "                                 device, num_epochs=100):\n",
        "    \"\"\"Enhanced training function for regression with learning rate scheduling\"\"\"\n",
        "    optimizer = torch.optim.AdamW(regression_head.parameters(), lr=1e-3, weight_decay=0.01)\n",
        "    scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode='min',\n",
        "                                                         factor=0.5, patience=5,\n",
        "                                                         verbose=True)\n",
        "    criterion = nn.MSELoss()  # Use MSE loss for regression\n",
        "\n",
        "    # Freeze encoder\n",
        "    encoder.eval()\n",
        "    for param in encoder.parameters():\n",
        "        param.requires_grad = False\n",
        "\n",
        "    best_val_rmse = float('inf')\n",
        "    best_model = None\n",
        "    patience = 10\n",
        "    patience_counter = 0\n",
        "\n",
        "    for epoch in range(num_epochs):\n",
        "        # Training\n",
        "        regression_head.train()\n",
        "        train_loss = 0\n",
        "        y_true_train = []\n",
        "        y_pred_train = []\n",
        "\n",
        "        for batch in train_loader:\n",
        "            batch = batch.to(device)\n",
        "            with torch.no_grad():\n",
        "                embeddings = encoder(batch)\n",
        "\n",
        "            outputs = regression_head(embeddings)\n",
        "            targets = batch.y.view(-1, 1)\n",
        "\n",
        "            loss = criterion(outputs, targets)\n",
        "\n",
        "            optimizer.zero_grad()\n",
        "            loss.backward()\n",
        "            torch.nn.utils.clip_grad_norm_(regression_head.parameters(), max_norm=1.0)\n",
        "            optimizer.step()\n",
        "\n",
        "            train_loss += loss.item()\n",
        "            y_true_train.extend(targets.cpu().numpy())\n",
        "            y_pred_train.extend(outputs.detach().cpu().numpy())\n",
        "\n",
        "        # Validation\n",
        "        regression_head.eval()\n",
        "        val_loss = 0\n",
        "        y_true_val = []\n",
        "        y_pred_val = []\n",
        "\n",
        "        with torch.no_grad():\n",
        "            for batch in val_loader:\n",
        "                batch = batch.to(device)\n",
        "                embeddings = encoder(batch)\n",
        "                outputs = regression_head(embeddings)\n",
        "                targets = batch.y.view(-1, 1)\n",
        "                loss = criterion(outputs, targets)\n",
        "\n",
        "                val_loss += loss.item()\n",
        "                y_true_val.extend(targets.cpu().numpy())\n",
        "                y_pred_val.extend(outputs.cpu().numpy())\n",
        "\n",
        "        # Calculate regression metrics\n",
        "        train_rmse = np.sqrt(mean_squared_error(y_true_train, y_pred_train))\n",
        "        train_r2 = r2_score(y_true_train, y_pred_train)\n",
        "        train_mae = mean_absolute_error(y_true_train, y_pred_train)\n",
        "\n",
        "        val_rmse = np.sqrt(mean_squared_error(y_true_val, y_pred_val))\n",
        "        val_r2 = r2_score(y_true_val, y_pred_val)\n",
        "        val_mae = mean_absolute_error(y_true_val, y_pred_val)\n",
        "\n",
        "        # Update learning rate based on validation RMSE\n",
        "        scheduler.step(val_rmse)\n",
        "\n",
        "        print(f'Epoch {epoch+1}/{num_epochs}:')\n",
        "        print(f'Train Loss: {train_loss/len(train_loader):.4f}')\n",
        "        print(f'Train RMSE: {train_rmse:.4f}')\n",
        "        print(f'Train R²: {train_r2:.4f}')\n",
        "        print(f'Train MAE: {train_mae:.4f}')\n",
        "        print(f'Val Loss: {val_loss/len(val_loader):.4f}')\n",
        "        print(f'Val RMSE: {val_rmse:.4f}')\n",
        "        print(f'Val R²: {val_r2:.4f}')\n",
        "        print(f'Val MAE: {val_mae:.4f}')\n",
        "        print(f'Learning Rate: {optimizer.param_groups[0][\"lr\"]:.6f}')\n",
        "\n",
        "        # Save best model based on validation RMSE\n",
        "        if val_rmse < best_val_rmse:\n",
        "            best_val_rmse = val_rmse\n",
        "            best_model = regression_head.state_dict()\n",
        "            patience_counter = 0\n",
        "            print(\"New best model saved!\")\n",
        "        else:\n",
        "            patience_counter += 1\n",
        "            if patience_counter >= patience:\n",
        "                print(f'Early stopping triggered after epoch {epoch+1}')\n",
        "                break\n",
        "\n",
        "    return best_model, best_val_rmse\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "9e722d57",
      "metadata": {
        "id": "9e722d57"
      },
      "outputs": [],
      "source": [
        "class SimpleWrapper:\n",
        "    \"\"\"Wrapper class for model predictions in DALEX\"\"\"\n",
        "    def __init__(self, model, device, preprocessed_data=None):\n",
        "        print(\"\\nInitializing SimpleWrapper:\")\n",
        "        print(f\"Model type: {type(model)}\")\n",
        "        print(f\"Device: {device}\")\n",
        "        print(f\"Initial preprocessed_data: {type(preprocessed_data) if preprocessed_data is not None else None}\")\n",
        "\n",
        "        self.model = model\n",
        "        self.device = device\n",
        "        self.preprocessed_data = preprocessed_data\n",
        "\n",
        "    def predict(self, X, *args):\n",
        "        \"\"\"Convert pandas/numpy input to model predictions\"\"\"\n",
        "        print(\"\\nSimpleWrapper predict called:\")\n",
        "        print(f\"Input type: {type(X)}\")\n",
        "        print(f\"Input shape: {X.shape if hasattr(X, 'shape') else 'No shape'}\")\n",
        "        print(f\"Additional args: {args}\")\n",
        "\n",
        "        try:\n",
        "            if isinstance(X, SimpleWrapper):\n",
        "                print(\"Input is SimpleWrapper, returning preprocessed_data\")\n",
        "                return self.preprocessed_data\n",
        "\n",
        "            if isinstance(X, (pd.DataFrame, np.ndarray)):\n",
        "                print(f\"Input is {type(X)}, returning slice of preprocessed_data\")\n",
        "                if self.preprocessed_data is not None:\n",
        "                    return self.preprocessed_data[:len(X)]\n",
        "                else:\n",
        "                    print(\"Warning: No preprocessed_data available\")\n",
        "                    return np.zeros(len(X))\n",
        "\n",
        "            print(f\"Unhandled input type: {type(X)}\")\n",
        "            return np.zeros(1)\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"Error in predict: {str(e)}\")\n",
        "            traceback.print_exc()\n",
        "            if hasattr(X, '__len__'):\n",
        "                return np.zeros(len(X))\n",
        "            return np.zeros(1)\n",
        "\n",
        "class MolecularRegressionFairnessAnalyzer:\n",
        "    def __init__(self, model, test_loader, device):\n",
        "        self.model = model\n",
        "        self.test_loader = test_loader\n",
        "        self.device = device\n",
        "        self.predictions = None\n",
        "        self.true_values = None\n",
        "        self.mol_features = None\n",
        "\n",
        "    def extract_molecular_features(self, smiles: str) -> Dict:\n",
        "        \"\"\"Extract QM7 solubility-relevant features and functional groups\"\"\"\n",
        "        mol = Chem.MolFromSmiles(smiles)\n",
        "        if mol is None:\n",
        "            return None\n",
        "\n",
        "        # QM7-relevant molecular descriptors\n",
        "        logp = Descriptors.MolLogP(mol)          # Partition coefficient\n",
        "        tpsa = Descriptors.TPSA(mol)             # Topological Polar Surface Area\n",
        "\n",
        "        mw = Descriptors.ExactMolWt(mol)         # Molecular Weight (may affect solubility)\n",
        "        rotatable_bonds = Descriptors.NumRotatableBonds(mol)  # Flexibility affects solubility\n",
        "        num_atoms = mol.GetNumAtoms()            # Total number of atoms\n",
        "        num_heavy_atoms = mol.GetNumHeavyAtoms() # Number of heavy atoms (not H)\n",
        "        rotatable_bonds = Descriptors.NumRotatableBonds(mol)\n",
        "        # Get atomic symbols to check allowed atoms in QM7\n",
        "        atomic_symbols = set(atom.GetSymbol() for atom in mol.GetAtoms())\n",
        "\n",
        "        # Functional group analysis with SMARTS patterns\n",
        "        functional_groups = {\n",
        "            'primary_amine': '[NX3H2]',\n",
        "            'secondary_amine': '[NX3H1]',\n",
        "            'tertiary_amine': '[NX3H0]',\n",
        "            'amide': '[NX3][CX3](=[OX1])',\n",
        "            'chloro': '[Cl]',\n",
        "            'bromo': '[Br]',\n",
        "            'fluoro': '[F]',\n",
        "            'iodo': '[I]',\n",
        "            'alcohol': '[OH]',\n",
        "            'phenol': '[OH1][c]',\n",
        "            'ether': '[OD2]([#6])[#6]',\n",
        "            'ester': '[#6][CX3](=O)[OX2H0][#6]',\n",
        "            'carbonyl': '[CX3]=O',\n",
        "            'aldehyde': '[CX3H1](=O)',\n",
        "            'ketone': '[#6][CX3](=O)[#6]',\n",
        "            'carboxyl': '[CX3](=O)[OX2H1]',\n",
        "            'acyl_halide': '[CX3](=[OX1])[F,Cl,Br,I]',\n",
        "            'phosphate': '[$(P(=[OX1])([OX2H,OX1-])([OX2H,OX1-])[OX2H,OX1-])]',\n",
        "            'sulfate': '[$(S(=O)(=O)(O)[O-,OH])]',\n",
        "            'sulfonamide': '[#16X4]([NX3])(=[OX1])(=[OX1])',\n",
        "            'nitro': '[N+](=O)[O-]'\n",
        "        }\n",
        "\n",
        "        # Calculate functional group counts\n",
        "        fg_counts = {}\n",
        "        for name, smarts in functional_groups.items():\n",
        "            pattern = Chem.MolFromSmarts(smarts)\n",
        "            if pattern:\n",
        "                matches = mol.GetSubstructMatches(pattern)\n",
        "                fg_counts[f'{name}_count'] = len(matches)\n",
        "\n",
        "        # Create features dictionary with QM7-specific criteria\n",
        "        features = {\n",
        "            'MW': mw,\n",
        "            'NumAtoms': num_atoms,\n",
        "            'NumHeavyAtoms': num_heavy_atoms,\n",
        "            'RotBonds': rotatable_bonds,\n",
        "            'QM7_violations': sum([\n",
        "                num_heavy_atoms > 23,     # QM7 limit on heavy atoms\n",
        "                mw > 200,                # Typical QM7 MW upper limit\n",
        "                num_atoms > 30,          # Typical QM7 total atoms limit\n",
        "                not set(atomic_symbols).issubset({'C', 'H', 'N', 'O', 'F'})  # Only allowed atoms\n",
        "            ]),\n",
        "            'QM7_score': sum([\n",
        "                num_heavy_atoms <= 23,    # Within QM7 heavy atom limit\n",
        "                mw <= 250,               # Within typical MW range\n",
        "                num_atoms <= 30,         # Within total atoms limit\n",
        "                'C' in atomic_symbols,   # Contains Carbon\n",
        "                set(atomic_symbols).issubset({'C', 'H', 'N', 'O', 'F'})  # Only allowed atoms\n",
        "            ]) / 5.0  # Normalized score (5 criteria)\n",
        "        }\n",
        "\n",
        "        # Add functional group counts to features\n",
        "        features.update(fg_counts)\n",
        "\n",
        "        # Debug print\n",
        "#         print(f\"Number of functional groups found: {len(fg_counts)}\")\n",
        "#         print(f\"Total number of features: {len(features)}\")\n",
        "\n",
        "        return features\n",
        "\n",
        "    def analyze(self):\n",
        "        \"\"\"Analyze fairness based on QM7 conditions\"\"\"\n",
        "        try:\n",
        "            print(\"\\n=== Starting analyze method ===\")\n",
        "\n",
        "            # Collect predictions and data\n",
        "            with torch.no_grad():\n",
        "                predictions = []\n",
        "                true_values = []\n",
        "                features_list = []\n",
        "\n",
        "                for batch in self.test_loader:\n",
        "                    batch = batch.to(self.device)\n",
        "                    outputs = self.model(batch)\n",
        "                    predictions.extend(outputs.cpu().numpy().flatten())\n",
        "                    true_values.extend(batch.y.cpu().numpy().flatten())\n",
        "\n",
        "                    if hasattr(batch, '_store') and hasattr(batch._store, 'smiles'):\n",
        "                        for smiles in batch._store.smiles:\n",
        "                            feat = self.extract_molecular_features(smiles)\n",
        "                            if feat:\n",
        "                                features_list.append(feat)\n",
        "\n",
        "            # Convert to DataFrame\n",
        "            X_test = pd.DataFrame(features_list)\n",
        "            y_test = np.array(true_values)\n",
        "            predictions = np.array(predictions)\n",
        "\n",
        "            # Create explainer\n",
        "            wrapped_model = SimpleWrapper(self.model, self.device)\n",
        "            wrapped_model.preprocessed_data = predictions\n",
        "\n",
        "            explainer = dx.Explainer(\n",
        "                model=wrapped_model,\n",
        "                data=X_test,\n",
        "                y=y_test,\n",
        "                predict_function=wrapped_model.predict,\n",
        "                model_type='regression',\n",
        "                verbose=True\n",
        "            )\n",
        "\n",
        "            fairness_results = {}\n",
        "\n",
        "            # Calculate fairness metrics for all molecular features\n",
        "            features_to_analyze = {\n",
        "                'MW': {'threshold': 500, 'direction': '>'},  # MW > 500\n",
        "                'LogP': {'threshold': 3, 'direction': '>'},  # LogP > 3\n",
        "                'TPSA': {'threshold': 75, 'direction': '<='},  # TPSA <= 75\n",
        "                'RotBonds': {'threshold': 10, 'direction': '>'},  # RotBonds > 10\n",
        "            }\n",
        "\n",
        "            # Add functional groups\n",
        "            for col in X_test.columns:\n",
        "                if col.endswith('_count'):\n",
        "                    # For functional groups, use median as threshold\n",
        "                    features_to_analyze[col] = {\n",
        "                        'threshold': X_test[col].median(),\n",
        "                        'direction': '>'\n",
        "                    }\n",
        "\n",
        "            print(\"\\nAnalyzing features:\")\n",
        "            for feature, params in features_to_analyze.items():\n",
        "                try:\n",
        "                    if feature in X_test.columns:\n",
        "                        print(f\"\\nProcessing {feature}:\")\n",
        "                        threshold = params['threshold']\n",
        "                        direction = params['direction']\n",
        "\n",
        "                        # Create protected groups based on threshold and direction\n",
        "                        if direction == '>':\n",
        "                            protected = pd.Series('compliant', index=X_test.index)\n",
        "                            protected[X_test[feature] > threshold] = 'non_compliant'\n",
        "                        else:  # '<='\n",
        "                            protected = pd.Series('compliant', index=X_test.index)\n",
        "                            protected[X_test[feature] <= threshold] = 'non_compliant'\n",
        "\n",
        "                        # Calculate fairness metrics\n",
        "                        f_metrics = explainer.model_fairness(\n",
        "                            protected=protected,\n",
        "                            privileged='compliant'\n",
        "                        )\n",
        "\n",
        "                        print(f\"{feature} groups distribution:\")\n",
        "                        print(protected.value_counts())\n",
        "                        print(\"Percentage:\", protected.value_counts(normalize=True).round(3) * 100)\n",
        "                        print(f\"{feature} fairness check:\")\n",
        "                        print(\"f_metrics.fairness_check() :\",f_metrics.fairness_check())\n",
        "\n",
        "\n",
        "                        fairness_results[feature] = f_metrics\n",
        "                        print(\"fairness_results[feature]: \",fairness_results[feature])\n",
        "\n",
        "                        # Plot fairness metrics\n",
        "                        plt.figure(figsize=(10, 6))\n",
        "                        f_metrics.plot()\n",
        "                        f_metrics.plot(type='density')\n",
        "                        plt.show()\n",
        "\n",
        "                except Exception as e:\n",
        "                    print(f\"Error analyzing {feature}: {str(e)}\")\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "            try:\n",
        "                self.visualize_fairness_metrics(fairness_results)\n",
        "            except Exception as e:\n",
        "                print(f\"Error in visualization: {str(e)}\")\n",
        "\n",
        "            return fairness_results\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"\\nError in analyze method: {str(e)}\")\n",
        "            traceback.print_exc()\n",
        "            return None\n",
        "\n",
        "    def visualize_fairness_metrics(self, fairness_results):\n",
        "        \"\"\"Create heatmap visualization of fairness metrics\"\"\"\n",
        "        try:\n",
        "            # Create DataFrame to store metrics\n",
        "            metrics_df = pd.DataFrame(columns=['Independence', 'Separation', 'Sufficiency'])\n",
        "\n",
        "            print(\"\\nDebugging fairness results:\")\n",
        "            print(f\"Number of features to process: {len(fairness_results)}\")\n",
        "\n",
        "            for feature, f_obj in fairness_results.items():\n",
        "                print(f\"\\n{'='*50}\")\n",
        "                print(f\"Processing feature: {feature}\")\n",
        "\n",
        "                # Get clean feature name\n",
        "                feature_name = feature.replace('_count', '').replace('_', ' ').title()\n",
        "                print(f\"Cleaned feature name: {feature_name}\")\n",
        "\n",
        "                # Access the metrics data directly\n",
        "                if hasattr(f_obj, 'result'):\n",
        "                    metrics_data = f_obj.result\n",
        "                    print(\"\\nMetrics data found:\")\n",
        "                    print(metrics_data)\n",
        "\n",
        "                    if (isinstance(metrics_data, pd.DataFrame) and\n",
        "                        not metrics_data.empty and\n",
        "                        'non_compliant' in metrics_data.index):\n",
        "\n",
        "                        metrics = metrics_data.loc['non_compliant']\n",
        "                        print(\"\\nExtracted metrics:\")\n",
        "                        print(f\"Independence: {metrics['independence']:.4f}\")\n",
        "                        print(f\"Separation: {metrics['separation']:.4f}\")\n",
        "                        print(f\"Sufficiency: {metrics['sufficiency']:.4f}\")\n",
        "\n",
        "                        # Store metrics\n",
        "                        metrics_df.loc[feature_name] = [\n",
        "                            metrics['independence'],\n",
        "                            metrics['separation'],\n",
        "                            metrics['sufficiency']\n",
        "                        ]\n",
        "                    else:\n",
        "                        print(\"No valid metrics in result\")\n",
        "                        metrics_df.loc[feature_name] = [0.0, 0.0, 0.0]\n",
        "                else:\n",
        "                    print(\"No result attribute found\")\n",
        "                    metrics_df.loc[feature_name] = [0.0, 0.0, 0.0]\n",
        "\n",
        "#                 print(\"\\nCurrent metrics DataFrame:\")\n",
        "#                 print(metrics_df)\n",
        "\n",
        "            print(\"\\n\" + \"=\"*50)\n",
        "            print(\"Final Metrics DataFrame:\")\n",
        "            print(metrics_df)\n",
        "\n",
        "            if not metrics_df.empty:\n",
        "                # Sort by average deviation from 1.0 (only for non-zero values)\n",
        "                metrics_df['avg_effect'] = abs(metrics_df - 1.0).mean(axis=1)\n",
        "                metrics_df = metrics_df.sort_values('avg_effect', ascending=False)\n",
        "                metrics_df = metrics_df.drop('avg_effect', axis=1)\n",
        "\n",
        "                # Create heatmap\n",
        "                plt.figure(figsize=(12, len(metrics_df) * 0.5))\n",
        "                sns.heatmap(metrics_df,\n",
        "                           annot=True,\n",
        "                           cmap='RdYlBu',\n",
        "                           center=1.0,\n",
        "                           fmt='.2f',\n",
        "                           vmin=0.5,\n",
        "                           vmax=2.0)\n",
        "\n",
        "                plt.title('QM9 Regression Fairness Metrics')\n",
        "                plt.ylabel('Molecular Features')\n",
        "                plt.xlabel('Fairness Metrics')\n",
        "\n",
        "                cbar = plt.gca().collections[0].colorbar\n",
        "                cbar.set_label('Metric Ratio (1.0 = Fair)', rotation=270, labelpad=15)\n",
        "\n",
        "                plt.tight_layout()\n",
        "                plt.show()\n",
        "\n",
        "                # Print feature summaries\n",
        "                print(\"\\nFeatures with significant bias (outside 0.8-1.25 range):\")\n",
        "                bias_mask = (metrics_df > 1.25) | (metrics_df < 0.8)\n",
        "                biased_features = metrics_df[bias_mask.any(axis=1) & (metrics_df != 0).any(axis=1)]\n",
        "                print(biased_features.round(3))\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"Error in visualization: {str(e)}\")\n",
        "            traceback.print_exc()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "e2d02408",
      "metadata": {
        "id": "e2d02408"
      },
      "outputs": [],
      "source": [
        "def analyze_regression_fairness(model, test_loader, device):\n",
        "    \"\"\"Main function to perform regression fairness analysis\"\"\"\n",
        "    try:\n",
        "        print(\"\\n=== Starting Regression Fairness Analysis ===\")\n",
        "\n",
        "        # Create analyzer and get fairness results\n",
        "        analyzer = MolecularRegressionFairnessAnalyzer(model, test_loader, device)\n",
        "        fairness_results = analyzer.analyze()\n",
        "\n",
        "        if fairness_results:\n",
        "            print(\"\\nRegression Fairness Analysis Results:\")\n",
        "\n",
        "#             # Print detailed metrics\n",
        "#             for criterion, f_object in fairness_results.items():\n",
        "#                 print(f\"\\n{criterion.upper()} Fairness Metrics:\")\n",
        "#                 print(f_object.fairness_check())\n",
        "\n",
        "            # Create heatmap visualization\n",
        "            print(\"\\nGenerating fairness metrics visualization...\")\n",
        "            analyzer.visualize_fairness_metrics(fairness_results)\n",
        "\n",
        "#             # Print summary\n",
        "#             print(\"\\nFairness Analysis Summary:\")\n",
        "#             for criterion in fairness_results:\n",
        "#                 print(f\"\\n{criterion.upper()} results available\")\n",
        "\n",
        "        else:\n",
        "            print(\"\\nNo fairness metrics were calculated successfully.\")\n",
        "\n",
        "        return fairness_results\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"\\nError during fairness analysis: {str(e)}\")\n",
        "        traceback.print_exc()\n",
        "        return None\n",
        "\n",
        "\n",
        "def main():\n",
        "    # Set device\n",
        "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "\n",
        "    # Load QM7 dataset\n",
        "    print(\"Loading QM7 dataset...\")\n",
        "    tasks, datasets, transformers = load_qm7(\n",
        "        featurizer='Raw',  # We'll use our own featurizer\n",
        "        splitter='random',\n",
        "        transformers=['balancing']\n",
        "    )\n",
        "\n",
        "    train_dataset, val_dataset, test_dataset = datasets\n",
        "\n",
        "    print(\"Creating feature extractor...\")\n",
        "    feature_extractor = MolecularFeatureExtractor()\n",
        "\n",
        "    print(\"Processing training set...\")\n",
        "    train_data = QM7Dataset(train_dataset.ids, train_dataset.y, feature_extractor)\n",
        "    print(\"Processing validation set...\")\n",
        "    val_data = QM7Dataset(val_dataset.ids, val_dataset.y, feature_extractor)\n",
        "    print(\"Processing test set...\")\n",
        "    test_data = QM7Dataset(test_dataset.ids, test_dataset.y, feature_extractor)\n",
        "\n",
        "    # Create data loaders\n",
        "    print(\"Creating data loaders...\")\n",
        "    batch_size = 32\n",
        "    train_loader = DataLoader(train_data, batch_size=batch_size, shuffle=True)\n",
        "    val_loader = DataLoader(val_data, batch_size=batch_size)\n",
        "    test_loader = DataLoader(test_data, batch_size=batch_size)\n",
        "\n",
        "\n",
        "    # Load pretrained encoder\n",
        "    print(\"Loading pretrained encoder...\")\n",
        "    encoder_path = '../checkpoints/encoders/final_encoder_20250216_111050.pt'\n",
        "    encoder, model_info = load_pretrained_encoder(encoder_path, device)\n",
        "\n",
        "    # Get dimensions from the processed data\n",
        "    sample_data = next(iter(train_loader))\n",
        "    node_dim = sample_data.x_cat.size(1) + sample_data.x_phys.size(1)\n",
        "    edge_dim = sample_data.edge_attr.size(1)\n",
        "    print(f\"Initializing model with node_dim={node_dim}, edge_dim={edge_dim}\")\n",
        "\n",
        "    # Initialize and train model\n",
        "    print(\"Initializing model...\")\n",
        "    model = HybridGNNRegressor(\n",
        "        encoder=encoder,\n",
        "        node_dim=node_dim,\n",
        "        edge_dim=edge_dim,\n",
        "        hidden_dim=128,\n",
        "        num_layers=3,\n",
        "        num_tasks=1,\n",
        "        dropout=0.3\n",
        "    ).to(device)\n",
        "\n",
        "    # Train model\n",
        "    best_model = train_hybrid_model_regression(\n",
        "        model=model,\n",
        "        train_loader=train_loader,\n",
        "        val_loader=val_loader,\n",
        "        device=device,\n",
        "        num_epochs=100,\n",
        "        lr=1e-3,\n",
        "        weight_decay=1e-4\n",
        "    )\n",
        "\n",
        "    if best_model is not None:\n",
        "        # Load best model for testing\n",
        "        model.load_state_dict(best_model['state_dict'])\n",
        "\n",
        "        # Evaluate on test set\n",
        "        model.eval()\n",
        "        y_true_test = []\n",
        "        y_pred_test = []\n",
        "\n",
        "        with torch.no_grad():\n",
        "            for batch in test_loader:\n",
        "                batch = batch.to(device)\n",
        "                outputs = model(batch)\n",
        "                targets = batch.y.view(-1, 1)\n",
        "\n",
        "                y_true_test.extend(targets.cpu().numpy())\n",
        "                y_pred_test.extend(outputs.cpu().numpy())  # Removed sigmoid for regression\n",
        "\n",
        "        # Calculate regression metrics\n",
        "        test_rmse = np.sqrt(mean_squared_error(y_true_test, y_pred_test))\n",
        "        test_r2 = r2_score(y_true_test, y_pred_test)\n",
        "        test_mae = mean_absolute_error(y_true_test, y_pred_test)\n",
        "\n",
        "        print(\"\\nTest Results:\")\n",
        "        print(f\"Test RMSE: {test_rmse:.4f}\")\n",
        "        print(f\"Test R²: {test_r2:.4f}\")\n",
        "        print(f\"Test MAE: {test_mae:.4f}\")\n",
        "\n",
        "        fairness_results = analyze_regression_fairness(model, test_loader, device)\n",
        "\n",
        "#         # Results will contain fairness objects for each criterion\n",
        "#         if fairness_results:\n",
        "#             print(\"\\nFairness Analysis Summary:\")\n",
        "#             for criterion in fairness_results:\n",
        "#                 print(f\"\\n{criterion.upper()} results available\")\n",
        "\n",
        "    return fairness_results\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    fairness_results = main()"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python [torchfix]",
      "language": "python",
      "name": "torchfix"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.18"
    },
    "colab": {
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}