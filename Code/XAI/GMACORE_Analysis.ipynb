{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "5379f427",
      "metadata": {
        "id": "5379f427"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "from rdkit import Chem\n",
        "from rdkit.Chem import AllChem, Descriptors\n",
        "from typing import Tuple\n",
        "import numpy as np\n",
        "from torch_geometric.data import Data\n",
        "import random\n",
        "from torch_geometric.loader import DataLoader\n",
        "from torch_geometric.data import Data\n",
        "from rdkit import RDLogger\n",
        "from rdkit.Chem import RemoveHs\n",
        "from datetime import datetime\n",
        "\n",
        "# Suppress RDKit warnings\n",
        "RDLogger.DisableLog('rdApp.warning')\n",
        "timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "8de17bc6",
      "metadata": {
        "id": "8de17bc6"
      },
      "outputs": [],
      "source": [
        "class MolecularFeatureExtractor:\n",
        "    def __init__(self):\n",
        "        self.atom_list = list(range(1, 119))\n",
        "        self.chirality_list = [\n",
        "            Chem.rdchem.ChiralType.CHI_UNSPECIFIED,\n",
        "            Chem.rdchem.ChiralType.CHI_TETRAHEDRAL_CW,\n",
        "            Chem.rdchem.ChiralType.CHI_TETRAHEDRAL_CCW,\n",
        "            Chem.rdchem.ChiralType.CHI_OTHER\n",
        "        ]\n",
        "        self.bond_list = [\n",
        "            Chem.rdchem.BondType.SINGLE,\n",
        "            Chem.rdchem.BondType.DOUBLE,\n",
        "            Chem.rdchem.BondType.TRIPLE,\n",
        "            Chem.rdchem.BondType.AROMATIC\n",
        "        ]\n",
        "        self.bonddir_list = [\n",
        "            Chem.rdchem.BondDir.NONE,\n",
        "            Chem.rdchem.BondDir.ENDUPRIGHT,\n",
        "            Chem.rdchem.BondDir.ENDDOWNRIGHT\n",
        "        ]\n",
        "\n",
        "    def calc_atom_features(self, atom: Chem.Atom) -> Tuple[list, list]:\n",
        "        \"\"\"Calculate atom features with better error handling\"\"\"\n",
        "        try:\n",
        "            # Basic features\n",
        "            atom_feat = [\n",
        "                self.atom_list.index(atom.GetAtomicNum()),\n",
        "                self.chirality_list.index(atom.GetChiralTag())\n",
        "            ]\n",
        "\n",
        "            # Physical features with error handling\n",
        "            phys_feat = []\n",
        "\n",
        "            # Molecular weight contribution\n",
        "            try:\n",
        "                contrib_mw = Descriptors.ExactMolWt(Chem.MolFromSmiles(f'[{atom.GetSymbol()}]'))\n",
        "                phys_feat.append(contrib_mw)\n",
        "            except:\n",
        "                phys_feat.append(0.0)\n",
        "\n",
        "            # LogP contribution\n",
        "            try:\n",
        "                contrib_logp = Descriptors.MolLogP(Chem.MolFromSmiles(f'[{atom.GetSymbol()}]'))\n",
        "                phys_feat.append(contrib_logp)\n",
        "            except:\n",
        "                phys_feat.append(0.0)\n",
        "\n",
        "            # Add other physical properties\n",
        "            phys_feat.extend([\n",
        "                atom.GetFormalCharge(),\n",
        "                int(atom.GetHybridization()),\n",
        "                int(atom.GetIsAromatic()),\n",
        "                atom.GetTotalNumHs(),\n",
        "                atom.GetTotalValence(),\n",
        "                atom.GetDegree()\n",
        "            ])\n",
        "\n",
        "            return atom_feat, phys_feat\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"Error calculating atom features: {e}\")\n",
        "            return [0, 0], [0.0] * 9\n",
        "\n",
        "    def get_atom_features(self, mol: Chem.Mol) -> Tuple[torch.Tensor, torch.Tensor]:\n",
        "        \"\"\"Extract atom features for the whole molecule\"\"\"\n",
        "        atom_feats = []\n",
        "        phys_feats = []\n",
        "\n",
        "        if mol is None:\n",
        "            return torch.tensor([[0, 0]], dtype=torch.long), torch.tensor([[0.0] * 9], dtype=torch.float)\n",
        "\n",
        "        for atom in mol.GetAtoms():\n",
        "            atom_feat, phys_feat = self.calc_atom_features(atom)\n",
        "            atom_feats.append(atom_feat)\n",
        "            phys_feats.append(phys_feat)\n",
        "\n",
        "        x = torch.tensor(atom_feats, dtype=torch.long)\n",
        "        phys = torch.tensor(phys_feats, dtype=torch.float)\n",
        "\n",
        "        return x, phys\n",
        "\n",
        "    def remove_unbonded_hydrogens(mol):\n",
        "        params = Chem.RemoveHsParameters()\n",
        "        params.removeDegreeZero = True\n",
        "        mol = Chem.RemoveHs(mol, params)\n",
        "        return mol\n",
        "\n",
        "\n",
        "    def get_bond_features(self, mol: Chem.Mol) -> Tuple[torch.Tensor, torch.Tensor]:\n",
        "        \"\"\"Extract bond features with better error handling\"\"\"\n",
        "        if mol is None:\n",
        "            return torch.tensor([[0], [0]], dtype=torch.long), torch.tensor([[0.0] * 5], dtype=torch.float)\n",
        "\n",
        "        row, col, edge_feat = [], [], []\n",
        "\n",
        "        for bond in mol.GetBonds():\n",
        "            try:\n",
        "                start, end = bond.GetBeginAtomIdx(), bond.GetEndAtomIdx()\n",
        "\n",
        "                # Add edges in both directions\n",
        "                row += [start, end]\n",
        "                col += [end, start]\n",
        "\n",
        "                # Bond features\n",
        "                bond_type = self.bond_list.index(bond.GetBondType())\n",
        "                bond_dir = self.bonddir_list.index(bond.GetBondDir())\n",
        "\n",
        "                # Calculate additional properties\n",
        "                feat = [\n",
        "                    bond_type,\n",
        "                    bond_dir,\n",
        "                    int(bond.GetIsConjugated()),\n",
        "                    int(self._is_rotatable(bond)),\n",
        "                    self._get_bond_length(mol, start, end)\n",
        "                ]\n",
        "\n",
        "                edge_feat.extend([feat, feat])\n",
        "\n",
        "            except Exception as e:\n",
        "                print(f\"Error processing bond: {e}\")\n",
        "                continue\n",
        "\n",
        "        if not row:  # If no valid bonds were processed\n",
        "            return torch.tensor([[0], [0]], dtype=torch.long), torch.tensor([[0.0] * 5], dtype=torch.float)\n",
        "\n",
        "        edge_index = torch.tensor([row, col], dtype=torch.long)\n",
        "        edge_attr = torch.tensor(edge_feat, dtype=torch.float)\n",
        "\n",
        "        return edge_index, edge_attr\n",
        "\n",
        "    def _is_rotatable(self, bond: Chem.Bond) -> bool:\n",
        "        \"\"\"Check if bond is rotatable\"\"\"\n",
        "        return (bond.GetBondType() == Chem.rdchem.BondType.SINGLE and\n",
        "                not bond.IsInRing() and\n",
        "                len(bond.GetBeginAtom().GetNeighbors()) > 1 and\n",
        "                len(bond.GetEndAtom().GetNeighbors()) > 1)\n",
        "\n",
        "    def _get_bond_length(self, mol: Chem.Mol, start: int, end: int) -> float:\n",
        "        \"\"\"Get bond length with error handling\"\"\"\n",
        "        try:\n",
        "            conf = mol.GetConformer()\n",
        "            if conf.Is3D():\n",
        "                return Chem.rdMolTransforms.GetBondLength(conf, start, end)\n",
        "        except:\n",
        "            pass\n",
        "        return 0.0\n",
        "\n",
        "    def process_molecule(self, smiles: str) -> Data:\n",
        "        \"\"\"Process SMILES string to graph data\"\"\"\n",
        "        try:\n",
        "            mol = Chem.MolFromSmiles(smiles)\n",
        "            if mol is None:\n",
        "                print(f\"Invalid SMILES: {smiles}\")\n",
        "                return None  # Skip invalid molecules\n",
        "            mol = RemoveHs(mol)\n",
        "\n",
        "            # Add explicit hydrogens\n",
        "            mol = Chem.AddHs(mol, addCoords=True)\n",
        "\n",
        "            # Sanitize molecule\n",
        "            Chem.SanitizeMol(mol)\n",
        "\n",
        "            # Check if the molecule has atoms\n",
        "            if mol.GetNumAtoms() == 0:\n",
        "                print(\"Molecule has no atoms, skipping.\")\n",
        "                return None\n",
        "\n",
        "            # Generate 3D coordinates\n",
        "            if not mol.GetNumConformers():\n",
        "                status = AllChem.EmbedMolecule(mol, AllChem.ETKDG())\n",
        "                if status != 0:\n",
        "                    print(\"Failed to generate 3D conformer\")\n",
        "                    return None  # Skip failed molecules\n",
        "\n",
        "                # Try MMFF or UFF optimization\n",
        "                try:\n",
        "                    AllChem.MMFFOptimizeMolecule(mol)\n",
        "                except:\n",
        "                    AllChem.UFFOptimizeMolecule(mol)\n",
        "\n",
        "            # Extract features\n",
        "            x_cat, x_phys = self.get_atom_features(mol)\n",
        "            edge_index, edge_attr = self.get_bond_features(mol)\n",
        "\n",
        "            # Save all necessary information\n",
        "            return Data(\n",
        "                x_cat=x_cat,\n",
        "                x_phys=x_phys,\n",
        "                edge_index=edge_index,\n",
        "                edge_attr=edge_attr,\n",
        "                num_nodes=x_cat.size(0),\n",
        "                smiles=smiles,  # Store original SMILES\n",
        "                mol=mol  # Store RDKit mol object for later use\n",
        "            )\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"Error processing molecule {smiles}: {e}\")\n",
        "            return None\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "35a311ed",
      "metadata": {
        "id": "35a311ed"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from torch_geometric.nn import GCNConv, global_mean_pool, MessagePassing\n",
        "from typing import Tuple, List, Optional\n",
        "import copy\n",
        "from dataclasses import dataclass\n",
        "\n",
        "class MemoryQueue:\n",
        "    \"\"\"Memory queue with temporal decay for contrastive learning\"\"\"\n",
        "    def __init__(self, size: int, dim: int, decay: float = 0.99999):\n",
        "        self.size = size\n",
        "        self.dim = dim\n",
        "        self.decay = decay\n",
        "        self.ptr = 0\n",
        "        self.full = False\n",
        "\n",
        "        # Initialize queue\n",
        "        self.queue = nn.Parameter(F.normalize(torch.randn(size, dim), dim=1), requires_grad=False)\n",
        "        self.queue_age = nn.Parameter(torch.zeros(size), requires_grad=False)\n",
        "\n",
        "#         self.register_buffer(\"queue\", torch.randn(size, dim))\n",
        "#         self.register_buffer(\"queue_age\", torch.zeros(size))  # Track age of each entry\n",
        "        self.queue = F.normalize(self.queue, dim=1)\n",
        "\n",
        "    def update_queue(self, keys: torch.Tensor):\n",
        "        \"\"\"Update queue with new keys\"\"\"\n",
        "        batch_size = keys.shape[0]\n",
        "\n",
        "        # Increment age of all entries\n",
        "        self.queue_age += 1\n",
        "\n",
        "        # Add new keys\n",
        "        if self.ptr + batch_size <= self.size:\n",
        "            self.queue[self.ptr:self.ptr + batch_size] = keys\n",
        "            self.queue_age[self.ptr:self.ptr + batch_size] = 0\n",
        "        else:\n",
        "            # Handle overflow\n",
        "            rem = self.size - self.ptr\n",
        "            self.queue[self.ptr:] = keys[:rem]\n",
        "            self.queue[:batch_size-rem] = keys[rem:]\n",
        "            self.queue_age[self.ptr:] = 0\n",
        "            self.queue_age[:batch_size-rem] = 0\n",
        "            self.full = True\n",
        "\n",
        "        self.ptr = (self.ptr + batch_size) % self.size\n",
        "\n",
        "    def get_decay_weights(self) -> torch.Tensor:\n",
        "        \"\"\"Get temporal decay weights for queue entries\"\"\"\n",
        "        return self.decay ** self.queue_age\n",
        "\n",
        "    def compute_contrastive_loss(self, query: torch.Tensor, positive_key: torch.Tensor,\n",
        "                                temperature: float = 0.07) -> torch.Tensor:\n",
        "        \"\"\"Compute contrastive loss with temporal decay\"\"\"\n",
        "        # Normalize embeddings\n",
        "        query = F.normalize(query, dim=1)\n",
        "        positive_key = F.normalize(positive_key, dim=1)\n",
        "        queue = F.normalize(self.queue, dim=1)\n",
        "\n",
        "        # Compute logits\n",
        "        l_pos = torch.einsum('nc,nc->n', [query, positive_key]).unsqueeze(-1)\n",
        "        l_neg = torch.einsum('nc,ck->nk', [query, queue.T])\n",
        "\n",
        "        # Apply temporal decay to negative samples\n",
        "        decay_weights = self.get_decay_weights()\n",
        "        l_neg = l_neg * decay_weights.unsqueeze(0)\n",
        "\n",
        "        # Temperature scaling\n",
        "        logits = torch.cat([l_pos, l_neg], dim=1) / temperature\n",
        "        labels = torch.zeros(logits.shape[0], dtype=torch.long, device=query.device)\n",
        "\n",
        "        return F.cross_entropy(logits, labels)\n",
        "\n",
        "class GraphGenerator(nn.Module):\n",
        "    \"\"\"Generator network with proper feature handling\"\"\"\n",
        "    def __init__(self, node_dim: int, edge_dim: int, hidden_dim: int = 128):\n",
        "        super().__init__()\n",
        "\n",
        "        # Node feature processing\n",
        "        self.node_encoder = nn.Sequential(\n",
        "            nn.Linear(node_dim, hidden_dim),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(hidden_dim, hidden_dim)\n",
        "        )\n",
        "\n",
        "        # Edge feature processing\n",
        "        self.edge_encoder = nn.Sequential(\n",
        "            nn.Linear(edge_dim, hidden_dim),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(hidden_dim, hidden_dim)\n",
        "        )\n",
        "\n",
        "        # Graph convolution layers\n",
        "        self.conv1 = GCNConv(hidden_dim, hidden_dim)\n",
        "        self.conv2 = GCNConv(hidden_dim, hidden_dim)\n",
        "        self.conv3 = GCNConv(hidden_dim, hidden_dim)\n",
        "\n",
        "        # Importance prediction layers\n",
        "        self.node_importance = nn.Sequential(\n",
        "            nn.Linear(hidden_dim, hidden_dim // 2),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(hidden_dim // 2, 1),\n",
        "            nn.Sigmoid()\n",
        "        )\n",
        "\n",
        "        self.edge_importance = nn.Sequential(\n",
        "            nn.Linear(hidden_dim * 2, hidden_dim),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(hidden_dim, 1),\n",
        "            nn.Sigmoid()\n",
        "        )\n",
        "\n",
        "    def normalize_features(self, x_cat, x_phys):\n",
        "        \"\"\"Normalize categorical and physical features separately\"\"\"\n",
        "        # Convert categorical features to one-hot\n",
        "        x_cat = x_cat.float()\n",
        "\n",
        "        # Normalize physical features\n",
        "        x_phys = x_phys.float()\n",
        "        if x_phys.size(0) > 1:  # Only normalize if we have more than one sample\n",
        "            x_phys = (x_phys - x_phys.mean(0)) / (x_phys.std(0) + 1e-5)\n",
        "\n",
        "        return x_cat, x_phys\n",
        "\n",
        "    def forward(self, data) -> Tuple[torch.Tensor, torch.Tensor]:\n",
        "        # Normalize features\n",
        "        x_cat, x_phys = self.normalize_features(data.x_cat, data.x_phys)\n",
        "\n",
        "        # Concatenate features\n",
        "        x = torch.cat([x_cat, x_phys], dim=-1)\n",
        "\n",
        "        edge_index = data.edge_index\n",
        "        edge_attr = data.edge_attr.float()  # Ensure float type\n",
        "\n",
        "        # Initial feature encoding\n",
        "        x = self.node_encoder(x)\n",
        "        edge_attr = self.edge_encoder(edge_attr)\n",
        "\n",
        "        # Graph convolutions\n",
        "        x = F.relu(self.conv1(x, edge_index))  # Removed edge_attr from GCNConv\n",
        "        x = F.relu(self.conv2(x, edge_index))\n",
        "        x = self.conv3(x, edge_index)\n",
        "\n",
        "        # Predict importance scores\n",
        "        node_scores = self.node_importance(x)\n",
        "\n",
        "        # Edge scores using both connected nodes\n",
        "        edge_features = torch.cat([\n",
        "            x[edge_index[0]],\n",
        "            x[edge_index[1]]\n",
        "        ], dim=-1)\n",
        "        edge_scores = self.edge_importance(edge_features)\n",
        "\n",
        "        return node_scores, edge_scores\n",
        "\n",
        "def get_model_config(dataset):\n",
        "    \"\"\"Get model configuration based on dataset features\"\"\"\n",
        "    sample_data = dataset[0]\n",
        "\n",
        "    # Calculate input dimensions\n",
        "    node_dim = sample_data.x_cat.shape[1] + sample_data.x_phys.shape[1]\n",
        "    edge_dim = sample_data.edge_attr.shape[1]\n",
        "\n",
        "    config = GanClConfig(\n",
        "        node_dim=node_dim,\n",
        "        edge_dim=edge_dim,\n",
        "        hidden_dim=128,\n",
        "        output_dim=128,\n",
        "        queue_size=65536,\n",
        "        momentum=0.999,\n",
        "        temperature=0.07,\n",
        "        decay=0.99999,\n",
        "        dropout_ratio=0.25\n",
        "    )\n",
        "\n",
        "    return config\n",
        "\n",
        "class GraphDiscriminator(nn.Module):\n",
        "    \"\"\"Discriminator/Encoder network\"\"\"\n",
        "    def __init__(self, node_dim: int, edge_dim: int, hidden_dim: int = 128, output_dim: int = 128):\n",
        "        super().__init__()\n",
        "\n",
        "        # Feature encoding\n",
        "        self.node_encoder = nn.Sequential(\n",
        "            nn.Linear(node_dim, hidden_dim),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(hidden_dim, hidden_dim)\n",
        "        )\n",
        "\n",
        "        self.edge_encoder = nn.Sequential(\n",
        "            nn.Linear(edge_dim, hidden_dim),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(hidden_dim, hidden_dim)\n",
        "        )\n",
        "\n",
        "        # Graph convolution layers\n",
        "        self.conv1 = GCNConv(hidden_dim, hidden_dim)\n",
        "        self.conv2 = GCNConv(hidden_dim, hidden_dim)\n",
        "        self.conv3 = GCNConv(hidden_dim, output_dim)\n",
        "\n",
        "        # Projection head for contrastive learning\n",
        "        self.projection = nn.Sequential(\n",
        "            nn.Linear(output_dim, hidden_dim),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(hidden_dim, output_dim)\n",
        "        )\n",
        "\n",
        "    def normalize_features(self, x_cat, x_phys):\n",
        "        \"\"\"Normalize categorical and physical features separately\"\"\"\n",
        "        # Convert categorical features to one-hot\n",
        "        x_cat = x_cat.float()\n",
        "\n",
        "        # Normalize physical features\n",
        "        x_phys = x_phys.float()\n",
        "        if x_phys.size(0) > 1:  # Only normalize if we have more than one sample\n",
        "            x_phys = (x_phys - x_phys.mean(0)) / (x_phys.std(0) + 1e-5)\n",
        "\n",
        "        return x_cat, x_phys\n",
        "\n",
        "    def forward(self, data):\n",
        "        # Normalize features\n",
        "        x_cat, x_phys = self.normalize_features(data.x_cat, data.x_phys)\n",
        "\n",
        "        # Concatenate features\n",
        "        x = torch.cat([x_cat, x_phys], dim=-1)\n",
        "\n",
        "        edge_index = data.edge_index\n",
        "        edge_attr = data.edge_attr.float()  # Ensure float type\n",
        "        batch = data.batch\n",
        "\n",
        "        # Initial feature encoding\n",
        "        x = self.node_encoder(x)\n",
        "        edge_attr = self.edge_encoder(edge_attr)\n",
        "\n",
        "        # Graph convolutions\n",
        "        x = F.relu(self.conv1(x, edge_index))  # Removed edge_attr from GCNConv\n",
        "        x = F.relu(self.conv2(x, edge_index))\n",
        "        x = self.conv3(x, edge_index)\n",
        "\n",
        "        # Global pooling\n",
        "        x = global_mean_pool(x, batch)\n",
        "\n",
        "        # Projection\n",
        "        x = self.projection(x)\n",
        "\n",
        "        return x\n",
        "\n",
        "@dataclass\n",
        "class GanClConfig:\n",
        "    \"\"\"Configuration for GAN-CL training\"\"\"\n",
        "    node_dim: int\n",
        "    edge_dim: int\n",
        "    hidden_dim: int = 128\n",
        "    output_dim: int = 128\n",
        "    queue_size: int = 65536\n",
        "    momentum: float = 0.999\n",
        "    temperature: float = 0.07\n",
        "    decay: float = 0.99999\n",
        "    dropout_ratio: float = 0.25\n",
        "\n",
        "class MolecularGANCL(nn.Module):\n",
        "    \"\"\"Combined GAN and Contrastive Learning framework\"\"\"\n",
        "    def __init__(self, config: GanClConfig):\n",
        "        super().__init__()\n",
        "        self.config = config\n",
        "\n",
        "        # Add weight initialization\n",
        "        def init_weights(m):\n",
        "            if isinstance(m, nn.Linear):\n",
        "                torch.nn.init.xavier_uniform_(m.weight)\n",
        "                m.bias.data.fill_(0.01)\n",
        "\n",
        "        # Initialize networks\n",
        "        self.generator = GraphGenerator(\n",
        "            config.node_dim,\n",
        "            config.edge_dim,\n",
        "            config.hidden_dim * 2\n",
        "        )\n",
        "\n",
        "        self.encoder = GraphDiscriminator(\n",
        "            config.node_dim,\n",
        "            config.edge_dim,\n",
        "            config.hidden_dim,\n",
        "            config.output_dim\n",
        "        )\n",
        "        self.encoder.apply(init_weights)\n",
        "\n",
        "        # Modified loss weights\n",
        "        self.contrastive_weight = 1.0\n",
        "        self.adversarial_weight = 0.1  # Increased from 0.05\n",
        "        self.similarity_weight = 0.01  # Decreased from 0.1\n",
        "\n",
        "        # Temperature annealing\n",
        "        self.initial_temperature = 0.1\n",
        "        self.min_temperature = 0.05\n",
        "\n",
        "        # Create momentum encoder\n",
        "        self.momentum_encoder = copy.deepcopy(self.encoder)\n",
        "        for param in self.momentum_encoder.parameters():\n",
        "            param.requires_grad = False\n",
        "\n",
        "        # Initialize memory queue\n",
        "        self.memory_queue = MemoryQueue(\n",
        "            config.queue_size,\n",
        "            config.output_dim,\n",
        "            config.decay\n",
        "        )\n",
        "\n",
        "    @torch.no_grad()\n",
        "    def _momentum_update(self):\n",
        "        \"\"\"Update momentum encoder\"\"\"\n",
        "        for param_q, param_k in zip(self.encoder.parameters(),\n",
        "                                  self.momentum_encoder.parameters()):\n",
        "            param_k.data = self.config.momentum * param_k.data + \\\n",
        "                          (1 - self.config.momentum) * param_q.data\n",
        "\n",
        "    def drop_graph_elements(self, data, node_scores: torch.Tensor,\n",
        "                          edge_scores: torch.Tensor) -> Data:\n",
        "        \"\"\"Apply dropout to graph based on importance scores\"\"\"\n",
        "        # Select elements to keep based on scores and dropout ratio\n",
        "#         node_mask = (node_scores < self.config.dropout_ratio).float()\n",
        "#         edge_mask = (edge_scores < self.config.dropout_ratio).float()\n",
        "\n",
        "        node_mask = (torch.rand_like(node_scores) > self.config.dropout_ratio).float()\n",
        "        edge_mask = (torch.rand_like(edge_scores) > self.config.dropout_ratio).float()\n",
        "\n",
        "        # Apply masks\n",
        "        x_cat_new = data.x_cat * node_mask\n",
        "        x_phys_new = data.x_phys * node_mask\n",
        "        edge_attr_new = data.edge_attr * edge_mask\n",
        "\n",
        "        # Create new graph data object\n",
        "        return Data(\n",
        "            x_cat=x_cat_new,\n",
        "            x_phys=x_phys_new,\n",
        "            edge_index=data.edge_index,\n",
        "            edge_attr=edge_attr_new,\n",
        "            batch=data.batch\n",
        "        )\n",
        "\n",
        "    def get_temperature(self, epoch, total_epochs):\n",
        "        \"\"\"Anneal temperature during training\"\"\"\n",
        "        progress = epoch / total_epochs\n",
        "        return max(self.initial_temperature * (1 - progress), self.min_temperature)\n",
        "\n",
        "    def forward(self, data, epoch=0, total_epochs=50):\n",
        "        # Get current temperature\n",
        "        temperature = self.get_temperature(epoch, total_epochs)\n",
        "\n",
        "        # Get importance scores from generator\n",
        "        node_scores, edge_scores = self.generator(data)\n",
        "\n",
        "        # Create perturbed graph\n",
        "        perturbed_data = self.drop_graph_elements(data, node_scores, edge_scores)\n",
        "\n",
        "        # Get embeddings\n",
        "        query_emb = self.encoder(perturbed_data)\n",
        "        with torch.no_grad():\n",
        "            key_emb = self.momentum_encoder(data)\n",
        "            original_emb = self.encoder(data).detach()\n",
        "\n",
        "        # Compute losses with modified weights\n",
        "        contrastive_loss = self.memory_queue.compute_contrastive_loss(\n",
        "            query_emb, key_emb, temperature\n",
        "        ) * self.contrastive_weight\n",
        "\n",
        "        adversarial_loss = -F.mse_loss(query_emb, original_emb) * self.adversarial_weight\n",
        "        similarity_loss = F.mse_loss(query_emb, original_emb) * self.similarity_weight\n",
        "\n",
        "        return contrastive_loss, adversarial_loss, similarity_loss\n",
        "\n",
        "    def get_embeddings(self, data) -> torch.Tensor:\n",
        "        \"\"\"Get embeddings for downstream tasks\"\"\"\n",
        "        with torch.no_grad():\n",
        "            return self.encoder(data)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "8e90813e",
      "metadata": {
        "id": "8e90813e"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import numpy as np\n",
        "from torch_geometric.data import DataLoader\n",
        "import os\n",
        "import json\n",
        "from tqdm import tqdm\n",
        "import pickle\n",
        "from datetime import datetime\n",
        "\n",
        "def calculate_molecular_features(mol):\n",
        "    \"\"\"Calculate comprehensive molecular features\"\"\"\n",
        "    if mol is None:\n",
        "        return None\n",
        "\n",
        "    features = {\n",
        "        # Graph-level properties\n",
        "        'MW': Descriptors.ExactMolWt(mol),\n",
        "        'LogP': Descriptors.MolLogP(mol),\n",
        "        'TPSA': Descriptors.TPSA(mol),\n",
        "\n",
        "        # Ring information\n",
        "        'num_rings': Chem.rdMolDescriptors.CalcNumRings(mol),\n",
        "        'num_aromatic_rings': Chem.rdMolDescriptors.CalcNumAromaticRings(mol),\n",
        "        'num_aliphatic_rings': Chem.rdMolDescriptors.CalcNumAliphaticRings(mol),\n",
        "\n",
        "        # Atom counts\n",
        "        'num_atoms': mol.GetNumAtoms(),\n",
        "        'num_heavy_atoms': mol.GetNumHeavyAtoms(),\n",
        "        'num_rotatable_bonds': Chem.rdMolDescriptors.CalcNumRotatableBonds(mol),\n",
        "\n",
        "        # Element counts\n",
        "        'num_C': sum(1 for atom in mol.GetAtoms() if atom.GetAtomicNum() == 6),\n",
        "        'num_N': sum(1 for atom in mol.GetAtoms() if atom.GetAtomicNum() == 7),\n",
        "        'num_O': sum(1 for atom in mol.GetAtoms() if atom.GetAtomicNum() == 8),\n",
        "        'num_F': sum(1 for atom in mol.GetAtoms() if atom.GetAtomicNum() == 9),\n",
        "        'num_P': sum(1 for atom in mol.GetAtoms() if atom.GetAtomicNum() == 15),\n",
        "        'num_S': sum(1 for atom in mol.GetAtoms() if atom.GetAtomicNum() == 16),\n",
        "        'num_Cl': sum(1 for atom in mol.GetAtoms() if atom.GetAtomicNum() == 17),\n",
        "        'num_Br': sum(1 for atom in mol.GetAtoms() if atom.GetAtomicNum() == 35),\n",
        "        'num_I': sum(1 for atom in mol.GetAtoms() if atom.GetAtomicNum() == 53),\n",
        "\n",
        "        # Functional groups using SMARTS patterns\n",
        "        'has_alcohol': len(mol.GetSubstructMatches(Chem.MolFromSmarts('[OH]'))),\n",
        "        'has_amine': len(mol.GetSubstructMatches(Chem.MolFromSmarts('[NH2]'))),\n",
        "        'has_carboxyl': len(mol.GetSubstructMatches(Chem.MolFromSmarts('[CX3](=O)[OX2H1]'))),\n",
        "        'has_carbonyl': len(mol.GetSubstructMatches(Chem.MolFromSmarts('[CX3]=O'))),\n",
        "        'has_ether': len(mol.GetSubstructMatches(Chem.MolFromSmarts('[OR]'))),\n",
        "        'has_ester': len(mol.GetSubstructMatches(Chem.MolFromSmarts('[#6][CX3](=O)[OX2H0][#6]'))),\n",
        "        'has_amide': len(mol.GetSubstructMatches(Chem.MolFromSmarts('[NX3][CX3](=[OX1])'))),\n",
        "        'has_halogen': len(mol.GetSubstructMatches(Chem.MolFromSmarts('[F,Cl,Br,I]')))\n",
        "    }\n",
        "\n",
        "    return features\n",
        "\n",
        "def save_embeddings(embeddings, graphs, filepath, smiles_list=None):\n",
        "    \"\"\"Save embeddings with comprehensive molecular details\"\"\"\n",
        "    # SMARTS patterns for substructure matching\n",
        "    patterns = {\n",
        "        # Ring systems\n",
        "        'aromatic_ring': '[a;r6]1:a:a:a:a:a:1',\n",
        "        'heterocycle': '[!#6;!#1;R]',  # Any non-carbon, non-hydrogen ring atom\n",
        "        'spiro': '[D4R]',\n",
        "        'bridged': '[R2]([R2])([R2])[R2]',\n",
        "        'macrocycle': '[r{8,}]',\n",
        "\n",
        "        # Functional groups\n",
        "        'alcohol': '[OH]',\n",
        "        'phenol': '[OH]c1ccccc1',\n",
        "        'amine': '[NH2]',\n",
        "        'carboxyl': '[CX3](=O)[OX2H1]',\n",
        "        'carbonyl': '[CX3]=O',\n",
        "        'ether': '[OR]',\n",
        "        'ester': '[#6][CX3](=O)[OX2H0][#6]',\n",
        "        'amide': '[NX3][CX3](=[OX1])',\n",
        "        'sulfonamide': '[#16X4]([NX3])(=[OX1])(=[OX1])',\n",
        "        'halogen': '[F,Cl,Br,I]'\n",
        "    }\n",
        "\n",
        "    molecular_details = []\n",
        "    print(\"\\nProcessing molecular details...\")\n",
        "\n",
        "    for graph in tqdm(graphs):\n",
        "        try:\n",
        "            # Convert graph to SMILES if not provided\n",
        "            smiles = graph.smiles if hasattr(graph, 'smiles') else ''\n",
        "            mol = Chem.MolFromSmiles(smiles) if smiles else None\n",
        "\n",
        "            if mol is not None:\n",
        "                # Basic molecular properties\n",
        "                props = {\n",
        "                    'smiles': smiles,\n",
        "                    'basic_properties': {\n",
        "                        'MW': Descriptors.ExactMolWt(mol),\n",
        "                        'LogP': Descriptors.MolLogP(mol),\n",
        "                        'TPSA': Descriptors.TPSA(mol),\n",
        "                        'num_atoms': mol.GetNumAtoms(),\n",
        "                        'num_bonds': mol.GetNumBonds(),\n",
        "                        'num_rotatable_bonds': Descriptors.NumRotatableBonds(mol),\n",
        "                        'num_h_acceptors': Descriptors.NumHAcceptors(mol),\n",
        "                        'num_h_donors': Descriptors.NumHDonors(mol)\n",
        "                    },\n",
        "\n",
        "                    # Ring information\n",
        "                    'ring_info': {\n",
        "                        'total_rings': Chem.rdMolDescriptors.CalcNumRings(mol),\n",
        "                        'aromatic_rings': Chem.rdMolDescriptors.CalcNumAromaticRings(mol),\n",
        "                        'aliphatic_rings': Chem.rdMolDescriptors.CalcNumAliphaticRings(mol),\n",
        "                        'spiro_atoms': len(Chem.GetSpiroAtoms(mol)),\n",
        "                        'bridgeheads': len(Chem.FindMolChiralCenters(mol, includeUnassigned=True))\n",
        "                    },\n",
        "\n",
        "                    # Ring sizes\n",
        "                    'ring_sizes': {},\n",
        "\n",
        "                    # Element counts\n",
        "                    'element_counts': {},\n",
        "\n",
        "                    # Structural features\n",
        "                    'structural_features': {},\n",
        "\n",
        "                    # Functional groups\n",
        "                    'functional_groups': {}\n",
        "                }\n",
        "\n",
        "                # Get ring sizes\n",
        "                sssr = Chem.GetSymmSSSR(mol)\n",
        "                for ring in sssr:\n",
        "                    size = len(ring)\n",
        "                    props['ring_sizes'][str(size)] = props['ring_sizes'].get(str(size), 0) + 1\n",
        "\n",
        "                # Count elements\n",
        "                for atom in mol.GetAtoms():\n",
        "                    symbol = atom.GetSymbol()\n",
        "                    props['element_counts'][symbol] = props['element_counts'].get(symbol, 0) + 1\n",
        "\n",
        "                # Match patterns\n",
        "                for name, smarts in patterns.items():\n",
        "                    pattern = Chem.MolFromSmarts(smarts)\n",
        "                    if pattern:\n",
        "                        matches = len(mol.GetSubstructMatches(pattern))\n",
        "                        if name in ['aromatic_ring', 'heterocycle', 'spiro', 'bridged', 'macrocycle']:\n",
        "                            props['structural_features'][name] = matches\n",
        "                        else:\n",
        "                            props['functional_groups'][name] = matches\n",
        "\n",
        "                molecular_details.append(props)\n",
        "            else:\n",
        "                molecular_details.append(None)\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"Error processing molecule: {e}\")\n",
        "            molecular_details.append(None)\n",
        "\n",
        "    # Save all data\n",
        "    save_data = {\n",
        "        'embeddings': embeddings,\n",
        "        'molecular_details': molecular_details,\n",
        "        'original_graphs': graphs\n",
        "    }\n",
        "\n",
        "    with open(filepath, 'wb') as f:\n",
        "        pickle.dump(save_data, f)\n",
        "\n",
        "    print(f\"\\nSaved {len(embeddings)} embeddings with molecular details\")\n",
        "\n",
        "    # Print summary\n",
        "    valid_mols = sum(1 for x in molecular_details if x is not None)\n",
        "    print(f\"Successfully processed {valid_mols} molecules\")\n",
        "\n",
        "    return molecular_details  # Return for potential validation\n",
        "\n",
        "def save_encoder(encoder, save_path, info=None):\n",
        "    \"\"\"Save encoder model for downstream tasks\"\"\"\n",
        "    save_dict = {\n",
        "        'encoder_state_dict': encoder.state_dict(),\n",
        "        'model_info': info or {}\n",
        "    }\n",
        "    torch.save(save_dict, save_path)\n",
        "\n",
        "def load_encoder(model_path, device='cpu'):\n",
        "    \"\"\"Load saved encoder model\"\"\"\n",
        "    checkpoint = torch.load(model_path, map_location=device)\n",
        "    encoder = GraphDiscriminator(\n",
        "        node_dim=checkpoint['model_info'].get('node_dim'),\n",
        "        edge_dim=checkpoint['model_info'].get('edge_dim'),\n",
        "        hidden_dim=checkpoint['model_info'].get('hidden_dim', 128),\n",
        "        output_dim=checkpoint['model_info'].get('output_dim', 128)\n",
        "    )\n",
        "    encoder.load_state_dict(checkpoint['encoder_state_dict'])\n",
        "    return encoder\n",
        "\n",
        "def train_gan_cl(train_loader, config, device='cuda',\n",
        "                save_dir='./checkpoints',\n",
        "                embedding_dir='./embeddings'):\n",
        "    \"\"\"Main training function for GAN-CL with fixed gradient computation\"\"\"\n",
        "\n",
        "    # Create directories\n",
        "    os.makedirs(save_dir, exist_ok=True)\n",
        "    os.makedirs(embedding_dir, exist_ok=True)\n",
        "    encoder_dir = os.path.join(save_dir, 'encoders')\n",
        "    os.makedirs(encoder_dir, exist_ok=True)\n",
        "\n",
        "    # Initialize model\n",
        "    model = MolecularGANCL(config).to(device)\n",
        "\n",
        "    # Initialize optimizers\n",
        "    optimizer_encoder = torch.optim.Adam(model.encoder.parameters(), lr=3e-4)\n",
        "    optimizer_generator = torch.optim.Adam(model.generator.parameters(), lr=1e-4)\n",
        "\n",
        "    # Save initial model info\n",
        "    model_info = {\n",
        "        'node_dim': config.node_dim,\n",
        "        'edge_dim': config.edge_dim,\n",
        "        'hidden_dim': config.hidden_dim,\n",
        "        'output_dim': config.output_dim,\n",
        "        'training_config': config.__dict__\n",
        "    }\n",
        "\n",
        "    # Training phases as before...\n",
        "    best_loss = float('inf')\n",
        "\n",
        "    # Training metrics\n",
        "    metrics = {\n",
        "        'contrastive_losses': [],\n",
        "        'adversarial_losses': [],\n",
        "        'similarity_losses': [],\n",
        "        'total_losses': []\n",
        "    }\n",
        "\n",
        "    # Training phases\n",
        "    print(\"Phase 1: Pretraining Contrastive Learning...\")\n",
        "    pretrain_epochs = 10\n",
        "    for epoch in range(pretrain_epochs):\n",
        "        contrastive_epoch_loss = 0\n",
        "\n",
        "        for batch in tqdm(train_loader, desc=f'Pretrain Epoch {epoch+1}/{pretrain_epochs}'):\n",
        "            batch = batch.to(device)\n",
        "\n",
        "            # Forward pass (without generator)\n",
        "            query_emb = model.encoder(batch)\n",
        "            with torch.no_grad():\n",
        "                key_emb = model.momentum_encoder(batch)\n",
        "\n",
        "            # Compute contrastive loss\n",
        "            contrastive_loss = model.memory_queue.compute_contrastive_loss(\n",
        "                query_emb, key_emb, model.config.temperature\n",
        "            )\n",
        "\n",
        "            # Update encoder\n",
        "            optimizer_encoder.zero_grad()\n",
        "            contrastive_loss.backward()\n",
        "            optimizer_encoder.step()\n",
        "\n",
        "            # Update momentum encoder\n",
        "            model._momentum_update()\n",
        "\n",
        "            # Update memory queue\n",
        "            model.memory_queue.update_queue(key_emb.detach())\n",
        "\n",
        "            contrastive_epoch_loss += contrastive_loss.item()\n",
        "\n",
        "        avg_loss = contrastive_epoch_loss / len(train_loader)\n",
        "        metrics['contrastive_losses'].append(avg_loss)\n",
        "        print(f'Pretrain Epoch {epoch+1}, Avg Loss: {avg_loss:.4f}')\n",
        "\n",
        "        # Save pretrained checkpoint\n",
        "        if (epoch + 1) % 5 == 0:\n",
        "            torch.save({\n",
        "                'epoch': epoch,\n",
        "                'model_state_dict': model.state_dict(),\n",
        "                'optimizer_encoder_state_dict': optimizer_encoder.state_dict(),\n",
        "                'loss': avg_loss,\n",
        "            }, os.path.join(save_dir, f'pretrain_checkpoint_{epoch+1}.pt'))\n",
        "\n",
        "    print(\"\\nPhase 2: Training GAN-CL...\")\n",
        "#     train_epochs = 50\n",
        "    train_epochs = 10\n",
        "    for epoch in range(train_epochs):\n",
        "        epoch_losses = {\n",
        "            'contrastive': 0,\n",
        "            'adversarial': 0,\n",
        "            'similarity': 0,\n",
        "            'total': 0\n",
        "        }\n",
        "\n",
        "        for batch in tqdm(train_loader, desc=f'Train Epoch {epoch+1}/{train_epochs}'):\n",
        "            batch = batch.to(device)\n",
        "\n",
        "            # Step 1: Train Encoder\n",
        "            optimizer_encoder.zero_grad()\n",
        "\n",
        "            # Get importance scores from generator\n",
        "            with torch.no_grad():\n",
        "                node_scores, edge_scores = model.generator(batch)\n",
        "\n",
        "            # Create perturbed graph\n",
        "            perturbed_data = model.drop_graph_elements(batch, node_scores, edge_scores)\n",
        "\n",
        "            # Get embeddings\n",
        "            query_emb = model.encoder(perturbed_data)\n",
        "            with torch.no_grad():\n",
        "                key_emb = model.momentum_encoder(batch)\n",
        "                original_emb = model.encoder(batch).detach()\n",
        "\n",
        "            # Compute losses for encoder\n",
        "            contrastive_loss = model.memory_queue.compute_contrastive_loss(\n",
        "                query_emb, key_emb, model.config.temperature\n",
        "            )\n",
        "            similarity_loss = F.mse_loss(query_emb, original_emb)\n",
        "\n",
        "            # Total loss for encoder\n",
        "            encoder_loss = contrastive_loss + 0.1 * similarity_loss\n",
        "\n",
        "            # Update encoder\n",
        "            encoder_loss.backward()\n",
        "            optimizer_encoder.step()\n",
        "\n",
        "            # Update momentum encoder\n",
        "            model._momentum_update()\n",
        "\n",
        "            # Step 2: Train Generator\n",
        "            optimizer_generator.zero_grad()\n",
        "\n",
        "            # Get new embeddings for adversarial loss\n",
        "            node_scores, edge_scores = model.generator(batch)\n",
        "            perturbed_data = model.drop_graph_elements(batch, node_scores, edge_scores)\n",
        "\n",
        "            with torch.no_grad():\n",
        "                original_emb = model.encoder(batch)\n",
        "            perturbed_emb = model.encoder(perturbed_data)\n",
        "\n",
        "            # Compute adversarial loss\n",
        "            adversarial_loss = -F.mse_loss(perturbed_emb, original_emb)\n",
        "\n",
        "            # Update generator\n",
        "            adversarial_loss.backward()\n",
        "            optimizer_generator.step()\n",
        "\n",
        "            # Update memory queue\n",
        "            model.memory_queue.update_queue(key_emb.detach())\n",
        "\n",
        "            # Update metrics\n",
        "            epoch_losses['contrastive'] += contrastive_loss.item()\n",
        "            epoch_losses['adversarial'] += adversarial_loss.item()\n",
        "            epoch_losses['similarity'] += similarity_loss.item()\n",
        "            epoch_losses['total'] += encoder_loss.item()\n",
        "\n",
        "        # Average losses\n",
        "        for k in epoch_losses:\n",
        "            epoch_losses[k] /= len(train_loader)\n",
        "            metrics[f'{k}_losses'].append(epoch_losses[k])\n",
        "\n",
        "        print(f'Epoch {epoch+1}, Losses: {epoch_losses}')\n",
        "\n",
        "        # Save checkpoint\n",
        "        if (epoch + 1) % 10 == 0:\n",
        "            torch.save({\n",
        "                'epoch': epoch,\n",
        "                'model_state_dict': model.state_dict(),\n",
        "                'optimizer_encoder_state_dict': optimizer_encoder.state_dict(),\n",
        "                'optimizer_generator_state_dict': optimizer_generator.state_dict(),\n",
        "                'losses': epoch_losses,\n",
        "            }, os.path.join(save_dir, f'gan_cl_checkpoint_{epoch+1}.pt'))\n",
        "\n",
        "\n",
        "        # In train_gan_cl function, in the embedding saving section:\n",
        "        if (epoch + 1) % 10 == 0:\n",
        "            model.eval()\n",
        "            all_embeddings = []\n",
        "            all_graphs = []\n",
        "\n",
        "            with torch.no_grad():\n",
        "                for batch in train_loader:\n",
        "                    batch = batch.to(device)\n",
        "                    embeddings = model.get_embeddings(batch)\n",
        "                    all_embeddings.append(embeddings.cpu())\n",
        "                    all_graphs.extend([data for data in batch])\n",
        "\n",
        "            all_embeddings = torch.cat(all_embeddings, dim=0)\n",
        "\n",
        "            # Save embeddings\n",
        "            timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')\n",
        "            save_path = os.path.join(\n",
        "                embedding_dir,\n",
        "                f'embeddings_epoch_{epoch+1}_{timestamp}.pkl'\n",
        "            )\n",
        "            save_embeddings(\n",
        "                embeddings=all_embeddings.numpy(),\n",
        "                graphs=all_graphs,\n",
        "                filepath=save_path\n",
        "            )\n",
        "\n",
        "            model.train()\n",
        "\n",
        "    # Save final metrics\n",
        "    with open(os.path.join(save_dir, 'training_metrics.json'), 'w') as f:\n",
        "        json.dump(metrics, f)\n",
        "\n",
        "            # Save encoder periodically\n",
        "        if (epoch + 1) % 10 == 0:\n",
        "            epoch_info = {\n",
        "                **model_info,\n",
        "                'epoch': epoch + 1,\n",
        "                'loss': epoch_losses['total']\n",
        "            }\n",
        "            save_encoder(\n",
        "                model.encoder,\n",
        "                os.path.join(encoder_dir, f'encoder_epoch_{epoch+1}.pt'),\n",
        "                epoch_info\n",
        "            )\n",
        "\n",
        "        # Save best encoder based on total loss\n",
        "        if epoch_losses['total'] < best_loss:\n",
        "            best_loss = epoch_losses['total']\n",
        "            save_encoder(\n",
        "                model.encoder,\n",
        "                os.path.join(encoder_dir, f'best_encoder_{timestamp}.pt'),\n",
        "                {**model_info, 'epoch': epoch + 1, 'loss': best_loss}\n",
        "            )\n",
        "\n",
        "    # Save final encoder\n",
        "    save_encoder(\n",
        "        model.encoder,\n",
        "        os.path.join(encoder_dir, f'final_encoder_{timestamp}.pt'),\n",
        "        {**model_info, 'epoch': train_epochs, 'loss': epoch_losses['total']}\n",
        "    )\n",
        "\n",
        "    return model, metrics\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "4dbc0517",
      "metadata": {
        "id": "4dbc0517"
      },
      "outputs": [],
      "source": [
        "def main():\n",
        "     # Enable anomaly detection during development\n",
        "    torch.autograd.set_detect_anomaly(True)\n",
        "    # Your existing data loading code here\n",
        "    torch.manual_seed(42)\n",
        "    np.random.seed(42)\n",
        "\n",
        "\n",
        "    dataset = []\n",
        "    failed_smiles = []\n",
        "\n",
        "    print(\"Starting data loading...\")\n",
        "    extractor = MolecularFeatureExtractor()\n",
        "    smiles_file = \"D:\\\\PhD\\\\Chapter3\\\\Unsupervised_GAN_Code\\\\pubchem-41-clean.txt\"\n",
        "\n",
        "    # Create dataset with molecules\n",
        "    original_molecules = []  # Store original RDKit molecules\n",
        "    with open(smiles_file, 'r') as f:\n",
        "        for line in f:\n",
        "            smiles = line.strip()\n",
        "            mol = Chem.MolFromSmiles(smiles)\n",
        "            if mol is not None:\n",
        "                data = extractor.process_molecule(smiles)\n",
        "                if data is not None:\n",
        "                    dataset.append(data)\n",
        "                    original_molecules.append({\n",
        "                        'smiles': smiles,\n",
        "                        'mol': mol,\n",
        "                        'features': {\n",
        "                            'MW': Descriptors.ExactMolWt(mol),\n",
        "                            'LogP': Descriptors.MolLogP(mol),\n",
        "                            'TPSA': Descriptors.TPSA(mol),\n",
        "                            'num_rings': Chem.rdMolDescriptors.CalcNumRings(mol),\n",
        "                            'aromatic_rings': Chem.rdMolDescriptors.CalcNumAromaticRings(mol),\n",
        "                            'aliphatic_rings': Chem.rdMolDescriptors.CalcNumAliphaticRings(mol)\n",
        "                        }\n",
        "                    })\n",
        "            else:\n",
        "                failed_smiles.append(smiles)\n",
        "\n",
        "    # Save original molecule information\n",
        "    timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')\n",
        "    with open(f'./original_molecules_{timestamp}.pkl', 'wb') as f:\n",
        "        pickle.dump(original_molecules, f)\n",
        "\n",
        "    print(f\"1. Loaded dataset with {len(dataset)} graphs.\")\n",
        "    print(f\"2. Failed SMILES count: {len(failed_smiles)}\")\n",
        "\n",
        "    if not dataset:\n",
        "        print(\"No valid graphs generated.\")\n",
        "        return None\n",
        "\n",
        "    # Setup training\n",
        "    batch_size = 32\n",
        "    train_loader = DataLoader(dataset, batch_size=batch_size, shuffle=True)\n",
        "    print(f\"3. Created DataLoader with {len(train_loader.dataset)} graphs\")\n",
        "\n",
        "    # Set device\n",
        "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "    print(f\"4. Using device: {device}\")\n",
        "\n",
        "    # Get configuration based on dataset\n",
        "    config = get_model_config(dataset)\n",
        "\n",
        "    # Train model\n",
        "    print(\"5. Starting GAN-CL training...\")\n",
        "    model, metrics = train_gan_cl(\n",
        "        train_loader,\n",
        "        config,\n",
        "        device=device,\n",
        "        save_dir='./checkpoints',\n",
        "        embedding_dir='./embeddings'\n",
        "    )\n",
        "\n",
        "    print(\"6. Training completed!\")\n",
        "\n",
        "    # Extract embeddings for XAI\n",
        "    print(\"7. Extracting final embeddings for XAI...\")\n",
        "    model.eval()\n",
        "    with torch.no_grad():\n",
        "        all_embeddings = []\n",
        "        all_graphs = []\n",
        "\n",
        "        for batch in tqdm(train_loader, desc=\"Extracting embeddings\"):\n",
        "            batch = batch.to(device)\n",
        "            embeddings = model.get_embeddings(batch)\n",
        "            all_embeddings.append(embeddings.cpu())\n",
        "            all_graphs.extend([data for data in batch])\n",
        "\n",
        "    all_embeddings = torch.cat(all_embeddings, dim=0).numpy()\n",
        "\n",
        "    # Save final embeddings\n",
        "    final_embedding_path = f'./embeddings/final_embeddings_{timestamp}.pkl'\n",
        "    save_embeddings(\n",
        "        embeddings=all_embeddings,\n",
        "        graphs=all_graphs,\n",
        "        filepath=final_embedding_path\n",
        "    )\n",
        "    print(f\"8. Final embeddings saved to {final_embedding_path}\")\n",
        "\n",
        "    return model, metrics, all_embeddings, all_graphs\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    model, metrics, embeddings, graphs = main()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "27d928ef",
      "metadata": {
        "id": "27d928ef"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python [conda env:base] *",
      "language": "python",
      "name": "conda-base-py"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.13.5"
    },
    "colab": {
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}