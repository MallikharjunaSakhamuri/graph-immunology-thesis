{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "560b994d-c93d-4227-ae23-fdd65d6506b7",
      "metadata": {
        "id": "560b994d-c93d-4227-ae23-fdd65d6506b7"
      },
      "outputs": [],
      "source": [
        "!pip install kaleido"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "054078c9-6c2a-4b27-b5ce-6c0395ffd05a",
      "metadata": {
        "id": "054078c9-6c2a-4b27-b5ce-6c0395ffd05a"
      },
      "outputs": [],
      "source": [
        "import kaleido"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "6ed9b741-9aaf-44d3-a2d0-36da534ac117",
      "metadata": {
        "id": "6ed9b741-9aaf-44d3-a2d0-36da534ac117"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from typing import Dict, List, Tuple, Optional\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "# Import molecular libraries\n",
        "from deepchem.molnet import load_bace_classification, load_bbbp, load_clintox, load_delaney, load_qm9\n",
        "from rdkit import Chem\n",
        "from rdkit.Chem import Descriptors\n",
        "from rdkit import RDLogger\n",
        "RDLogger.DisableLog('rdApp.*')\n",
        "\n",
        "# DALEX for fairness\n",
        "import dalex as dx\n",
        "\n",
        "# For visualizations\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import plotly.graph_objects as go\n",
        "from plotly.subplots import make_subplots\n",
        "\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.metrics import accuracy_score, confusion_matrix, mean_squared_error, r2_score\n",
        "from sklearn.ensemble import RandomForestClassifier, RandomForestRegressor\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "import os\n",
        "from tqdm import tqdm\n",
        "import traceback\n",
        "\n",
        "# Graph neural network imports (from your code)\n",
        "from torch_geometric.data import Data, Dataset, DataLoader\n",
        "from torch_geometric.nn import GCNConv, global_mean_pool\n",
        "\n",
        "# Set style for visualizations\n",
        "plt.style.use('seaborn-v0_8-whitegrid')\n",
        "sns.set_palette(\"husl\")\n",
        "\n",
        "class MolecularFeatureExtractor:\n",
        "    \"\"\"Feature extractor for molecular graphs\"\"\"\n",
        "    def __init__(self):\n",
        "        self.atom_list = list(range(1, 119))\n",
        "        self.chirality_list = [\n",
        "            Chem.rdchem.ChiralType.CHI_UNSPECIFIED,\n",
        "            Chem.rdchem.ChiralType.CHI_TETRAHEDRAL_CW,\n",
        "            Chem.rdchem.ChiralType.CHI_TETRAHEDRAL_CCW,\n",
        "            Chem.rdchem.ChiralType.CHI_OTHER\n",
        "        ]\n",
        "        self.bond_list = [\n",
        "            Chem.rdchem.BondType.SINGLE,\n",
        "            Chem.rdchem.BondType.DOUBLE,\n",
        "            Chem.rdchem.BondType.TRIPLE,\n",
        "            Chem.rdchem.BondType.AROMATIC\n",
        "        ]\n",
        "\n",
        "    def process_molecule(self, smiles: str) -> Dict:\n",
        "        \"\"\"Extract molecular features for fairness analysis\"\"\"\n",
        "        try:\n",
        "            mol = Chem.MolFromSmiles(smiles)\n",
        "            if mol is None:\n",
        "                return None\n",
        "\n",
        "            # Basic molecular descriptors\n",
        "            features = {\n",
        "                'MW': Descriptors.ExactMolWt(mol),\n",
        "                'LogP': Descriptors.MolLogP(mol),\n",
        "                'HBD': Descriptors.NumHDonors(mol),\n",
        "                'HBA': Descriptors.NumHAcceptors(mol),\n",
        "                'TPSA': Descriptors.TPSA(mol),\n",
        "                'RotBonds': Descriptors.NumRotatableBonds(mol),\n",
        "                'AromaticRings': Descriptors.NumAromaticRings(mol),\n",
        "                'SP3_Fraction': Descriptors.FractionCSP3(mol),\n",
        "                'QED': Descriptors.qed(mol)\n",
        "            }\n",
        "\n",
        "            # Functional groups\n",
        "            functional_groups = {\n",
        "                'primary_amine': '[NX3H2]',\n",
        "                'secondary_amine': '[NX3H1]',\n",
        "                'tertiary_amine': '[NX3H0]',\n",
        "                'amide': '[NX3][CX3](=[OX1])',\n",
        "                'chloro': '[Cl]',\n",
        "                'bromo': '[Br]',\n",
        "                'fluoro': '[F]',\n",
        "                'iodo': '[I]',\n",
        "                'alcohol': '[OH]',\n",
        "                'phenol': '[OH1][c]',\n",
        "                'ether': '[OD2]([#6])[#6]',\n",
        "                'ester': '[#6][CX3](=O)[OX2H0][#6]',\n",
        "                'carbonyl': '[CX3]=O',\n",
        "                'aldehyde': '[CX3H1](=O)',\n",
        "                'ketone': '[#6][CX3](=O)[#6]',\n",
        "                'carboxyl': '[CX3](=O)[OX2H1]',\n",
        "                'sulfate': '[$(S(=O)(=O)(O)[O-,OH])]',\n",
        "                'nitro': '[N+](=O)[O-]'\n",
        "            }\n",
        "\n",
        "            for name, smarts in functional_groups.items():\n",
        "                pattern = Chem.MolFromSmarts(smarts)\n",
        "                if pattern:\n",
        "                    matches = mol.GetSubstructMatches(pattern)\n",
        "                    features[f'{name}_count'] = len(matches)\n",
        "                else:\n",
        "                    features[f'{name}_count'] = 0\n",
        "\n",
        "            return features\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"Error processing molecule {smiles}: {e}\")\n",
        "            return None\n",
        "\n",
        "class SimpleWrapper:\n",
        "    \"\"\"Wrapper class for DALEX predictions\"\"\"\n",
        "    def __init__(self, predictions):\n",
        "        self.predictions = predictions\n",
        "\n",
        "    def predict(self, X, *args, **kwargs):\n",
        "        # Handle any additional arguments DALEX might pass\n",
        "        if isinstance(X, pd.DataFrame):\n",
        "            return self.predictions[:len(X)]\n",
        "        elif isinstance(X, np.ndarray):\n",
        "            return self.predictions[:len(X)]\n",
        "        return self.predictions\n",
        "\n",
        "class MultiDatasetFairnessAnalyzer:\n",
        "    \"\"\"Comprehensive fairness analyzer for multiple molecular datasets\"\"\"\n",
        "\n",
        "    def __init__(self, output_dir='./DALEX-Fairness-Analysis'):\n",
        "        self.output_dir = output_dir\n",
        "        os.makedirs(output_dir, exist_ok=True)\n",
        "        self.feature_extractor = MolecularFeatureExtractor()\n",
        "\n",
        "        # Define compliance criteria for each dataset\n",
        "        self.compliance_criteria = {\n",
        "            'bace': {\n",
        "                'name': 'BACE_favorable',\n",
        "                'check': lambda df: (df['MW'] >= 300) & (df['MW'] <= 600) &\n",
        "                                   (df['LogP'] >= 1) & (df['LogP'] <= 5)\n",
        "            },\n",
        "            'bbbp': {\n",
        "                'name': 'BBB_favorable',\n",
        "                'check': lambda df: (df['MW'] <= 400) & (df['LogP'] <= 5) &\n",
        "                                   (df['HBD'] <= 3) & (df['TPSA'] <= 90)\n",
        "            },\n",
        "            'clintox': {\n",
        "                'name': 'RO5_compliant',\n",
        "                'check': lambda df: (df['MW'] <= 500) & (df['LogP'] <= 5) &\n",
        "                                   (df['HBD'] <= 5) & (df['HBA'] <= 10)\n",
        "            },\n",
        "            'esol': {\n",
        "                'name': 'ESOL_favorable',\n",
        "                'check': lambda df: (df['MW'] <= 500) & (df['LogP'] <= 5) &\n",
        "                                   (df['TPSA'] <= 140) & (df['RotBonds'] <= 10)\n",
        "            },\n",
        "            'qm9': {\n",
        "                'name': 'QM9_compliant',\n",
        "                'check': lambda df: (df['MW'] <= 250) & (df['RotBonds'] <= 10)\n",
        "            }\n",
        "        }\n",
        "\n",
        "    def load_dataset(self, dataset_name):\n",
        "        \"\"\"Load and preprocess dataset\"\"\"\n",
        "        print(f\"\\nLoading {dataset_name.upper()} dataset...\")\n",
        "\n",
        "        try:\n",
        "            if dataset_name == 'bace':\n",
        "                tasks, datasets, _ = load_bace_classification(featurizer='ECFP', splitter='random')\n",
        "                task_type = 'classification'\n",
        "            elif dataset_name == 'bbbp':\n",
        "                tasks, datasets, _ = load_bbbp(featurizer='ECFP', splitter='random')\n",
        "                task_type = 'classification'\n",
        "            elif dataset_name == 'clintox':\n",
        "                tasks, datasets, _ = load_clintox(featurizer='ECFP', splitter='random')\n",
        "                task_type = 'classification'\n",
        "            elif dataset_name == 'esol':\n",
        "                tasks, datasets, _ = load_delaney(featurizer='ECFP', splitter='random')\n",
        "                task_type = 'regression'\n",
        "            elif dataset_name == 'qm9':\n",
        "                tasks, datasets, _ = load_qm9(featurizer='ECFP', splitter='random', reload=False)\n",
        "                task_type = 'regression'\n",
        "                # tasks, datasets, _ = load_qm9(featurizer='ECFP', splitter='random')\n",
        "                # task_type = 'regression'\n",
        "            else:\n",
        "                raise ValueError(f\"Unknown dataset: {dataset_name}\")\n",
        "\n",
        "            train_dataset, valid_dataset, test_dataset = datasets\n",
        "\n",
        "            # Combine datasets\n",
        "            all_smiles = train_dataset.ids.tolist() + valid_dataset.ids.tolist() + test_dataset.ids.tolist()\n",
        "\n",
        "            if len(train_dataset.y.shape) > 1:\n",
        "                all_y = np.concatenate([train_dataset.y[:, 0], valid_dataset.y[:, 0], test_dataset.y[:, 0]])\n",
        "            else:\n",
        "                all_y = np.concatenate([train_dataset.y, valid_dataset.y, test_dataset.y]).flatten()\n",
        "\n",
        "            # Remove NaN values\n",
        "            valid_mask = ~np.isnan(all_y)\n",
        "            all_smiles = [s for s, m in zip(all_smiles, valid_mask) if m]\n",
        "            all_y = all_y[valid_mask]\n",
        "\n",
        "            # For regression, convert to binary for fairness analysis\n",
        "            if task_type == 'regression':\n",
        "                median_val = np.nanmedian(all_y)\n",
        "                all_y_binary = (all_y > median_val).astype(int)\n",
        "            else:\n",
        "                all_y_binary = all_y.astype(int)\n",
        "\n",
        "            # Split into train and test\n",
        "            train_size = int(0.8 * len(all_smiles))\n",
        "            train_smiles = all_smiles[:train_size]\n",
        "            test_smiles = all_smiles[train_size:]\n",
        "            train_y = all_y[:train_size]\n",
        "            test_y = all_y[train_size:]\n",
        "            train_y_binary = all_y_binary[:train_size]\n",
        "            test_y_binary = all_y_binary[train_size:]\n",
        "\n",
        "            return {\n",
        "                'train_smiles': train_smiles,\n",
        "                'test_smiles': test_smiles,\n",
        "                'train_y': train_y,\n",
        "                'test_y': test_y,\n",
        "                'train_y_binary': train_y_binary,\n",
        "                'test_y_binary': test_y_binary,\n",
        "                'task_type': task_type,\n",
        "                'task_name': tasks[0] if tasks else dataset_name\n",
        "            }\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"Error loading {dataset_name}: {e}\")\n",
        "            return None\n",
        "\n",
        "    def extract_features(self, smiles_list):\n",
        "        \"\"\"Extract molecular features from SMILES\"\"\"\n",
        "        features = []\n",
        "        valid_indices = []\n",
        "\n",
        "        for idx, smiles in enumerate(tqdm(smiles_list, desc=\"Extracting features\")):\n",
        "            feat = self.feature_extractor.process_molecule(smiles)\n",
        "            if feat is not None:\n",
        "                features.append(feat)\n",
        "                valid_indices.append(idx)\n",
        "\n",
        "        return pd.DataFrame(features), valid_indices\n",
        "\n",
        "    def train_model(self, X_train, y_train, task_type):\n",
        "        \"\"\"Train a simple model for fairness analysis\"\"\"\n",
        "        if task_type == 'classification':\n",
        "            model = RandomForestClassifier(n_estimators=100, max_depth=10, random_state=42)\n",
        "        else:\n",
        "            model = RandomForestRegressor(n_estimators=100, max_depth=10, random_state=42)\n",
        "\n",
        "        model.fit(X_train, y_train)\n",
        "        return model\n",
        "\n",
        "    def create_dalex_fairness_plots(self, explainer, protected, dataset_name, criteria_name):\n",
        "        \"\"\"Generate DALEX fairness visualizations using native DALEX methods\"\"\"\n",
        "        print(f\"\\nGenerating DALEX fairness plots for {dataset_name}...\")\n",
        "\n",
        "        # Create dataset-specific directory\n",
        "        dataset_dir = os.path.join(self.output_dir, dataset_name)\n",
        "        os.makedirs(dataset_dir, exist_ok=True)\n",
        "\n",
        "        try:\n",
        "            # Ensure protected is a proper pandas Series\n",
        "            if not isinstance(protected, pd.Series):\n",
        "                protected = pd.Series(protected)\n",
        "\n",
        "            protected = protected.reset_index(drop=True)\n",
        "\n",
        "            print(f\"Protected attribute shape: {protected.shape}\")\n",
        "            print(f\"Protected attribute unique values: {protected.unique()}\")\n",
        "\n",
        "            # Create DALEX fairness object\n",
        "            fobject = explainer.model_fairness(\n",
        "                protected=protected,\n",
        "                privileged='compliant' if 'compliant' in protected.unique() else protected.unique()[0]\n",
        "            )\n",
        "\n",
        "            # Get fairness check results\n",
        "            print(\"\\nRunning fairness check...\")\n",
        "            fairness_check = fobject.fairness_check(epsilon=0.8, verbose=True)\n",
        "\n",
        "            # Get fairness results DataFrame\n",
        "            fairness_results = fobject.result\n",
        "            print(f\"\\nFairness Metrics for {dataset_name}:\")\n",
        "            print(fairness_results)\n",
        "\n",
        "            # Save metrics to file\n",
        "            metrics_file = os.path.join(dataset_dir, f'{dataset_name}_fairness_metrics.txt')\n",
        "            with open(metrics_file, 'w') as f:\n",
        "                f.write(f\"DALEX Fairness Metrics for {dataset_name.upper()}\\n\")\n",
        "                f.write(f\"Criteria: {criteria_name}\\n\")\n",
        "                f.write(\"=\"*50 + \"\\n\")\n",
        "                f.write(str(fairness_results))\n",
        "                f.write(\"\\n\\nFairness Check Summary:\\n\")\n",
        "                f.write(str(fairness_check))\n",
        "            print(f\"Saved metrics to: {metrics_file}\")\n",
        "\n",
        "            # Fix plotly compatibility issue by downgrading or patching\n",
        "            import plotly.graph_objects as go\n",
        "\n",
        "            # Patch the plotly issue\n",
        "            import plotly\n",
        "            if hasattr(plotly, '__version__'):\n",
        "                print(f\"Plotly version: {plotly.__version__}\")\n",
        "\n",
        "            # Generate DALEX fairness plots with proper error handling\n",
        "            print(\"\\nGenerating DALEX fairness visualizations...\")\n",
        "\n",
        "            try:\n",
        "                # Method 1: Generate fairness check bar plot (like Image 1)\n",
        "                print(\"Creating fairness metrics bar plot...\")\n",
        "\n",
        "                # Calculate fairness differences\n",
        "                if len(fairness_results) > 1:\n",
        "                    # Get the unprivileged group (non-compliant)\n",
        "                    unpriv_group = [g for g in fairness_results.index if g != 'compliant'][0]\n",
        "\n",
        "                    # Calculate differences for standard fairness metrics\n",
        "                    differences = {\n",
        "                        'Accuracy Equality': fairness_results.loc[unpriv_group, 'ACC'] - fairness_results.loc['compliant', 'ACC'] if 'ACC' in fairness_results.columns else 0,\n",
        "                        'Predictive Parity': fairness_results.loc[unpriv_group, 'PPV'] - fairness_results.loc['compliant', 'PPV'] if 'PPV' in fairness_results.columns else 0,\n",
        "                        'Predictive Equality': fairness_results.loc[unpriv_group, 'FPR'] - fairness_results.loc['compliant', 'FPR'] if 'FPR' in fairness_results.columns else 0,\n",
        "                        'Equal Opportunity': fairness_results.loc[unpriv_group, 'TPR'] - fairness_results.loc['compliant', 'TPR'] if 'TPR' in fairness_results.columns else 0,\n",
        "                        'Statistical Parity': fairness_results.loc[unpriv_group, 'STP'] - fairness_results.loc['compliant', 'STP'] if 'STP' in fairness_results.columns else 0\n",
        "                    }\n",
        "\n",
        "                    # Create the bar plot (like Image 1)\n",
        "                    fig = go.Figure()\n",
        "\n",
        "                    metrics_names = list(differences.keys())\n",
        "                    metrics_values = list(differences.values())\n",
        "\n",
        "                    # Add descriptions\n",
        "                    descriptions = {\n",
        "                        'Accuracy Equality': 'Difference in overall accuracy',\n",
        "                        'Predictive Parity': 'Difference in Positive Predictive Values',\n",
        "                        'Predictive Equality': 'Difference in False Positive Rates',\n",
        "                        'Equal Opportunity': 'Difference in True Positive Rates',\n",
        "                        'Statistical Parity': 'Difference in overall prediction rates'\n",
        "                    }\n",
        "\n",
        "                    # Create horizontal bar chart\n",
        "                    fig.add_trace(go.Bar(\n",
        "                        y=metrics_names,\n",
        "                        x=metrics_values,\n",
        "                        orientation='h',\n",
        "                        marker_color='#2E4057',\n",
        "                        text=[f'{v:.3f}' for v in metrics_values],\n",
        "                        textposition='outside',\n",
        "                        hovertemplate='<b>%{y}</b><br>' +\n",
        "                                     '%{x:.3f}<br>' +\n",
        "                                     '<extra></extra>'\n",
        "                    ))\n",
        "\n",
        "                    # Add vertical line at 0 (perfect fairness)\n",
        "                    fig.add_vline(x=0, line_dash=\"dash\", line_color=\"red\",\n",
        "                                 annotation_text=\"0 = Perfect Fairness\")\n",
        "\n",
        "                    # Update layout to match reference image\n",
        "                    fig.update_layout(\n",
        "                        title=dict(\n",
        "                            text=f'Fairness Metrics - {dataset_name.upper()}<br>Subgroups: {criteria_name} Compliant vs Non-compliant',\n",
        "                            font=dict(size=16)\n",
        "                        ),\n",
        "                        xaxis_title='Difference (Non-compliant - Compliant)',\n",
        "                        yaxis_title='Metric',\n",
        "                        template='plotly_white',\n",
        "                        showlegend=False,\n",
        "                        height=400,\n",
        "                        margin=dict(l=200),\n",
        "                        xaxis=dict(range=[-0.5, 0.5])\n",
        "                    )\n",
        "\n",
        "                    # Add metric descriptions as y-axis labels\n",
        "                    fig.update_yaxes(\n",
        "                        ticktext=[f\"{name}<br><sub>{descriptions[name]}</sub>\" for name in metrics_names],\n",
        "                        tickvals=list(range(len(metrics_names)))\n",
        "                    )\n",
        "\n",
        "                    # Save the plot\n",
        "                    html_file = os.path.join(dataset_dir, f'{dataset_name}_fairness_bar_chart.html')\n",
        "                    fig.write_html(html_file)\n",
        "                    print(f\"Saved: {html_file}\")\n",
        "\n",
        "                    # Save as image if kaleido is installed\n",
        "                    try:\n",
        "                        png_file = os.path.join(dataset_dir, f'{dataset_name}_fairness_bar_chart.png')\n",
        "                        fig.write_image(png_file, width=1200, height=600)\n",
        "                        print(f\"Saved PNG: {png_file}\")\n",
        "                    except Exception as e:\n",
        "                        print(f\"Could not save PNG (install kaleido with: pip install kaleido): {e}\")\n",
        "\n",
        "            except Exception as e:\n",
        "                print(f\"Could not generate fairness bar chart: {e}\")\n",
        "\n",
        "            try:\n",
        "                # Method 2: Generate performance radar plot (like Image 2)\n",
        "                print(\"Creating performance radar plot...\")\n",
        "\n",
        "                if len(fairness_results) > 1:\n",
        "                    import plotly.graph_objects as go\n",
        "\n",
        "                    fig = go.Figure()\n",
        "\n",
        "                    # Define metrics to show in radar\n",
        "                    radar_metrics = ['TPR', 'TNR', 'ACC', 'PPV']\n",
        "                    radar_labels = ['True Positive Rate', 'True Negative Rate', 'Accuracy', 'Precision']\n",
        "\n",
        "                    for group in fairness_results.index:\n",
        "                        values = []\n",
        "                        for metric in radar_metrics:\n",
        "                            if metric in fairness_results.columns:\n",
        "                                values.append(fairness_results.loc[group, metric])\n",
        "                            else:\n",
        "                                values.append(0)\n",
        "\n",
        "                        # Close the radar\n",
        "                        values.append(values[0])\n",
        "                        labels_plot = radar_labels + [radar_labels[0]]\n",
        "\n",
        "                        fig.add_trace(go.Scatterpolar(\n",
        "                            r=values,\n",
        "                            theta=labels_plot,\n",
        "                            fill='toself',\n",
        "                            name=f'{criteria_name} {group.replace(\"_\", \"-\")}',\n",
        "                            opacity=0.6\n",
        "                        ))\n",
        "\n",
        "                    fig.update_layout(\n",
        "                        polar=dict(\n",
        "                            radialaxis=dict(\n",
        "                                visible=True,\n",
        "                                range=[0, 1]\n",
        "                            )\n",
        "                        ),\n",
        "                        title=f'Performance Metrics by Group - {dataset_name.upper()}',\n",
        "                        showlegend=True,\n",
        "                        template='plotly_white'\n",
        "                    )\n",
        "\n",
        "                    # Save the plot\n",
        "                    html_file = os.path.join(dataset_dir, f'{dataset_name}_performance_radar.html')\n",
        "                    fig.write_html(html_file)\n",
        "                    print(f\"Saved: {html_file}\")\n",
        "\n",
        "                    try:\n",
        "                        png_file = os.path.join(dataset_dir, f'{dataset_name}_performance_radar.png')\n",
        "                        fig.write_image(png_file, width=800, height=800)\n",
        "                        print(f\"Saved PNG: {png_file}\")\n",
        "                    except Exception as e:\n",
        "                        print(f\"Could not save PNG (install kaleido with: pip install kaleido): {e}\")\n",
        "\n",
        "            except Exception as e:\n",
        "                print(f\"Could not generate performance radar: {e}\")\n",
        "\n",
        "            try:\n",
        "                # Method 3: Try to use DALEX's native plot method if available\n",
        "                print(\"Attempting DALEX native plot...\")\n",
        "\n",
        "                # Monkey-patch plotly to fix the titlefont issue\n",
        "                import plotly.graph_objs as go\n",
        "                original_layout = go.Layout\n",
        "\n",
        "                class PatchedLayout(original_layout):\n",
        "                    def __init__(self, *args, **kwargs):\n",
        "                        # Fix titlefont to title.font\n",
        "                        if 'titlefont' in kwargs:\n",
        "                            if 'title' not in kwargs:\n",
        "                                kwargs['title'] = {}\n",
        "                            if isinstance(kwargs['title'], str):\n",
        "                                kwargs['title'] = {'text': kwargs['title']}\n",
        "                            kwargs['title']['font'] = kwargs.pop('titlefont')\n",
        "                        super().__init__(*args, **kwargs)\n",
        "\n",
        "                go.Layout = PatchedLayout\n",
        "\n",
        "                # Now try DALEX's plot\n",
        "                try:\n",
        "                    # Get the plot object from DALEX\n",
        "                    plot = fobject.plot(show=False)\n",
        "\n",
        "                    if plot is not None:\n",
        "                        # Save as HTML\n",
        "                        html_file = os.path.join(dataset_dir, f'{dataset_name}_dalex_fairness_check.html')\n",
        "                        plot.write_html(html_file)\n",
        "                        print(f\"Saved DALEX fairness check: {html_file}\")\n",
        "\n",
        "                        # Save as PNG (requires kaleido)\n",
        "                        try:\n",
        "                            png_file = os.path.join(dataset_dir, f'{dataset_name}_dalex_fairness_check.png')\n",
        "                            plot.write_image(png_file, width=1200, height=800)\n",
        "                            print(f\"Saved PNG: {png_file}\")\n",
        "                        except Exception as e:\n",
        "                            print(f\"Could not save PNG (install kaleido): {e}\")\n",
        "\n",
        "                except Exception as e:\n",
        "                    print(f\"DALEX plot generation failed: {e}\")\n",
        "\n",
        "                    # Alternative: Create fairness check visualization manually\n",
        "                    print(\"Creating manual fairness check visualization...\")\n",
        "                    self.create_fairness_check_plot(fairness_results, fairness_check,\n",
        "                                                   dataset_name, criteria_name, dataset_dir)\n",
        "\n",
        "                # Restore original Layout\n",
        "                go.Layout = original_layout\n",
        "\n",
        "            except Exception as e:\n",
        "                print(f\"Could not generate DALEX native plot: {e}\")\n",
        "                # Create manual fairness check as fallback\n",
        "                self.create_fairness_check_plot(fairness_results, fairness_check,\n",
        "                                               dataset_name, criteria_name, dataset_dir)\n",
        "\n",
        "            return fobject, fairness_results\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"Error creating DALEX fairness plots: {e}\")\n",
        "            traceback.print_exc()\n",
        "\n",
        "            # Use fallback only if DALEX completely fails\n",
        "            print(\"\\nUsing fallback visualization...\")\n",
        "            return self.create_dalex_fairness_plots_fallback(explainer, protected, dataset_name, criteria_name)\n",
        "\n",
        "    def create_fairness_check_plot(self, fairness_results, fairness_check,\n",
        "                                   dataset_name, criteria_name, dataset_dir):\n",
        "        \"\"\"Create fairness check visualization like Image 4\"\"\"\n",
        "        import plotly.graph_objects as go\n",
        "        from plotly.subplots import make_subplots\n",
        "\n",
        "        # Create subplots for independence, separation, sufficiency\n",
        "        fig = make_subplots(\n",
        "            rows=3, cols=1,\n",
        "            subplot_titles=('independence', 'separation', 'sufficiency'),\n",
        "            vertical_spacing=0.15,\n",
        "            specs=[[{'type': 'bar'}], [{'type': 'bar'}], [{'type': 'bar'}]]\n",
        "        )\n",
        "\n",
        "        # Calculate fairness scores based on DALEX metrics\n",
        "        if len(fairness_results) > 1:\n",
        "            unpriv_group = [g for g in fairness_results.index if g != 'compliant'][0]\n",
        "\n",
        "            # Independence: Statistical Parity (STP ratio)\n",
        "            independence_score = fairness_results.loc[unpriv_group, 'STP'] if 'STP' in fairness_results.columns else 1.0\n",
        "\n",
        "            # Separation: Equal Opportunity (TPR ratio)\n",
        "            separation_score = fairness_results.loc[unpriv_group, 'TPR'] if 'TPR' in fairness_results.columns else 1.0\n",
        "\n",
        "            # Sufficiency: Predictive Parity (PPV ratio)\n",
        "            sufficiency_score = fairness_results.loc[unpriv_group, 'PPV'] if 'PPV' in fairness_results.columns else 1.0\n",
        "\n",
        "            # Scale scores for visualization (like in reference image)\n",
        "            independence_display = independence_score * 30\n",
        "            separation_display = (separation_score - 1) * 5 + 1  # Scale around 1\n",
        "            sufficiency_display = (sufficiency_score - 1) * 2 + 1  # Scale around 1\n",
        "\n",
        "            # Add bars\n",
        "            fig.add_trace(\n",
        "                go.Bar(\n",
        "                    x=[independence_display],\n",
        "                    y=[f'{criteria_name}-noncompliant'],\n",
        "                    orientation='h',\n",
        "                    marker_color='teal',\n",
        "                    showlegend=False,\n",
        "                    text=[f'{independence_display:.1f}'],\n",
        "                    textposition='outside'\n",
        "                ),\n",
        "                row=1, col=1\n",
        "            )\n",
        "\n",
        "            fig.add_trace(\n",
        "                go.Bar(\n",
        "                    x=[separation_display],\n",
        "                    y=[f'{criteria_name}-noncompliant'],\n",
        "                    orientation='h',\n",
        "                    marker_color='teal',\n",
        "                    showlegend=False,\n",
        "                    text=[f'{separation_display:.1f}'],\n",
        "                    textposition='outside'\n",
        "                ),\n",
        "                row=2, col=1\n",
        "            )\n",
        "\n",
        "            fig.add_trace(\n",
        "                go.Bar(\n",
        "                    x=[sufficiency_display],\n",
        "                    y=[f'{criteria_name}-noncompliant'],\n",
        "                    orientation='h',\n",
        "                    marker_color='teal',\n",
        "                    showlegend=False,\n",
        "                    text=[f'{sufficiency_display:.1f}'],\n",
        "                    textposition='outside'\n",
        "                ),\n",
        "                row=3, col=1\n",
        "            )\n",
        "\n",
        "            # Update layout\n",
        "            fig.update_layout(\n",
        "                title=dict(\n",
        "                    text=f'Fairness Check - {dataset_name.upper()}',\n",
        "                    font=dict(size=20, color='blue'),\n",
        "                    x=0.5,\n",
        "                    xanchor='center'\n",
        "                ),\n",
        "                height=600,\n",
        "                showlegend=False,\n",
        "                template='plotly_white',\n",
        "                plot_bgcolor='rgba(255, 192, 203, 0.1)'  # Light pink background\n",
        "            )\n",
        "\n",
        "            # Update axes\n",
        "            fig.update_xaxes(title_text='score', row=1, col=1, range=[0, 35])\n",
        "            fig.update_xaxes(title_text='score', row=2, col=1, range=[0, 5])\n",
        "            fig.update_xaxes(title_text='score', row=3, col=1, range=[0, 2])\n",
        "\n",
        "            # Update y-axes\n",
        "            for i in range(1, 4):\n",
        "                fig.update_yaxes(title_text='subgroup', row=i, col=1)\n",
        "\n",
        "            # Save the plot\n",
        "            html_file = os.path.join(dataset_dir, f'{dataset_name}_fairness_check.html')\n",
        "            fig.write_html(html_file)\n",
        "            print(f\"Saved fairness check: {html_file}\")\n",
        "\n",
        "            # Save as PNG\n",
        "            try:\n",
        "                png_file = os.path.join(dataset_dir, f'{dataset_name}_fairness_check.png')\n",
        "                fig.write_image(png_file, width=800, height=600)\n",
        "                print(f\"Saved PNG: {png_file}\")\n",
        "            except Exception as e:\n",
        "                print(f\"Could not save PNG (install kaleido): {e}\")\n",
        "\n",
        "    def create_dalex_fairness_plots_fallback(self, explainer, protected, dataset_name, criteria_name):\n",
        "        \"\"\"Fallback method for creating fairness visualizations using matplotlib\"\"\"\n",
        "        print(\"Using fallback method for fairness visualizations...\")\n",
        "\n",
        "        dataset_dir = os.path.join(self.output_dir, dataset_name)\n",
        "\n",
        "        # Get predictions and true values\n",
        "        y_true = explainer.y\n",
        "        y_pred = explainer.predict(explainer.data)\n",
        "\n",
        "        # Ensure we have two groups\n",
        "        unique_groups = protected.unique()\n",
        "        if len(unique_groups) < 2:\n",
        "            print(\"Cannot perform fairness analysis with only one group\")\n",
        "            return None, None\n",
        "\n",
        "        # For regression, convert to binary for fairness metrics\n",
        "        if explainer.model_type == 'regression':\n",
        "            y_pred_binary = (y_pred > np.median(y_pred)).astype(int)\n",
        "            y_true_binary = (y_true > np.median(y_true)).astype(int)\n",
        "        else:\n",
        "            y_pred_binary = (y_pred > 0.5).astype(int)\n",
        "            y_true_binary = y_true\n",
        "\n",
        "        # Calculate metrics for each group\n",
        "        metrics_by_group = {}\n",
        "\n",
        "        for group in unique_groups:\n",
        "            group_mask = (protected == group).values\n",
        "\n",
        "            if group_mask.sum() > 0:\n",
        "                from sklearn.metrics import accuracy_score, precision_score, recall_score, confusion_matrix\n",
        "\n",
        "                y_true_group = y_true_binary[group_mask]\n",
        "                y_pred_group = y_pred_binary[group_mask]\n",
        "\n",
        "                # Calculate confusion matrix\n",
        "                tn, fp, fn, tp = confusion_matrix(y_true_group, y_pred_group).ravel() if len(np.unique(y_true_group)) > 1 else (0, 0, 0, 0)\n",
        "\n",
        "                # Calculate metrics\n",
        "                metrics_by_group[group] = {\n",
        "                    'accuracy': accuracy_score(y_true_group, y_pred_group),\n",
        "                    'precision': precision_score(y_true_group, y_pred_group, zero_division=0),\n",
        "                    'recall': recall_score(y_true_group, y_pred_group, zero_division=0),\n",
        "                    'selection_rate': np.mean(y_pred_group),\n",
        "                    'tpr': tp / (tp + fn) if (tp + fn) > 0 else 0,\n",
        "                    'fpr': fp / (fp + tn) if (fp + tn) > 0 else 0,\n",
        "                    'tnr': tn / (tn + fp) if (tn + fp) > 0 else 0,\n",
        "                    'group_size': group_mask.sum()\n",
        "                }\n",
        "\n",
        "        # Calculate fairness metrics (using first two groups)\n",
        "        groups = list(metrics_by_group.keys())[:2]\n",
        "        if len(groups) == 2:\n",
        "            fairness_metrics = {\n",
        "                'Statistical_Parity': metrics_by_group[groups[1]]['selection_rate'] - metrics_by_group[groups[0]]['selection_rate'],\n",
        "                'Equal_Opportunity': metrics_by_group[groups[1]]['tpr'] - metrics_by_group[groups[0]]['tpr'],\n",
        "                'Predictive_Equality': metrics_by_group[groups[1]]['fpr'] - metrics_by_group[groups[0]]['fpr'],\n",
        "                'Accuracy_Equality': metrics_by_group[groups[1]]['accuracy'] - metrics_by_group[groups[0]]['accuracy'],\n",
        "                'Disparate_Impact': metrics_by_group[groups[1]]['selection_rate'] / (metrics_by_group[groups[0]]['selection_rate'] + 1e-10)\n",
        "            }\n",
        "        else:\n",
        "            fairness_metrics = {}\n",
        "\n",
        "        # Create comprehensive visualization\n",
        "        fig = plt.figure(figsize=(16, 12))\n",
        "\n",
        "        # 1. Fairness Metrics Bar Chart\n",
        "        ax1 = plt.subplot(3, 3, 1)\n",
        "        if fairness_metrics:\n",
        "            metrics_names = list(fairness_metrics.keys())\n",
        "            metrics_values = list(fairness_metrics.values())\n",
        "            colors = ['red' if abs(v) > 0.1 else 'green' for v in metrics_values]\n",
        "            ax1.barh(metrics_names, metrics_values, color=colors, alpha=0.7)\n",
        "            ax1.axvline(0, color='black', linestyle='--', alpha=0.5)\n",
        "            ax1.set_xlabel('Difference (0 = Fair)')\n",
        "            ax1.set_title('Fairness Metrics', fontweight='bold')\n",
        "            ax1.grid(True, alpha=0.3)\n",
        "\n",
        "        # 2. Group Performance Comparison\n",
        "        ax2 = plt.subplot(3, 3, 2)\n",
        "        group_names = list(metrics_by_group.keys())\n",
        "        x = np.arange(len(['Accuracy', 'Precision', 'Recall']))\n",
        "        width = 0.35\n",
        "\n",
        "        for i, group in enumerate(group_names[:2]):\n",
        "            values = [\n",
        "                metrics_by_group[group]['accuracy'],\n",
        "                metrics_by_group[group]['precision'],\n",
        "                metrics_by_group[group]['recall']\n",
        "            ]\n",
        "            ax2.bar(x + i*width, values, width, label=f'{group} (n={metrics_by_group[group][\"group_size\"]})')\n",
        "\n",
        "        ax2.set_ylabel('Score')\n",
        "        ax2.set_title('Performance by Group', fontweight='bold')\n",
        "        ax2.set_xticks(x + width/2)\n",
        "        ax2.set_xticklabels(['Accuracy', 'Precision', 'Recall'])\n",
        "        ax2.legend()\n",
        "        ax2.grid(True, alpha=0.3)\n",
        "        ax2.set_ylim([0, 1])\n",
        "\n",
        "        # 3. Selection Rate Comparison\n",
        "        ax3 = plt.subplot(3, 3, 3)\n",
        "        selection_rates = [metrics_by_group[g]['selection_rate'] for g in group_names]\n",
        "        ax3.bar(group_names, selection_rates, color=['blue', 'orange'])\n",
        "        ax3.set_ylabel('Selection Rate')\n",
        "        ax3.set_title('Selection Rate by Group', fontweight='bold')\n",
        "        ax3.set_ylim([0, 1])\n",
        "        ax3.grid(True, alpha=0.3)\n",
        "\n",
        "        # 4. ROC Space\n",
        "        ax4 = plt.subplot(3, 3, 4)\n",
        "        for group in group_names[:2]:\n",
        "            fpr = metrics_by_group[group]['fpr']\n",
        "            tpr = metrics_by_group[group]['tpr']\n",
        "            ax4.scatter(fpr, tpr, s=100, label=group)\n",
        "        ax4.plot([0, 1], [0, 1], 'k--', alpha=0.5)\n",
        "        ax4.set_xlabel('False Positive Rate')\n",
        "        ax4.set_ylabel('True Positive Rate')\n",
        "        ax4.set_title('ROC Space', fontweight='bold')\n",
        "        ax4.legend()\n",
        "        ax4.grid(True, alpha=0.3)\n",
        "        ax4.set_xlim([-0.05, 1.05])\n",
        "        ax4.set_ylim([-0.05, 1.05])\n",
        "\n",
        "        # 5. Confusion Matrix Comparison\n",
        "        for idx, group in enumerate(group_names[:2]):\n",
        "            ax = plt.subplot(3, 3, 5 + idx)\n",
        "            group_mask = (protected == group).values\n",
        "\n",
        "            if group_mask.sum() > 0:\n",
        "                cm = confusion_matrix(y_true_binary[group_mask], y_pred_binary[group_mask])\n",
        "                sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', ax=ax)\n",
        "                ax.set_title(f'Confusion Matrix - {group}', fontweight='bold')\n",
        "                ax.set_xlabel('Predicted')\n",
        "                ax.set_ylabel('Actual')\n",
        "\n",
        "        # 7. Sample Distribution\n",
        "        ax7 = plt.subplot(3, 3, 7)\n",
        "        sizes = [metrics_by_group[g]['group_size'] for g in group_names]\n",
        "        ax7.pie(sizes, labels=group_names, autopct='%1.1f%%', startangle=90)\n",
        "        ax7.set_title('Group Distribution', fontweight='bold')\n",
        "\n",
        "        # 8. Metric Differences Table\n",
        "        ax8 = plt.subplot(3, 3, 8)\n",
        "        ax8.axis('tight')\n",
        "        ax8.axis('off')\n",
        "\n",
        "        if len(groups) == 2:\n",
        "            table_data = []\n",
        "            for metric in ['accuracy', 'precision', 'recall', 'tpr', 'fpr']:\n",
        "                diff = metrics_by_group[groups[1]][metric] - metrics_by_group[groups[0]][metric]\n",
        "                table_data.append([\n",
        "                    metric.capitalize(),\n",
        "                    f\"{metrics_by_group[groups[0]][metric]:.3f}\",\n",
        "                    f\"{metrics_by_group[groups[1]][metric]:.3f}\",\n",
        "                    f\"{diff:+.3f}\"\n",
        "                ])\n",
        "\n",
        "            table = ax8.table(cellText=table_data,\n",
        "                            colLabels=[' Metric', groups[0][:15], groups[1][:15], 'Difference'],\n",
        "                            cellLoc='center',\n",
        "                            loc='center')\n",
        "            table.auto_set_font_size(False)\n",
        "            table.set_fontsize(9)\n",
        "            table.scale(1.2, 1.5)\n",
        "\n",
        "        # 9. Fairness Score Summary\n",
        "        ax9 = plt.subplot(3, 3, 9)\n",
        "        ax9.axis('off')\n",
        "\n",
        "        if fairness_metrics:\n",
        "            # Calculate overall fairness score\n",
        "            fairness_violations = sum([\n",
        "                abs(fairness_metrics.get('Statistical_Parity', 0)) > 0.1,\n",
        "                abs(fairness_metrics.get('Equal_Opportunity', 0)) > 0.1,\n",
        "                abs(fairness_metrics.get('Predictive_Equality', 0)) > 0.1,\n",
        "                abs(fairness_metrics.get('Accuracy_Equality', 0)) > 0.1,\n",
        "                fairness_metrics.get('Disparate_Impact', 1) < 0.8 or fairness_metrics.get('Disparate_Impact', 1) > 1.25\n",
        "            ])\n",
        "\n",
        "            fairness_score = 1 - (fairness_violations / 5)\n",
        "            color = 'green' if fairness_score > 0.8 else 'orange' if fairness_score > 0.6 else 'red'\n",
        "\n",
        "            ax9.text(0.5, 0.7, 'Overall Fairness Score', ha='center', fontsize=14, fontweight='bold')\n",
        "            ax9.text(0.5, 0.4, f'{fairness_score:.1%}', ha='center', fontsize=24, color=color, fontweight='bold')\n",
        "            ax9.text(0.5, 0.1, f'{5-fairness_violations}/5 metrics within threshold', ha='center', fontsize=10)\n",
        "\n",
        "        plt.suptitle(f'Fairness Analysis - {dataset_name.upper()}\\nCriteria: {criteria_name}',\n",
        "                    fontsize=16, fontweight='bold')\n",
        "        plt.tight_layout()\n",
        "\n",
        "        # Save plot\n",
        "        plot_file = os.path.join(dataset_dir, f'{dataset_name}_fairness_analysis.png')\n",
        "        plt.savefig(plot_file, dpi=300, bbox_inches='tight')\n",
        "        plt.close()\n",
        "        print(f\"Saved comprehensive fairness analysis to: {plot_file}\")\n",
        "\n",
        "        # Save metrics to file\n",
        "        metrics_file = os.path.join(dataset_dir, f'{dataset_name}_fairness_metrics.json')\n",
        "        import json\n",
        "        with open(metrics_file, 'w') as f:\n",
        "            json.dump({\n",
        "                'group_metrics': metrics_by_group,\n",
        "                'fairness_metrics': fairness_metrics,\n",
        "                'criteria': criteria_name\n",
        "            }, f, indent=2)\n",
        "        print(f\"Saved metrics to: {metrics_file}\")\n",
        "\n",
        "        # Create results DataFrame\n",
        "        fairness_results = pd.DataFrame({\n",
        "            'metric': list(fairness_metrics.keys()),\n",
        "            'value': list(fairness_metrics.values())\n",
        "        })\n",
        "\n",
        "        return None, fairness_results\n",
        "\n",
        "    def create_functional_group_heatmap(self, X_test, y_test, y_pred, dataset_name):\n",
        "        \"\"\"Create functional group fairness heatmap\"\"\"\n",
        "        print(f\"\\nGenerating functional group heatmap for {dataset_name}...\")\n",
        "\n",
        "        dataset_dir = os.path.join(self.output_dir, dataset_name)\n",
        "\n",
        "        # Calculate fairness metrics for functional groups\n",
        "        fg_metrics = {}\n",
        "\n",
        "        for col in X_test.columns:\n",
        "            if col.endswith('_count'):\n",
        "                # Use median as threshold\n",
        "                threshold = X_test[col].median()\n",
        "                privileged = X_test[col] <= threshold\n",
        "\n",
        "                # Calculate metrics\n",
        "                if privileged.sum() > 0 and (~privileged).sum() > 0:\n",
        "                    # Binary predictions\n",
        "                    y_pred_binary = (y_pred > 0.5).astype(int) if y_pred.max() <= 1 else (y_pred > np.median(y_pred)).astype(int)\n",
        "\n",
        "                    # Disparate Impact\n",
        "                    prob_priv = np.mean(y_pred_binary[privileged])\n",
        "                    prob_unpriv = np.mean(y_pred_binary[~privileged])\n",
        "                    di = prob_unpriv / (prob_priv + 1e-6)\n",
        "\n",
        "                    # Statistical Parity Difference\n",
        "                    spd = prob_unpriv - prob_priv\n",
        "\n",
        "                    # Equal Opportunity Difference\n",
        "                    pos_mask = y_test == 1\n",
        "                    if (privileged & pos_mask).sum() > 0 and (~privileged & pos_mask).sum() > 0:\n",
        "                        tpr_priv = np.mean(y_pred_binary[privileged & pos_mask] == 1)\n",
        "                        tpr_unpriv = np.mean(y_pred_binary[~privileged & pos_mask] == 1)\n",
        "                        eod = tpr_unpriv - tpr_priv\n",
        "                    else:\n",
        "                        eod = 0.0\n",
        "\n",
        "                    fg_name = col.replace('_count', '')\n",
        "                    fg_metrics[fg_name] = {'DI': di, 'SPD': spd, 'EOD': eod}\n",
        "\n",
        "        if fg_metrics:\n",
        "            # Create DataFrame\n",
        "            metrics_df = pd.DataFrame(fg_metrics).T\n",
        "\n",
        "            # Create heatmap\n",
        "            plt.figure(figsize=(10, max(8, len(metrics_df) * 0.4)))\n",
        "\n",
        "            sns.heatmap(metrics_df,\n",
        "                       annot=True,\n",
        "                       fmt='.2f',\n",
        "                       cmap='RdBu_r',\n",
        "                       center=0 if 'SPD' in metrics_df.columns else 1,\n",
        "                       cbar_kws={'label': 'Metric Value'},\n",
        "                       vmin=-1, vmax=2)\n",
        "\n",
        "            plt.title(f'Functional Group Fairness - {dataset_name.upper()}', fontsize=14, fontweight='bold')\n",
        "            plt.xlabel('Fairness Metrics', fontsize=12)\n",
        "            plt.ylabel('Functional Groups', fontsize=12)\n",
        "            plt.tight_layout()\n",
        "\n",
        "            # Save plot\n",
        "            heatmap_file = os.path.join(dataset_dir, f'{dataset_name}_functional_group_heatmap.png')\n",
        "            plt.savefig(heatmap_file, dpi=300, bbox_inches='tight')\n",
        "            plt.close()\n",
        "            print(f\"Saved: {heatmap_file}\")\n",
        "\n",
        "            return fg_metrics\n",
        "\n",
        "        return None\n",
        "\n",
        "    def analyze_dataset(self, dataset_name):\n",
        "        \"\"\"Complete fairness analysis for a single dataset\"\"\"\n",
        "        print(f\"\\n{'='*60}\")\n",
        "        print(f\"Analyzing {dataset_name.upper()}\")\n",
        "        print(f\"{'='*60}\")\n",
        "\n",
        "        # Load dataset\n",
        "        data = self.load_dataset(dataset_name)\n",
        "        if data is None:\n",
        "            return None\n",
        "\n",
        "        # Extract features\n",
        "        print(\"Extracting molecular features...\")\n",
        "        train_features, train_valid_idx = self.extract_features(data['train_smiles'])\n",
        "        test_features, test_valid_idx = self.extract_features(data['test_smiles'])\n",
        "\n",
        "        # Filter labels\n",
        "        if data['task_type'] == 'classification':\n",
        "            train_y = data['train_y_binary'][train_valid_idx]\n",
        "            test_y = data['test_y_binary'][test_valid_idx]\n",
        "        else:\n",
        "            train_y = data['train_y'][train_valid_idx]\n",
        "            test_y = data['test_y'][test_valid_idx]\n",
        "            # For regression fairness, use binary version\n",
        "            test_y_binary = data['test_y_binary'][test_valid_idx]\n",
        "\n",
        "        # Standardize features\n",
        "        scaler = StandardScaler()\n",
        "        X_train = scaler.fit_transform(train_features.fillna(0))\n",
        "        X_test = scaler.transform(test_features.fillna(0))\n",
        "\n",
        "        # Train model\n",
        "        print(\"Training model...\")\n",
        "        model = self.train_model(X_train, train_y, data['task_type'])\n",
        "\n",
        "        # Get predictions\n",
        "        if data['task_type'] == 'classification':\n",
        "            y_pred = model.predict_proba(X_test)[:, 1]\n",
        "        else:\n",
        "            y_pred = model.predict(X_test)\n",
        "\n",
        "        # Create DALEX explainer\n",
        "        print(\"Creating DALEX explainer...\")\n",
        "        wrapper = SimpleWrapper(y_pred)\n",
        "\n",
        "        # Prepare data for explainer\n",
        "        X_test_df = pd.DataFrame(X_test, columns=train_features.columns)\n",
        "\n",
        "        # Ensure y is the correct type and shape\n",
        "        if data['task_type'] == 'classification':\n",
        "            y_for_explainer = test_y\n",
        "        else:\n",
        "            # For regression fairness, use binary version\n",
        "            y_for_explainer = test_y_binary\n",
        "\n",
        "        # Create explainer with explicit parameters\n",
        "        explainer = dx.Explainer(\n",
        "            model=wrapper,\n",
        "            data=X_test_df,\n",
        "            y=y_for_explainer,\n",
        "            model_type=data['task_type'],\n",
        "            label=f\"{dataset_name.upper()} Model\",\n",
        "            verbose=False\n",
        "        )\n",
        "\n",
        "        # Get compliance criteria\n",
        "        criteria = self.compliance_criteria.get(dataset_name)\n",
        "        if criteria:\n",
        "            compliant = criteria['check'](test_features)\n",
        "            criteria_name = criteria['name']\n",
        "        else:\n",
        "            # Default to RO5\n",
        "            compliant = (test_features['MW'] <= 500) & (test_features['LogP'] <= 5)\n",
        "            criteria_name = 'RO5_compliant'\n",
        "\n",
        "        # Create protected attribute\n",
        "        protected = pd.Series('non_compliant', index=test_features.index)\n",
        "        protected[compliant] = 'compliant'\n",
        "\n",
        "        print(f\"\\nGroup distribution for {criteria_name}:\")\n",
        "        print(protected.value_counts())\n",
        "        print(f\"Percentage: {protected.value_counts(normalize=True).round(3) * 100}\")\n",
        "\n",
        "        # Check if we have both groups for fairness analysis\n",
        "        if len(protected.unique()) < 2:\n",
        "            print(f\"\\n Warning: Only one group found for {dataset_name}. Creating artificial split for fairness analysis...\")\n",
        "\n",
        "            # Create artificial split based on median of a key feature\n",
        "            if 'MW' in test_features.columns:\n",
        "                median_mw = test_features['MW'].median()\n",
        "                protected = pd.Series('below_median_MW', index=test_features.index)\n",
        "                protected[test_features['MW'] > median_mw] = 'above_median_MW'\n",
        "                criteria_name = 'MW_median_split'\n",
        "            else:\n",
        "                # Use random split as last resort\n",
        "                mid_point = len(protected) // 2\n",
        "                protected = pd.Series('group_A', index=test_features.index)\n",
        "                protected.iloc[mid_point:] = 'group_B'\n",
        "                criteria_name = 'random_split'\n",
        "\n",
        "            print(f\"Created artificial groups based on {criteria_name}:\")\n",
        "            print(protected.value_counts())\n",
        "\n",
        "        # Generate DALEX fairness plots\n",
        "        fairness_object, fairness_results = self.create_dalex_fairness_plots(\n",
        "            explainer, protected, dataset_name, criteria_name\n",
        "        )\n",
        "\n",
        "        # Generate functional group heatmap\n",
        "        y_test_for_fg = test_y_binary if data['task_type'] == 'regression' else test_y\n",
        "        fg_metrics = self.create_functional_group_heatmap(\n",
        "            test_features, y_test_for_fg, y_pred, dataset_name\n",
        "        )\n",
        "\n",
        "        # Calculate performance metrics\n",
        "        if data['task_type'] == 'classification':\n",
        "            from sklearn.metrics import roc_auc_score\n",
        "            auc = roc_auc_score(test_y, y_pred)\n",
        "            print(f\"\\nTest AUC: {auc:.4f}\")\n",
        "        else:\n",
        "            from sklearn.metrics import mean_squared_error, r2_score\n",
        "            rmse = np.sqrt(mean_squared_error(test_y, y_pred))\n",
        "            r2 = r2_score(test_y, y_pred)\n",
        "            print(f\"\\nTest RMSE: {rmse:.4f}\")\n",
        "            print(f\"Test R: {r2:.4f}\")\n",
        "\n",
        "        return {\n",
        "            'dataset': dataset_name,\n",
        "            'fairness_object': fairness_object,\n",
        "            'fairness_results': fairness_results,\n",
        "            'functional_group_metrics': fg_metrics,\n",
        "            'task_type': data['task_type']\n",
        "        }\n",
        "\n",
        "    def run_all_analyses(self, datasets=None):\n",
        "        \"\"\"Run fairness analysis for all datasets\"\"\"\n",
        "        if datasets is None:\n",
        "            datasets = ['bace', 'bbbp', 'clintox', 'esol', 'qm9']\n",
        "\n",
        "        all_results = {}\n",
        "\n",
        "        print(f\"\\n{'='*60}\")\n",
        "        print(\"STARTING MULTI-DATASET FAIRNESS ANALYSIS\")\n",
        "        print(f\"{'='*60}\")\n",
        "        print(f\"Datasets to analyze: {datasets}\")\n",
        "\n",
        "        for dataset_name in datasets:\n",
        "            try:\n",
        "                results = self.analyze_dataset(dataset_name)\n",
        "                if results:\n",
        "                    all_results[dataset_name] = results\n",
        "                    print(f\"\\n Successfully analyzed {dataset_name}\")\n",
        "                else:\n",
        "                    print(f\"\\n Failed to analyze {dataset_name}\")\n",
        "\n",
        "            except Exception as e:\n",
        "                print(f\"\\n Error analyzing {dataset_name}: {e}\")\n",
        "                traceback.print_exc()\n",
        "                continue\n",
        "\n",
        "        # Create summary\n",
        "        self.create_summary_report(all_results)\n",
        "\n",
        "        return all_results\n",
        "\n",
        "    def create_summary_report(self, all_results):\n",
        "        \"\"\"Create summary report across all datasets\"\"\"\n",
        "        print(f\"\\n{'='*60}\")\n",
        "        print(\"FAIRNESS ANALYSIS SUMMARY\")\n",
        "        print(f\"{'='*60}\")\n",
        "\n",
        "        summary_file = os.path.join(self.output_dir, 'fairness_summary.txt')\n",
        "\n",
        "        with open(summary_file, 'w') as f:\n",
        "            f.write(\"MULTI-DATASET FAIRNESS ANALYSIS SUMMARY\\n\")\n",
        "            f.write(\"=\"*60 + \"\\n\\n\")\n",
        "\n",
        "            for dataset_name, results in all_results.items():\n",
        "                f.write(f\"\\n{dataset_name.upper()}\\n\")\n",
        "                f.write(\"-\"*30 + \"\\n\")\n",
        "                f.write(f\"Task Type: {results['task_type']}\\n\")\n",
        "\n",
        "                if results['fairness_results'] is not None:\n",
        "                    f.write(\"Fairness Metrics: Available\\n\")\n",
        "                else:\n",
        "                    f.write(\"Fairness Metrics: Failed\\n\")\n",
        "\n",
        "                if results['functional_group_metrics'] is not None:\n",
        "                    f.write(f\"Functional Groups Analyzed: {len(results['functional_group_metrics'])}\\n\")\n",
        "                else:\n",
        "                    f.write(\"Functional Groups: Not analyzed\\n\")\n",
        "\n",
        "                f.write(\"\\n\")\n",
        "\n",
        "        print(f\"\\nSummary saved to: {summary_file}\")\n",
        "\n",
        "        # Print summary to console\n",
        "        print(\"\\nDatasets Successfully Analyzed:\")\n",
        "        for dataset_name in all_results.keys():\n",
        "            print(f\"   {dataset_name.upper()}\")\n",
        "\n",
        "        print(f\"\\nAll results saved to: {self.output_dir}\")\n",
        "        print(\"\\nGenerated files for each dataset:\")\n",
        "        print(\"  - DALEX fairness plots (HTML)\")\n",
        "        print(\"  - Fairness metrics (TXT)\")\n",
        "        print(\"  - Functional group heatmap (PNG)\")\n",
        "\n",
        "# Main execution function\n",
        "def main():\n",
        "    \"\"\"Main function to run fairness analysis for all datasets\"\"\"\n",
        "\n",
        "    # Initialize analyzer\n",
        "    analyzer = MultiDatasetFairnessAnalyzer(output_dir='./DALEX-Fairness-Results')\n",
        "\n",
        "    # Define datasets to analyze\n",
        "    # datasets = ['bace', 'bbbp', 'clintox', 'esol', 'qm9']\n",
        "    datasets = ['esol']\n",
        "\n",
        "    # Run analysis for all datasets\n",
        "    results = analyzer.run_all_analyses(datasets)\n",
        "\n",
        "    print(f\"\\n{'='*60}\")\n",
        "    print(\"ALL ANALYSES COMPLETE!\")\n",
        "    print(f\"{'='*60}\")\n",
        "\n",
        "    print(\"\\nTo view results:\")\n",
        "    print(\"1. Check the DALEX-Fairness-Results folder\")\n",
        "    print(\"2. Open HTML files in a browser for interactive plots\")\n",
        "    print(\"3. View PNG files for static visualizations\")\n",
        "    print(\"4. Read TXT files for detailed metrics\")\n",
        "\n",
        "    return results\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    results = main()"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python [torchfix]",
      "language": "python",
      "name": "torchfix"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.18"
    },
    "colab": {
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}