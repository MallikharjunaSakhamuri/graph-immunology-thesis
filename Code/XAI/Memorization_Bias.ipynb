{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "6ed9b741-9aaf-44d3-a2d0-36da534ac117",
      "metadata": {
        "id": "6ed9b741-9aaf-44d3-a2d0-36da534ac117"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from typing import Dict, List, Tuple, Optional\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "# Import molecular libraries\n",
        "from deepchem.molnet import load_bace_classification, load_bbbp, load_clintox, load_delaney, load_qm9\n",
        "from rdkit import Chem\n",
        "from rdkit.Chem import Descriptors\n",
        "from rdkit import RDLogger\n",
        "RDLogger.DisableLog('rdApp.*')\n",
        "\n",
        "# XAI and ML libraries\n",
        "import shap\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.metrics import roc_auc_score, r2_score\n",
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "from sklearn.ensemble import RandomForestClassifier, RandomForestRegressor\n",
        "\n",
        "import os\n",
        "from tqdm import tqdm\n",
        "import pickle\n",
        "from scipy import stats\n",
        "\n",
        "# Graph neural network imports\n",
        "from torch_geometric.nn import GCNConv, global_mean_pool\n",
        "from torch_geometric.data import Data\n",
        "\n",
        "# Set style for publication-quality figures\n",
        "plt.style.use('seaborn-v0_8-whitegrid')\n",
        "sns.set_palette(\"husl\")\n",
        "\n",
        "class MolecularFeatureExtractor:\n",
        "    \"\"\"Feature extractor for molecular graphs\"\"\"\n",
        "    def __init__(self):\n",
        "        self.atom_list = list(range(1, 119))\n",
        "        self.chirality_list = [\n",
        "            Chem.rdchem.ChiralType.CHI_UNSPECIFIED,\n",
        "            Chem.rdchem.ChiralType.CHI_TETRAHEDRAL_CW,\n",
        "            Chem.rdchem.ChiralType.CHI_TETRAHEDRAL_CCW,\n",
        "            Chem.rdchem.ChiralType.CHI_OTHER\n",
        "        ]\n",
        "        self.bond_list = [\n",
        "            Chem.rdchem.BondType.SINGLE,\n",
        "            Chem.rdchem.BondType.DOUBLE,\n",
        "            Chem.rdchem.BondType.TRIPLE,\n",
        "            Chem.rdchem.BondType.AROMATIC\n",
        "        ]\n",
        "\n",
        "    def calc_atom_features(self, atom: Chem.Atom) -> Tuple[list, list]:\n",
        "        try:\n",
        "            atom_feat = [\n",
        "                self.atom_list.index(atom.GetAtomicNum()),\n",
        "                self.chirality_list.index(atom.GetChiralTag())\n",
        "            ]\n",
        "            phys_feat = []\n",
        "\n",
        "            try:\n",
        "                contrib_mw = Descriptors.ExactMolWt(Chem.MolFromSmiles(f'[{atom.GetSymbol()}]'))\n",
        "                phys_feat.append(contrib_mw)\n",
        "            except:\n",
        "                phys_feat.append(0.0)\n",
        "\n",
        "            try:\n",
        "                contrib_logp = Descriptors.MolLogP(Chem.MolFromSmiles(f'[{atom.GetSymbol()}]'))\n",
        "                phys_feat.append(contrib_logp)\n",
        "            except:\n",
        "                phys_feat.append(0.0)\n",
        "\n",
        "            phys_feat.extend([\n",
        "                atom.GetFormalCharge(),\n",
        "                int(atom.GetHybridization()),\n",
        "                int(atom.GetIsAromatic()),\n",
        "                atom.GetTotalNumHs(),\n",
        "                atom.GetTotalValence(),\n",
        "                atom.GetDegree()\n",
        "            ])\n",
        "\n",
        "            return atom_feat, phys_feat\n",
        "\n",
        "        except:\n",
        "            return [0, 0], [0.0] * 9\n",
        "\n",
        "    def get_atom_features(self, mol: Chem.Mol) -> Tuple[torch.Tensor, torch.Tensor]:\n",
        "        atom_feats = []\n",
        "        phys_feats = []\n",
        "\n",
        "        if mol is None:\n",
        "            return torch.tensor([[0, 0]], dtype=torch.long), torch.tensor([[0.0] * 9], dtype=torch.float)\n",
        "\n",
        "        for atom in mol.GetAtoms():\n",
        "            atom_feat, phys_feat = self.calc_atom_features(atom)\n",
        "            atom_feats.append(atom_feat)\n",
        "            phys_feats.append(phys_feat)\n",
        "\n",
        "        x = torch.tensor(atom_feats, dtype=torch.long)\n",
        "        phys = torch.tensor(phys_feats, dtype=torch.float)\n",
        "\n",
        "        return x, phys\n",
        "\n",
        "    def get_bond_features(self, mol: Chem.Mol) -> Tuple[torch.Tensor, torch.Tensor]:\n",
        "        if mol is None:\n",
        "            return torch.tensor([[0], [0]], dtype=torch.long), torch.tensor([[0.0] * 5], dtype=torch.float)\n",
        "\n",
        "        row, col, edge_feat = [], [], []\n",
        "\n",
        "        for bond in mol.GetBonds():\n",
        "            try:\n",
        "                start, end = bond.GetBeginAtomIdx(), bond.GetEndAtomIdx()\n",
        "                row += [start, end]\n",
        "                col += [end, start]\n",
        "\n",
        "                bond_type = self.bond_list.index(bond.GetBondType())\n",
        "                feat = [bond_type, 0, int(bond.GetIsConjugated()), 0, 0]\n",
        "                edge_feat.extend([feat, feat])\n",
        "\n",
        "            except:\n",
        "                continue\n",
        "\n",
        "        if not row:\n",
        "            return torch.tensor([[0], [0]], dtype=torch.long), torch.tensor([[0.0] * 5], dtype=torch.float)\n",
        "\n",
        "        edge_index = torch.tensor([row, col], dtype=torch.long)\n",
        "        edge_attr = torch.tensor(edge_feat, dtype=torch.float)\n",
        "\n",
        "        return edge_index, edge_attr\n",
        "\n",
        "    def process_molecule(self, smiles: str) -> Data:\n",
        "        try:\n",
        "            mol = Chem.MolFromSmiles(smiles)\n",
        "            if mol is None:\n",
        "                return None\n",
        "\n",
        "            Chem.SanitizeMol(mol)\n",
        "            if mol.GetNumAtoms() == 0:\n",
        "                return None\n",
        "\n",
        "            x_cat, x_phys = self.get_atom_features(mol)\n",
        "            edge_index, edge_attr = self.get_bond_features(mol)\n",
        "\n",
        "            return Data(\n",
        "                x_cat=x_cat,\n",
        "                x_phys=x_phys,\n",
        "                edge_index=edge_index,\n",
        "                edge_attr=edge_attr,\n",
        "                num_nodes=x_cat.size(0)\n",
        "            )\n",
        "\n",
        "        except:\n",
        "            return None\n",
        "\n",
        "class GraphDiscriminator(nn.Module):\n",
        "    \"\"\"Encoder network for molecular graphs\"\"\"\n",
        "    def __init__(self, node_dim: int, edge_dim: int, hidden_dim: int = 128, output_dim: int = 128):\n",
        "        super().__init__()\n",
        "\n",
        "        self.node_encoder = nn.Sequential(\n",
        "            nn.Linear(node_dim, hidden_dim),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(hidden_dim, hidden_dim)\n",
        "        )\n",
        "\n",
        "        self.edge_encoder = nn.Sequential(\n",
        "            nn.Linear(edge_dim, hidden_dim),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(hidden_dim, hidden_dim)\n",
        "        )\n",
        "\n",
        "        self.conv1 = GCNConv(hidden_dim, hidden_dim)\n",
        "        self.conv2 = GCNConv(hidden_dim, hidden_dim)\n",
        "        self.conv3 = GCNConv(hidden_dim, output_dim)\n",
        "\n",
        "        self.projection = nn.Sequential(\n",
        "            nn.Linear(output_dim, hidden_dim),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(hidden_dim, output_dim)\n",
        "        )\n",
        "\n",
        "    def normalize_features(self, x_cat, x_phys):\n",
        "        x_cat = x_cat.float()\n",
        "        x_phys = x_phys.float()\n",
        "        if x_phys.size(0) > 1:\n",
        "            x_phys = (x_phys - x_phys.mean(0)) / (x_phys.std(0) + 1e-5)\n",
        "        return x_cat, x_phys\n",
        "\n",
        "    def forward(self, data):\n",
        "        x_cat, x_phys = self.normalize_features(data.x_cat, data.x_phys)\n",
        "        x = torch.cat([x_cat, x_phys], dim=-1)\n",
        "\n",
        "        edge_index = data.edge_index\n",
        "        batch = data.batch\n",
        "\n",
        "        x = self.node_encoder(x)\n",
        "\n",
        "        x = F.relu(self.conv1(x, edge_index))\n",
        "        x = F.relu(self.conv2(x, edge_index))\n",
        "        x = self.conv3(x, edge_index)\n",
        "\n",
        "        x = global_mean_pool(x, batch)\n",
        "        x = self.projection(x)\n",
        "\n",
        "        return x\n",
        "\n",
        "class MemorizationBiasSHAPAnalyzer:\n",
        "    \"\"\"Generate SHAP visualizations for memorization bias\"\"\"\n",
        "\n",
        "    def __init__(self, encoder_path: str, output_dir: str = './Memorization-SHAP'):\n",
        "        self.encoder_path = encoder_path\n",
        "        self.output_dir = output_dir\n",
        "        self.device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "\n",
        "        os.makedirs(output_dir, exist_ok=True)\n",
        "\n",
        "        # Load encoder\n",
        "        self.encoder, self.model_info = self.load_encoder()\n",
        "        self.extractor = MolecularFeatureExtractor()\n",
        "\n",
        "        # Molecular descriptors\n",
        "        self.descriptor_names = [\n",
        "            'MolWt', 'LogP', 'TPSA', 'NumRotatableBonds',\n",
        "            'NumHDonors', 'NumHAcceptors', 'NumAromaticRings',\n",
        "            'NumSaturatedRings', 'NumAliphaticRings', 'BertzCT'\n",
        "        ]\n",
        "\n",
        "    def load_encoder(self):\n",
        "        checkpoint = torch.load(self.encoder_path, map_location=self.device)\n",
        "        encoder = GraphDiscriminator(\n",
        "            node_dim=checkpoint['model_info']['node_dim'],\n",
        "            edge_dim=checkpoint['model_info']['edge_dim'],\n",
        "            hidden_dim=checkpoint['model_info'].get('hidden_dim', 128),\n",
        "            output_dim=checkpoint['model_info'].get('output_dim', 128)\n",
        "        )\n",
        "        encoder.load_state_dict(checkpoint['encoder_state_dict'])\n",
        "        encoder.to(self.device)\n",
        "        encoder.eval()\n",
        "        return encoder, checkpoint['model_info']\n",
        "\n",
        "    def compute_molecular_descriptors(self, smiles_list):\n",
        "        \"\"\"Compute molecular descriptors\"\"\"\n",
        "        descriptors = []\n",
        "        for smiles in smiles_list:\n",
        "            mol = Chem.MolFromSmiles(smiles)\n",
        "            if mol is None:\n",
        "                descriptors.append([0] * len(self.descriptor_names))\n",
        "                continue\n",
        "\n",
        "            desc = [\n",
        "                Descriptors.MolWt(mol),\n",
        "                Descriptors.MolLogP(mol),\n",
        "                Descriptors.TPSA(mol),\n",
        "                Descriptors.NumRotatableBonds(mol),\n",
        "                Descriptors.NumHDonors(mol),\n",
        "                Descriptors.NumHAcceptors(mol),\n",
        "                Descriptors.NumAromaticRings(mol),\n",
        "                Descriptors.NumSaturatedRings(mol),\n",
        "                Descriptors.NumAliphaticRings(mol),\n",
        "                Descriptors.BertzCT(mol)\n",
        "            ]\n",
        "            descriptors.append(desc)\n",
        "\n",
        "        return np.array(descriptors)\n",
        "\n",
        "    def compute_tanimoto_similarity(self, test_smiles, train_smiles, k=5):\n",
        "        \"\"\"Compute Tanimoto similarity using Morgan fingerprints\"\"\"\n",
        "        from rdkit import DataStructs\n",
        "        from rdkit.Chem import AllChem\n",
        "\n",
        "        print(\"Computing Tanimoto similarities...\")\n",
        "\n",
        "        # Generate fingerprints for all molecules\n",
        "        test_fps = []\n",
        "        for smiles in test_smiles:\n",
        "            mol = Chem.MolFromSmiles(smiles)\n",
        "            if mol:\n",
        "                fp = AllChem.GetMorganFingerprintAsBitVect(mol, 2, nBits=2048)\n",
        "                test_fps.append(fp)\n",
        "            else:\n",
        "                test_fps.append(None)\n",
        "\n",
        "        train_fps = []\n",
        "        for smiles in train_smiles:\n",
        "            mol = Chem.MolFromSmiles(smiles)\n",
        "            if mol:\n",
        "                fp = AllChem.GetMorganFingerprintAsBitVect(mol, 2, nBits=2048)\n",
        "                train_fps.append(fp)\n",
        "            else:\n",
        "                train_fps.append(None)\n",
        "\n",
        "        # Compute similarities\n",
        "        nearest_k_similarities = []\n",
        "        for test_fp in test_fps:\n",
        "            if test_fp is None:\n",
        "                nearest_k_similarities.append(0.5)  # Default for invalid molecules\n",
        "                continue\n",
        "\n",
        "            similarities = []\n",
        "            for train_fp in train_fps:\n",
        "                if train_fp is not None:\n",
        "                    sim = DataStructs.TanimotoSimilarity(test_fp, train_fp)\n",
        "                    similarities.append(sim)\n",
        "\n",
        "            if similarities:\n",
        "                top_k = sorted(similarities, reverse=True)[:k]\n",
        "                nearest_k_similarities.append(np.mean(top_k))\n",
        "            else:\n",
        "                nearest_k_similarities.append(0.5)\n",
        "\n",
        "        return np.array(nearest_k_similarities)\n",
        "\n",
        "    def extract_embeddings(self, smiles_list):\n",
        "        \"\"\"Extract embeddings\"\"\"\n",
        "        embeddings = []\n",
        "        valid_indices = []\n",
        "\n",
        "        for idx, smiles in enumerate(tqdm(smiles_list, desc=\"Extracting embeddings\")):\n",
        "            try:\n",
        "                data = self.extractor.process_molecule(smiles)\n",
        "                if data is not None:\n",
        "                    data.batch = torch.zeros(data.x_cat.size(0), dtype=torch.long)\n",
        "                    data = data.to(self.device)\n",
        "\n",
        "                    with torch.no_grad():\n",
        "                        emb = self.encoder(data)\n",
        "                        embeddings.append(emb.cpu().numpy().squeeze())\n",
        "                        valid_indices.append(idx)\n",
        "            except:\n",
        "                continue\n",
        "\n",
        "        if embeddings:\n",
        "            return np.vstack(embeddings), valid_indices\n",
        "        return np.empty((0, 128)), []\n",
        "\n",
        "    def compute_similarity_scores(self, test_embeddings, train_embeddings, k=5):\n",
        "        \"\"\"Compute similarity scores\"\"\"\n",
        "        similarities = cosine_similarity(test_embeddings, train_embeddings)\n",
        "        nearest_k_similarities = []\n",
        "        for sim_row in similarities:\n",
        "            top_k = np.sort(sim_row)[-k:]\n",
        "            nearest_k_similarities.append(np.mean(top_k))\n",
        "\n",
        "        return np.array(nearest_k_similarities)\n",
        "\n",
        "    def categorize_by_similarity(self, similarity_scores):\n",
        "        \"\"\"Categorize into quartiles\"\"\"\n",
        "        quartiles = np.percentile(similarity_scores, [25, 50, 75])\n",
        "\n",
        "        categories = np.zeros(len(similarity_scores), dtype=int)\n",
        "        categories[similarity_scores <= quartiles[0]] = 0  # Q1: Novel\n",
        "        categories[(similarity_scores > quartiles[0]) & (similarity_scores <= quartiles[1])] = 1\n",
        "        categories[(similarity_scores > quartiles[1]) & (similarity_scores <= quartiles[2])] = 2\n",
        "        categories[similarity_scores > quartiles[2]] = 3  # Q4: Similar\n",
        "\n",
        "        return categories, quartiles\n",
        "\n",
        "    def visualization_1_split_summary(self, shap_values, X_test, categories, dataset_name):\n",
        "        \"\"\"Option 1: SHAP Summary Plot Split by Similarity with fixed feature order\"\"\"\n",
        "        fig = plt.figure(figsize=(16, 8))\n",
        "\n",
        "        # Create grid with space for colorbar\n",
        "        gs = fig.add_gridspec(1, 3, width_ratios=[1, 1, 0.05], wspace=0.3)\n",
        "        ax1 = fig.add_subplot(gs[0, 0])\n",
        "        ax2 = fig.add_subplot(gs[0, 1])\n",
        "        cbar_ax = fig.add_subplot(gs[0, 2])\n",
        "\n",
        "        axes = [ax1, ax2]\n",
        "\n",
        "        # Define fixed feature order\n",
        "        feature_order = list(range(len(self.descriptor_names)))\n",
        "\n",
        "        # Novel samples (Q1) and Similar samples (Q4)\n",
        "        novel_mask = categories == 0\n",
        "        similar_mask = categories == 3\n",
        "\n",
        "        for ax_idx, (mask, title) in enumerate([(novel_mask, 'Novel Samples (Low Similarity to Training)'),\n",
        "                                                  (similar_mask, 'Similar Samples (High Similarity to Training)')]):\n",
        "            if sum(mask) > 0:\n",
        "                ax = axes[ax_idx]\n",
        "\n",
        "                # Get data for this subset\n",
        "                shap_subset = shap_values[mask]\n",
        "                X_subset = X_test[mask]\n",
        "\n",
        "                # Create custom summary plot with fixed order\n",
        "                for i, feat_idx in enumerate(feature_order):\n",
        "                    # Get SHAP and feature values for this feature\n",
        "                    shap_vals = shap_subset[:, feat_idx]\n",
        "                    feat_vals = X_subset[:, feat_idx]\n",
        "\n",
        "                    # Normalize feature values for color mapping\n",
        "                    feat_normalized = (feat_vals - feat_vals.min()) / (feat_vals.max() - feat_vals.min() + 1e-10)\n",
        "\n",
        "                    # Create scatter plot\n",
        "                    scatter = ax.scatter(shap_vals, [i]*len(shap_vals),\n",
        "                                       c=feat_normalized, cmap='coolwarm',\n",
        "                                       alpha=0.6, s=20, vmin=0, vmax=1)\n",
        "\n",
        "                ax.set_yticks(range(len(self.descriptor_names)))\n",
        "                ax.set_yticklabels(self.descriptor_names)\n",
        "                ax.set_xlabel('SHAP value (impact on model output)', fontsize=11)\n",
        "                ax.set_ylabel('Feature value', fontsize=11)\n",
        "                ax.set_title(title, fontsize=12, fontweight='bold')\n",
        "                ax.grid(True, alpha=0.3)\n",
        "                ax.axvline(0, color='black', linestyle='-', linewidth=0.5)\n",
        "\n",
        "        # Add colorbar in dedicated axis\n",
        "        sm = plt.cm.ScalarMappable(cmap='coolwarm', norm=plt.Normalize(vmin=0, vmax=1))\n",
        "        sm.set_array([])\n",
        "        cbar = plt.colorbar(sm, cax=cbar_ax)\n",
        "        cbar.set_label('Feature value', rotation=270, labelpad=15)\n",
        "        cbar.set_ticks([0, 0.5, 1])\n",
        "        cbar.set_ticklabels(['Low', '', 'High'])\n",
        "\n",
        "        plt.suptitle(f'{dataset_name.upper()} - SHAP Patterns: Novel vs Similar Samples\\n'\n",
        "                    'Scattered patterns (left) indicate uncertainty; Concentrated patterns (right) indicate memorization',\n",
        "                    fontsize=16, fontweight='bold')\n",
        "\n",
        "        save_path = os.path.join(self.output_dir, f'{dataset_name}_1_split_summary.png')\n",
        "        plt.savefig(save_path, dpi=300, bbox_inches='tight')\n",
        "        plt.close()\n",
        "        print(f\"Saved: {save_path}\")\n",
        "\n",
        "    def visualization_2_variance_analysis(self, shap_values, similarity_scores, dataset_name):\n",
        "        \"\"\"Option 2: SHAP Value Variance vs Similarity\"\"\"\n",
        "        fig, ax = plt.subplots(figsize=(10, 6))\n",
        "\n",
        "        # Handle different SHAP value formats\n",
        "        if len(shap_values.shape) == 3:\n",
        "            # For binary classification: shape (n_samples, n_features, n_classes)\n",
        "            # Use the positive class (index 1) or take mean across classes\n",
        "            shap_values = shap_values[:, :, 1] if shap_values.shape[2] == 2 else shap_values.mean(axis=2)\n",
        "        elif len(shap_values.shape) == 1:\n",
        "            # If 1D, reshape to 2D\n",
        "            shap_values = shap_values.reshape(-1, 1)\n",
        "\n",
        "        # Calculate variance of SHAP values for each sample\n",
        "        shap_variance = np.var(shap_values, axis=1)\n",
        "\n",
        "        # Scatter plot with trend line\n",
        "        ax.scatter(similarity_scores, shap_variance, alpha=0.5, s=30)\n",
        "\n",
        "        # Add trend line\n",
        "        z = np.polyfit(similarity_scores, shap_variance, 1)\n",
        "        p = np.poly1d(z)\n",
        "        x_trend = np.linspace(similarity_scores.min(), similarity_scores.max(), 100)\n",
        "        ax.plot(x_trend, p(x_trend), \"r-\", linewidth=2, label=f'Trend (slope={z[0]:.2f})')\n",
        "\n",
        "        # Add quartile boundaries\n",
        "        quartiles = np.percentile(similarity_scores, [25, 50, 75])\n",
        "        for q, label in zip(quartiles, ['Q1/Q2', 'Q2/Q3', 'Q3/Q4']):\n",
        "            ax.axvline(q, color='gray', linestyle='--', alpha=0.5, label=label)\n",
        "\n",
        "        ax.set_xlabel('Similarity to Training Set', fontsize=12, fontweight='bold')\n",
        "        ax.set_ylabel('SHAP Value Variance', fontsize=12, fontweight='bold')\n",
        "        ax.set_title(f'{dataset_name.upper()} - Model Uncertainty vs Sample Novelty\\n'\n",
        "                    'High variance indicates uncertain feature importance',\n",
        "                    fontsize=14, fontweight='bold')\n",
        "        ax.legend()\n",
        "        ax.grid(True, alpha=0.3)\n",
        "\n",
        "        save_path = os.path.join(self.output_dir, f'{dataset_name}_2_variance_analysis.png')\n",
        "        plt.savefig(save_path, dpi=300, bbox_inches='tight')\n",
        "        plt.close()\n",
        "        print(f\"Saved: {save_path}\")\n",
        "\n",
        "    def visualization_3_cohesion_score(self, shap_values, X_test, similarity_scores, dataset_name):\n",
        "        \"\"\"Option 3: SHAP Cohesion Score\"\"\"\n",
        "        fig, ax = plt.subplots(figsize=(10, 6))\n",
        "\n",
        "        # Calculate cohesion: alignment between SHAP importance and actual feature magnitude\n",
        "        cohesion_scores = []\n",
        "        for i in range(len(shap_values)):\n",
        "            # Normalize features and SHAP values\n",
        "            feat_norm = np.abs(X_test[i]) / (np.abs(X_test[i]).sum() + 1e-10)\n",
        "            shap_norm = np.abs(shap_values[i]) / (np.abs(shap_values[i]).sum() + 1e-10)\n",
        "            # Cohesion as correlation between feature magnitude and SHAP importance\n",
        "            cohesion = np.corrcoef(feat_norm, shap_norm)[0, 1]\n",
        "            cohesion_scores.append(cohesion if not np.isnan(cohesion) else 0)\n",
        "\n",
        "        cohesion_scores = np.array(cohesion_scores)\n",
        "\n",
        "        # Plot with color gradient\n",
        "        scatter = ax.scatter(similarity_scores, cohesion_scores,\n",
        "                           c=similarity_scores, cmap='RdYlBu',\n",
        "                           alpha=0.6, s=50, edgecolors='black', linewidth=0.5)\n",
        "\n",
        "        # Add trend line\n",
        "        mask = ~np.isnan(cohesion_scores)\n",
        "        if sum(mask) > 1:\n",
        "            z = np.polyfit(similarity_scores[mask], cohesion_scores[mask], 1)\n",
        "            p = np.poly1d(z)\n",
        "            x_trend = np.linspace(similarity_scores.min(), similarity_scores.max(), 100)\n",
        "            ax.plot(x_trend, p(x_trend), \"k-\", linewidth=2,\n",
        "                   label=f'Trend (correlation={np.corrcoef(similarity_scores[mask], cohesion_scores[mask])[0,1]:.3f})')\n",
        "\n",
        "        plt.colorbar(scatter, label='Similarity Score')\n",
        "\n",
        "        ax.set_xlabel('Similarity to Training Set', fontsize=12, fontweight='bold')\n",
        "        ax.set_ylabel('SHAP Cohesion Score', fontsize=12, fontweight='bold')\n",
        "        ax.set_title(f'{dataset_name.upper()} - Explanation Coherence vs Sample Familiarity\\n'\n",
        "                    'Higher cohesion means model explanations align with feature magnitudes',\n",
        "                    fontsize=14, fontweight='bold')\n",
        "        ax.legend()\n",
        "        ax.grid(True, alpha=0.3)\n",
        "\n",
        "        save_path = os.path.join(self.output_dir, f'{dataset_name}_3_cohesion_score.png')\n",
        "        plt.savefig(save_path, dpi=300, bbox_inches='tight')\n",
        "        plt.close()\n",
        "        print(f\"Saved: {save_path}\")\n",
        "\n",
        "    def visualization_4_interaction_heatmap(self, shap_values, categories, dataset_name):\n",
        "        \"\"\"Option 4: SHAP Feature Interaction Patterns\"\"\"\n",
        "        fig, axes = plt.subplots(1, 2, figsize=(14, 6))\n",
        "\n",
        "        # Calculate feature correlation matrices for novel vs similar\n",
        "        novel_mask = categories == 0\n",
        "        similar_mask = categories == 3\n",
        "\n",
        "        if sum(novel_mask) > 5 and sum(similar_mask) > 5:\n",
        "            # Novel samples correlation\n",
        "            novel_shap = shap_values[novel_mask]\n",
        "            novel_corr = np.corrcoef(novel_shap.T)\n",
        "\n",
        "            # Similar samples correlation\n",
        "            similar_shap = shap_values[similar_mask]\n",
        "            similar_corr = np.corrcoef(similar_shap.T)\n",
        "\n",
        "            # Plot heatmaps\n",
        "            sns.heatmap(novel_corr, ax=axes[0], cmap='coolwarm', center=0,\n",
        "                       xticklabels=self.descriptor_names,\n",
        "                       yticklabels=self.descriptor_names,\n",
        "                       cbar_kws={'label': 'Correlation'})\n",
        "            axes[0].set_title('Novel Samples\\nWeak/Inconsistent Interactions', fontsize=12, fontweight='bold')\n",
        "\n",
        "            sns.heatmap(similar_corr, ax=axes[1], cmap='coolwarm', center=0,\n",
        "                       xticklabels=self.descriptor_names,\n",
        "                       yticklabels=self.descriptor_names,\n",
        "                       cbar_kws={'label': 'Correlation'})\n",
        "            axes[1].set_title('Similar Samples\\nStrong/Memorized Interactions', fontsize=12, fontweight='bold')\n",
        "\n",
        "            # Rotate labels\n",
        "            for ax in axes:\n",
        "                ax.set_xticklabels(ax.get_xticklabels(), rotation=45, ha='right')\n",
        "                ax.set_yticklabels(ax.get_yticklabels(), rotation=0)\n",
        "\n",
        "        plt.suptitle(f'{dataset_name.upper()} - Feature Interaction Patterns\\n'\n",
        "                    'Stronger correlations in similar samples indicate memorized feature combinations',\n",
        "                    fontsize=14, fontweight='bold')\n",
        "        plt.tight_layout()\n",
        "\n",
        "        save_path = os.path.join(self.output_dir, f'{dataset_name}_4_interaction_heatmap.png')\n",
        "        plt.savefig(save_path, dpi=300, bbox_inches='tight')\n",
        "        plt.close()\n",
        "        print(f\"Saved: {save_path}\")\n",
        "\n",
        "    def load_dataset(self, dataset_name):\n",
        "        \"\"\"Load dataset\"\"\"\n",
        "        print(f\"\\nLoading {dataset_name.upper()} dataset...\")\n",
        "\n",
        "        try:\n",
        "            if dataset_name == 'bace':\n",
        "                tasks, datasets, _ = load_bace_classification(featurizer='ECFP', splitter='scaffold')\n",
        "                task_type = 'classification'\n",
        "            elif dataset_name == 'bbbp':\n",
        "                tasks, datasets, _ = load_bbbp(featurizer='ECFP', splitter='scaffold')\n",
        "                task_type = 'classification'\n",
        "            elif dataset_name == 'clintox':\n",
        "                tasks, datasets, _ = load_clintox(featurizer='ECFP', splitter='random')\n",
        "                task_type = 'classification'\n",
        "            elif dataset_name == 'esol':\n",
        "                try:\n",
        "                    tasks, datasets, _ = load_delaney(featurizer='ECFP', splitter='scaffold')\n",
        "                except:\n",
        "                    tasks, datasets, _ = load_delaney(featurizer='ECFP', splitter='random')\n",
        "                task_type = 'regression'\n",
        "            elif dataset_name == 'qm9':\n",
        "                tasks, datasets, _ = load_qm9(featurizer='ECFP', splitter='random')\n",
        "                task_type = 'regression'\n",
        "            else:\n",
        "                raise ValueError(f\"Unknown dataset: {dataset_name}\")\n",
        "\n",
        "            train_dataset, _, test_dataset = datasets\n",
        "\n",
        "            train_smiles = train_dataset.ids.tolist()\n",
        "            test_smiles = test_dataset.ids.tolist()\n",
        "\n",
        "            if len(train_dataset.y.shape) > 1:\n",
        "                train_y = train_dataset.y[:, 0].flatten()\n",
        "                test_y = test_dataset.y[:, 0].flatten()\n",
        "            else:\n",
        "                train_y = train_dataset.y.flatten()\n",
        "                test_y = test_dataset.y.flatten()\n",
        "\n",
        "            return {\n",
        "                'train_smiles': train_smiles,\n",
        "                'test_smiles': test_smiles,\n",
        "                'train_y': train_y,\n",
        "                'test_y': test_y,\n",
        "                'task_type': task_type\n",
        "            }\n",
        "        except Exception as e:\n",
        "            print(f\"Error loading {dataset_name}: {e}\")\n",
        "            return None\n",
        "\n",
        "    def analyze_dataset(self, dataset_name):\n",
        "        \"\"\"Generate all SHAP visualizations for a dataset\"\"\"\n",
        "        print(f\"\\n{'='*60}\")\n",
        "        print(f\"Generating SHAP Visualizations for {dataset_name.upper()}\")\n",
        "        print(f\"{'='*60}\")\n",
        "\n",
        "        # Load dataset\n",
        "        data = self.load_dataset(dataset_name)\n",
        "        if data is None:\n",
        "            return None\n",
        "\n",
        "        # Extract embeddings for filtering only\n",
        "        print(\"\\nExtracting embeddings for validation...\")\n",
        "        train_embeddings, train_valid_idx = self.extract_embeddings(data['train_smiles'])\n",
        "        test_embeddings, test_valid_idx = self.extract_embeddings(data['test_smiles'])\n",
        "\n",
        "        # Filter data\n",
        "        train_y = data['train_y'][train_valid_idx]\n",
        "        test_y = data['test_y'][test_valid_idx]\n",
        "        train_smiles_valid = [data['train_smiles'][i] for i in train_valid_idx]\n",
        "        test_smiles_valid = [data['test_smiles'][i] for i in test_valid_idx]\n",
        "\n",
        "        # Remove NaN\n",
        "        train_mask = ~np.isnan(train_y)\n",
        "        test_mask = ~np.isnan(test_y)\n",
        "\n",
        "        train_y = train_y[train_mask]\n",
        "        test_y = test_y[test_mask]\n",
        "        train_smiles_valid = [s for s, m in zip(train_smiles_valid, train_mask) if m]\n",
        "        test_smiles_valid = [s for s, m in zip(test_smiles_valid, test_mask) if m]\n",
        "\n",
        "        print(f\"Valid samples - Train: {len(train_y)}, Test: {len(test_y)}\")\n",
        "\n",
        "        # Compute molecular descriptors\n",
        "        print(\"\\nComputing molecular descriptors...\")\n",
        "        test_descriptors = self.compute_molecular_descriptors(test_smiles_valid)\n",
        "\n",
        "        # Ensure all arrays have same length by creating a common mask\n",
        "        # valid_desc_mask = np.all(test_descriptors != 0, axis=1)  # Remove zero descriptor rows\n",
        "        valid_desc_mask = np.sum(test_descriptors == 0, axis=1) < 5\n",
        "\n",
        "        # Apply mask to all arrays\n",
        "        test_descriptors = test_descriptors[valid_desc_mask]\n",
        "        test_y = test_y[valid_desc_mask]\n",
        "        test_smiles_valid = [s for i, s in enumerate(test_smiles_valid) if valid_desc_mask[i]]\n",
        "\n",
        "        print(f\"After descriptor validation: {len(test_y)} samples\")\n",
        "\n",
        "        # Standardize\n",
        "        scaler = StandardScaler()\n",
        "        test_descriptors_scaled = scaler.fit_transform(test_descriptors)\n",
        "\n",
        "        # Compute Tanimoto similarity instead of cosine\n",
        "        print(\"\\nComputing Tanimoto similarity scores...\")\n",
        "        similarity_scores = self.compute_tanimoto_similarity(test_smiles_valid, train_smiles_valid, k=5)\n",
        "\n",
        "        print(f\"Similarity range: {similarity_scores.min():.3f} to {similarity_scores.max():.3f}\")\n",
        "        print(f\"Mean similarity: {similarity_scores.mean():.3f} Â± {similarity_scores.std():.3f}\")\n",
        "\n",
        "        # Categorize by similarity\n",
        "        categories, quartiles = self.categorize_by_similarity(similarity_scores)\n",
        "        print(f\"Similarity quartiles: {[f'{q:.3f}' for q in quartiles]}\")\n",
        "        print(f\"Samples per quartile: {[sum(categories == q) for q in range(4)]}\")\n",
        "\n",
        "        # Train interpretable model for SHAP\n",
        "        print(\"\\nTraining interpretable model...\")\n",
        "        if data['task_type'] == 'classification':\n",
        "            model = RandomForestClassifier(n_estimators=100, max_depth=10, random_state=42)\n",
        "        else:\n",
        "            model = RandomForestRegressor(n_estimators=100, max_depth=10, random_state=42)\n",
        "\n",
        "        model.fit(test_descriptors_scaled, test_y)\n",
        "\n",
        "        # Compute SHAP values\n",
        "        print(\"\\nComputing SHAP values...\")\n",
        "        explainer = shap.TreeExplainer(model)\n",
        "        shap_values = explainer.shap_values(test_descriptors_scaled)\n",
        "\n",
        "        # Normalize SHAP values format\n",
        "        if isinstance(shap_values, list):\n",
        "            # For multiclass, take the positive class\n",
        "            shap_values = shap_values[1] if len(shap_values) == 2 else np.array(shap_values).mean(axis=0)\n",
        "        elif len(shap_values.shape) == 3:\n",
        "            # For binary classification with 3D array\n",
        "            shap_values = shap_values[:, :, 1] if shap_values.shape[2] == 2 else shap_values.mean(axis=2)\n",
        "\n",
        "        # Ensure 2D shape\n",
        "        if len(shap_values.shape) == 1:\n",
        "            shap_values = shap_values.reshape(-1, 1)\n",
        "\n",
        "        print(f\"Final SHAP shape: {shap_values.shape}\")\n",
        "\n",
        "        # Generate all visualizations\n",
        "        print(\"\\nGenerating visualizations...\")\n",
        "        self.visualization_1_split_summary(shap_values, test_descriptors_scaled, categories, dataset_name)\n",
        "        self.visualization_2_variance_analysis(shap_values, similarity_scores, dataset_name)\n",
        "        self.visualization_3_cohesion_score(shap_values, test_descriptors_scaled, similarity_scores, dataset_name)\n",
        "        self.visualization_4_interaction_heatmap(shap_values, categories, dataset_name)\n",
        "\n",
        "        print(f\"\\nAll visualizations saved for {dataset_name}\")\n",
        "\n",
        "        return {\n",
        "            'dataset': dataset_name,\n",
        "            'shap_values': shap_values,\n",
        "            'similarity_scores': similarity_scores,\n",
        "            'categories': categories\n",
        "        }\n",
        "\n",
        "    def run_all_analyses(self):\n",
        "        \"\"\"Run analysis for all datasets\"\"\"\n",
        "        # datasets = ['bace', 'bbbp', 'clintox', 'esol', 'qm9']\n",
        "        datasets = ['esol']\n",
        "        all_results = {}\n",
        "\n",
        "        for dataset_name in datasets:\n",
        "            try:\n",
        "                results = self.analyze_dataset(dataset_name)\n",
        "                if results:\n",
        "                    all_results[dataset_name] = results\n",
        "            except Exception as e:\n",
        "                print(f\"Error analyzing {dataset_name}: {e}\")\n",
        "                import traceback\n",
        "                traceback.print_exc()\n",
        "                continue\n",
        "\n",
        "        print(f\"\\n{'='*60}\")\n",
        "        print(\"ALL SHAP VISUALIZATIONS COMPLETE\")\n",
        "        print(f\"{'='*60}\")\n",
        "\n",
        "        return all_results\n",
        "\n",
        "# Main execution\n",
        "if __name__ == \"__main__\":\n",
        "    encoder_path = './checkpoints/encoders/final_encoder_20250815_125248.pt'\n",
        "    output_dir = './Memorization-SHAP'\n",
        "\n",
        "    analyzer = MemorizationBiasSHAPAnalyzer(encoder_path, output_dir)\n",
        "    results = analyzer.run_all_analyses()\n",
        "\n",
        "    print(f\"\\nAll SHAP visualizations saved to: {output_dir}\")"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python [torchfix]",
      "language": "python",
      "name": "torchfix"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.18"
    },
    "colab": {
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}