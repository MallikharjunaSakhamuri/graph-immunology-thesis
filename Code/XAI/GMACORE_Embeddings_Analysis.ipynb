{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "5379f427",
      "metadata": {
        "id": "5379f427"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import numpy as np\n",
        "import random\n",
        "import os\n",
        "import json\n",
        "import pickle\n",
        "import copy\n",
        "import pandas as pd\n",
        "from tqdm import tqdm\n",
        "from datetime import datetime\n",
        "from typing import Tuple, List, Dict, Any, Optional\n",
        "from collections import defaultdict\n",
        "from dataclasses import dataclass\n",
        "\n",
        "from rdkit import Chem, RDLogger\n",
        "from rdkit.Chem import (\n",
        "    AllChem, Descriptors, MolSurf, Fragments, Lipinski, RemoveHs\n",
        ")\n",
        "from rdkit.Chem.rdMolDescriptors import (\n",
        "    CalcNumRings, CalcNumAromaticRings, CalcNumHeterocycles,\n",
        "    CalcNumAliphaticRings, CalcTPSA\n",
        ")\n",
        "\n",
        "from torch_geometric.data import Data, DataLoader\n",
        "from torch_geometric.nn import GCNConv, global_mean_pool, MessagePassing\n",
        "\n",
        "# Suppress RDKit warnings\n",
        "RDLogger.DisableLog('rdApp.warning')\n",
        "\n",
        "# Working timestamp\n",
        "timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')\n",
        "\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import torch\n",
        "from rdkit import Chem\n",
        "from rdkit.Chem import Descriptors, Fragments, Lipinski\n",
        "from rdkit.Chem.rdMolDescriptors import CalcNumRings, CalcNumAromaticRings, CalcNumHeterocycles\n",
        "from rdkit.Chem.rdMolDescriptors import CalcNumAliphaticRings, CalcTPSA\n",
        "from typing import List, Dict, Any, Tuple\n",
        "import os\n",
        "import json\n",
        "import pickle\n",
        "from tqdm import tqdm\n",
        "from datetime import datetime\n",
        "from collections import defaultdict\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "8de17bc6",
      "metadata": {
        "id": "8de17bc6"
      },
      "outputs": [],
      "source": [
        "class MolecularFeatureExtractor:\n",
        "    def __init__(self):\n",
        "        self.atom_list = list(range(1, 119))\n",
        "        self.chirality_list = [\n",
        "            Chem.rdchem.ChiralType.CHI_UNSPECIFIED,\n",
        "            Chem.rdchem.ChiralType.CHI_TETRAHEDRAL_CW,\n",
        "            Chem.rdchem.ChiralType.CHI_TETRAHEDRAL_CCW,\n",
        "            Chem.rdchem.ChiralType.CHI_OTHER\n",
        "        ]\n",
        "        self.bond_list = [\n",
        "            Chem.rdchem.BondType.SINGLE,\n",
        "            Chem.rdchem.BondType.DOUBLE,\n",
        "            Chem.rdchem.BondType.TRIPLE,\n",
        "            Chem.rdchem.BondType.AROMATIC\n",
        "        ]\n",
        "        self.bonddir_list = [\n",
        "            Chem.rdchem.BondDir.NONE,\n",
        "            Chem.rdchem.BondDir.ENDUPRIGHT,\n",
        "            Chem.rdchem.BondDir.ENDDOWNRIGHT\n",
        "        ]\n",
        "\n",
        "    def calc_atom_features(self, atom: Chem.Atom) -> Tuple[list, list]:\n",
        "        \"\"\"Calculate atom features with better error handling\"\"\"\n",
        "        try:\n",
        "            # Basic features\n",
        "            atom_feat = [\n",
        "                self.atom_list.index(atom.GetAtomicNum()),\n",
        "                self.chirality_list.index(atom.GetChiralTag())\n",
        "            ]\n",
        "\n",
        "            # Physical features with error handling\n",
        "            phys_feat = []\n",
        "\n",
        "            # Molecular weight contribution\n",
        "            try:\n",
        "                contrib_mw = Descriptors.ExactMolWt(Chem.MolFromSmiles(f'[{atom.GetSymbol()}]'))\n",
        "                phys_feat.append(contrib_mw)\n",
        "            except:\n",
        "                phys_feat.append(0.0)\n",
        "\n",
        "            # LogP contribution\n",
        "            try:\n",
        "                contrib_logp = Descriptors.MolLogP(Chem.MolFromSmiles(f'[{atom.GetSymbol()}]'))\n",
        "                phys_feat.append(contrib_logp)\n",
        "            except:\n",
        "                phys_feat.append(0.0)\n",
        "\n",
        "            # Add other physical properties\n",
        "            phys_feat.extend([\n",
        "                atom.GetFormalCharge(),\n",
        "                int(atom.GetHybridization()),\n",
        "                int(atom.GetIsAromatic()),\n",
        "                atom.GetTotalNumHs(),\n",
        "                atom.GetTotalValence(),\n",
        "                atom.GetDegree()\n",
        "            ])\n",
        "\n",
        "            return atom_feat, phys_feat\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"Error calculating atom features: {e}\")\n",
        "            return [0, 0], [0.0] * 9\n",
        "\n",
        "    def get_atom_features(self, mol: Chem.Mol) -> Tuple[torch.Tensor, torch.Tensor]:\n",
        "        \"\"\"Extract atom features for the whole molecule\"\"\"\n",
        "        atom_feats = []\n",
        "        phys_feats = []\n",
        "\n",
        "        if mol is None:\n",
        "            return torch.tensor([[0, 0]], dtype=torch.long), torch.tensor([[0.0] * 9], dtype=torch.float)\n",
        "\n",
        "        for atom in mol.GetAtoms():\n",
        "            atom_feat, phys_feat = self.calc_atom_features(atom)\n",
        "            atom_feats.append(atom_feat)\n",
        "            phys_feats.append(phys_feat)\n",
        "\n",
        "        x = torch.tensor(atom_feats, dtype=torch.long)\n",
        "        phys = torch.tensor(phys_feats, dtype=torch.float)\n",
        "\n",
        "        return x, phys\n",
        "\n",
        "    def remove_unbonded_hydrogens(mol):\n",
        "        params = Chem.RemoveHsParameters()\n",
        "        params.removeDegreeZero = True\n",
        "        mol = Chem.RemoveHs(mol, params)\n",
        "        return mol\n",
        "\n",
        "\n",
        "    def get_bond_features(self, mol: Chem.Mol) -> Tuple[torch.Tensor, torch.Tensor]:\n",
        "        \"\"\"Extract bond features with better error handling\"\"\"\n",
        "        if mol is None:\n",
        "            return torch.tensor([[0], [0]], dtype=torch.long), torch.tensor([[0.0] * 5], dtype=torch.float)\n",
        "\n",
        "        row, col, edge_feat = [], [], []\n",
        "\n",
        "        for bond in mol.GetBonds():\n",
        "            try:\n",
        "                start, end = bond.GetBeginAtomIdx(), bond.GetEndAtomIdx()\n",
        "\n",
        "                # Add edges in both directions\n",
        "                row += [start, end]\n",
        "                col += [end, start]\n",
        "\n",
        "                # Bond features\n",
        "                bond_type = self.bond_list.index(bond.GetBondType())\n",
        "                bond_dir = self.bonddir_list.index(bond.GetBondDir())\n",
        "\n",
        "                # Calculate additional properties\n",
        "                feat = [\n",
        "                    bond_type,\n",
        "                    bond_dir,\n",
        "                    int(bond.GetIsConjugated()),\n",
        "                    int(self._is_rotatable(bond)),\n",
        "                    self._get_bond_length(mol, start, end)\n",
        "                ]\n",
        "\n",
        "                edge_feat.extend([feat, feat])\n",
        "\n",
        "            except Exception as e:\n",
        "                print(f\"Error processing bond: {e}\")\n",
        "                continue\n",
        "\n",
        "        if not row:  # If no valid bonds were processed\n",
        "            return torch.tensor([[0], [0]], dtype=torch.long), torch.tensor([[0.0] * 5], dtype=torch.float)\n",
        "\n",
        "        edge_index = torch.tensor([row, col], dtype=torch.long)\n",
        "        edge_attr = torch.tensor(edge_feat, dtype=torch.float)\n",
        "\n",
        "        return edge_index, edge_attr\n",
        "\n",
        "    def _is_rotatable(self, bond: Chem.Bond) -> bool:\n",
        "        \"\"\"Check if bond is rotatable\"\"\"\n",
        "        return (bond.GetBondType() == Chem.rdchem.BondType.SINGLE and\n",
        "                not bond.IsInRing() and\n",
        "                len(bond.GetBeginAtom().GetNeighbors()) > 1 and\n",
        "                len(bond.GetEndAtom().GetNeighbors()) > 1)\n",
        "\n",
        "    def _get_bond_length(self, mol: Chem.Mol, start: int, end: int) -> float:\n",
        "        \"\"\"Get bond length with error handling\"\"\"\n",
        "        try:\n",
        "            conf = mol.GetConformer()\n",
        "            if conf.Is3D():\n",
        "                return Chem.rdMolTransforms.GetBondLength(conf, start, end)\n",
        "        except:\n",
        "            pass\n",
        "        return 0.0\n",
        "\n",
        "    def process_molecule(self, smiles: str) -> Data:\n",
        "        \"\"\"Process SMILES string to graph data\"\"\"\n",
        "        try:\n",
        "            mol = Chem.MolFromSmiles(smiles)\n",
        "            if mol is None:\n",
        "                print(f\"Invalid SMILES: {smiles}\")\n",
        "                return None  # Skip invalid molecules\n",
        "            mol = RemoveHs(mol)\n",
        "\n",
        "            # Add explicit hydrogens\n",
        "            mol = Chem.AddHs(mol, addCoords=True)\n",
        "\n",
        "            # Sanitize molecule\n",
        "            Chem.SanitizeMol(mol)\n",
        "\n",
        "            # Check if the molecule has atoms\n",
        "            if mol.GetNumAtoms() == 0:\n",
        "                print(\"Molecule has no atoms, skipping.\")\n",
        "                return None\n",
        "\n",
        "            # Generate 3D coordinates\n",
        "            if not mol.GetNumConformers():\n",
        "                status = AllChem.EmbedMolecule(mol, AllChem.ETKDG())\n",
        "                if status != 0:\n",
        "                    print(\"Failed to generate 3D conformer\")\n",
        "                    return None  # Skip failed molecules\n",
        "\n",
        "                # Try MMFF or UFF optimization\n",
        "                try:\n",
        "                    AllChem.MMFFOptimizeMolecule(mol)\n",
        "                except:\n",
        "                    AllChem.UFFOptimizeMolecule(mol)\n",
        "\n",
        "            # Extract features\n",
        "            x_cat, x_phys = self.get_atom_features(mol)\n",
        "            edge_index, edge_attr = self.get_bond_features(mol)\n",
        "\n",
        "            return Data(\n",
        "                x_cat=x_cat,\n",
        "                x_phys=x_phys,\n",
        "                edge_index=edge_index,\n",
        "                edge_attr=edge_attr,\n",
        "                num_nodes=x_cat.size(0)\n",
        "            )\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"Error processing molecule {smiles}: {e}\")\n",
        "            return None\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "35a311ed",
      "metadata": {
        "id": "35a311ed"
      },
      "outputs": [],
      "source": [
        "class MemoryQueue:\n",
        "    \"\"\"Memory queue with temporal decay for contrastive learning\"\"\"\n",
        "    def __init__(self, size: int, dim: int, decay: float = 0.99999):\n",
        "        self.size = size\n",
        "        self.dim = dim\n",
        "        self.decay = decay\n",
        "        self.ptr = 0\n",
        "        self.full = False\n",
        "\n",
        "        # Initialize queue\n",
        "        self.queue = nn.Parameter(F.normalize(torch.randn(size, dim), dim=1), requires_grad=False)\n",
        "        self.queue_age = nn.Parameter(torch.zeros(size), requires_grad=False)\n",
        "\n",
        "#         self.register_buffer(\"queue\", torch.randn(size, dim))\n",
        "#         self.register_buffer(\"queue_age\", torch.zeros(size))  # Track age of each entry\n",
        "        self.queue = F.normalize(self.queue, dim=1)\n",
        "\n",
        "    def update_queue(self, keys: torch.Tensor):\n",
        "        \"\"\"Update queue with new keys\"\"\"\n",
        "        batch_size = keys.shape[0]\n",
        "\n",
        "        # Increment age of all entries\n",
        "        self.queue_age += 1\n",
        "\n",
        "        # Add new keys\n",
        "        if self.ptr + batch_size <= self.size:\n",
        "            self.queue[self.ptr:self.ptr + batch_size] = keys\n",
        "            self.queue_age[self.ptr:self.ptr + batch_size] = 0\n",
        "        else:\n",
        "            # Handle overflow\n",
        "            rem = self.size - self.ptr\n",
        "            self.queue[self.ptr:] = keys[:rem]\n",
        "            self.queue[:batch_size-rem] = keys[rem:]\n",
        "            self.queue_age[self.ptr:] = 0\n",
        "            self.queue_age[:batch_size-rem] = 0\n",
        "            self.full = True\n",
        "\n",
        "        self.ptr = (self.ptr + batch_size) % self.size\n",
        "\n",
        "    def get_decay_weights(self) -> torch.Tensor:\n",
        "        \"\"\"Get temporal decay weights for queue entries\"\"\"\n",
        "        return self.decay ** self.queue_age\n",
        "\n",
        "    def compute_contrastive_loss(self, query: torch.Tensor, positive_key: torch.Tensor,\n",
        "                                temperature: float = 0.07) -> torch.Tensor:\n",
        "        \"\"\"Compute contrastive loss with temporal decay\"\"\"\n",
        "        # Normalize embeddings\n",
        "        query = F.normalize(query, dim=1)\n",
        "        positive_key = F.normalize(positive_key, dim=1)\n",
        "        queue = F.normalize(self.queue, dim=1)\n",
        "\n",
        "        # Compute logits\n",
        "        l_pos = torch.einsum('nc,nc->n', [query, positive_key]).unsqueeze(-1)\n",
        "        l_neg = torch.einsum('nc,ck->nk', [query, queue.T])\n",
        "\n",
        "        # Apply temporal decay to negative samples\n",
        "        decay_weights = self.get_decay_weights()\n",
        "        l_neg = l_neg * decay_weights.unsqueeze(0)\n",
        "\n",
        "        # Temperature scaling\n",
        "        logits = torch.cat([l_pos, l_neg], dim=1) / temperature\n",
        "        labels = torch.zeros(logits.shape[0], dtype=torch.long, device=query.device)\n",
        "\n",
        "        return F.cross_entropy(logits, labels)\n",
        "\n",
        "class GraphGenerator(nn.Module):\n",
        "    \"\"\"Generator network with proper feature handling\"\"\"\n",
        "    def __init__(self, node_dim: int, edge_dim: int, hidden_dim: int = 128):\n",
        "        super().__init__()\n",
        "\n",
        "        # Node feature processing\n",
        "        self.node_encoder = nn.Sequential(\n",
        "            nn.Linear(node_dim, hidden_dim),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(hidden_dim, hidden_dim)\n",
        "        )\n",
        "\n",
        "        # Edge feature processing\n",
        "        self.edge_encoder = nn.Sequential(\n",
        "            nn.Linear(edge_dim, hidden_dim),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(hidden_dim, hidden_dim)\n",
        "        )\n",
        "\n",
        "        # Graph convolution layers\n",
        "        self.conv1 = GCNConv(hidden_dim, hidden_dim)\n",
        "        self.conv2 = GCNConv(hidden_dim, hidden_dim)\n",
        "        self.conv3 = GCNConv(hidden_dim, hidden_dim)\n",
        "\n",
        "        # Importance prediction layers\n",
        "        self.node_importance = nn.Sequential(\n",
        "            nn.Linear(hidden_dim, hidden_dim // 2),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(hidden_dim // 2, 1),\n",
        "            nn.Sigmoid()\n",
        "        )\n",
        "\n",
        "        self.edge_importance = nn.Sequential(\n",
        "            nn.Linear(hidden_dim * 2, hidden_dim),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(hidden_dim, 1),\n",
        "            nn.Sigmoid()\n",
        "        )\n",
        "\n",
        "    def normalize_features(self, x_cat, x_phys):\n",
        "        \"\"\"Normalize categorical and physical features separately\"\"\"\n",
        "        # Convert categorical features to one-hot\n",
        "        x_cat = x_cat.float()\n",
        "\n",
        "        # Normalize physical features\n",
        "        x_phys = x_phys.float()\n",
        "        if x_phys.size(0) > 1:  # Only normalize if we have more than one sample\n",
        "            x_phys = (x_phys - x_phys.mean(0)) / (x_phys.std(0) + 1e-5)\n",
        "\n",
        "        return x_cat, x_phys\n",
        "\n",
        "    def forward(self, data) -> Tuple[torch.Tensor, torch.Tensor]:\n",
        "        # Normalize features\n",
        "        x_cat, x_phys = self.normalize_features(data.x_cat, data.x_phys)\n",
        "\n",
        "        # Concatenate features\n",
        "        x = torch.cat([x_cat, x_phys], dim=-1)\n",
        "\n",
        "        edge_index = data.edge_index\n",
        "        edge_attr = data.edge_attr.float()  # Ensure float type\n",
        "\n",
        "        # Initial feature encoding\n",
        "        x = self.node_encoder(x)\n",
        "        edge_attr = self.edge_encoder(edge_attr)\n",
        "\n",
        "        # Graph convolutions\n",
        "        x = F.relu(self.conv1(x, edge_index))  # Removed edge_attr from GCNConv\n",
        "        x = F.relu(self.conv2(x, edge_index))\n",
        "        x = self.conv3(x, edge_index)\n",
        "\n",
        "        # Predict importance scores\n",
        "        node_scores = self.node_importance(x)\n",
        "\n",
        "        # Edge scores using both connected nodes\n",
        "        edge_features = torch.cat([\n",
        "            x[edge_index[0]],\n",
        "            x[edge_index[1]]\n",
        "        ], dim=-1)\n",
        "        edge_scores = self.edge_importance(edge_features)\n",
        "\n",
        "        return node_scores, edge_scores\n",
        "\n",
        "def get_model_config(dataset):\n",
        "    \"\"\"Get model configuration based on dataset features\"\"\"\n",
        "    sample_data = dataset[0]\n",
        "\n",
        "    # Calculate input dimensions\n",
        "    node_dim = sample_data.x_cat.shape[1] + sample_data.x_phys.shape[1]\n",
        "    edge_dim = sample_data.edge_attr.shape[1]\n",
        "\n",
        "    config = GanClConfig(\n",
        "        node_dim=node_dim,\n",
        "        edge_dim=edge_dim,\n",
        "        hidden_dim=128,\n",
        "        output_dim=128,\n",
        "        queue_size=65536,\n",
        "        momentum=0.999,\n",
        "        temperature=0.07,\n",
        "        decay=0.99999,\n",
        "        dropout_ratio=0.25\n",
        "    )\n",
        "\n",
        "    return config\n",
        "\n",
        "class GraphDiscriminator(nn.Module):\n",
        "    \"\"\"Discriminator/Encoder network\"\"\"\n",
        "    def __init__(self, node_dim: int, edge_dim: int, hidden_dim: int = 128, output_dim: int = 128):\n",
        "        super().__init__()\n",
        "\n",
        "        # Feature encoding\n",
        "        self.node_encoder = nn.Sequential(\n",
        "            nn.Linear(node_dim, hidden_dim),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(hidden_dim, hidden_dim)\n",
        "        )\n",
        "\n",
        "        self.edge_encoder = nn.Sequential(\n",
        "            nn.Linear(edge_dim, hidden_dim),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(hidden_dim, hidden_dim)\n",
        "        )\n",
        "\n",
        "        # Graph convolution layers\n",
        "        self.conv1 = GCNConv(hidden_dim, hidden_dim)\n",
        "        self.conv2 = GCNConv(hidden_dim, hidden_dim)\n",
        "        self.conv3 = GCNConv(hidden_dim, output_dim)\n",
        "\n",
        "        # Projection head for contrastive learning\n",
        "        self.projection = nn.Sequential(\n",
        "            nn.Linear(output_dim, hidden_dim),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(hidden_dim, output_dim)\n",
        "        )\n",
        "\n",
        "    def normalize_features(self, x_cat, x_phys):\n",
        "        \"\"\"Normalize categorical and physical features separately\"\"\"\n",
        "        # Convert categorical features to one-hot\n",
        "        x_cat = x_cat.float()\n",
        "\n",
        "        # Normalize physical features\n",
        "        x_phys = x_phys.float()\n",
        "        if x_phys.size(0) > 1:  # Only normalize if we have more than one sample\n",
        "            x_phys = (x_phys - x_phys.mean(0)) / (x_phys.std(0) + 1e-5)\n",
        "\n",
        "        return x_cat, x_phys\n",
        "\n",
        "    def forward(self, data):\n",
        "        # Normalize features\n",
        "        x_cat, x_phys = self.normalize_features(data.x_cat, data.x_phys)\n",
        "\n",
        "        # Concatenate features\n",
        "        x = torch.cat([x_cat, x_phys], dim=-1)\n",
        "\n",
        "        edge_index = data.edge_index\n",
        "        edge_attr = data.edge_attr.float()  # Ensure float type\n",
        "        batch = data.batch\n",
        "\n",
        "        # Initial feature encoding\n",
        "        x = self.node_encoder(x)\n",
        "        edge_attr = self.edge_encoder(edge_attr)\n",
        "\n",
        "        # Graph convolutions\n",
        "        x = F.relu(self.conv1(x, edge_index))  # Removed edge_attr from GCNConv\n",
        "        x = F.relu(self.conv2(x, edge_index))\n",
        "        x = self.conv3(x, edge_index)\n",
        "\n",
        "        # Global pooling\n",
        "        x = global_mean_pool(x, batch)\n",
        "\n",
        "        # Projection\n",
        "        x = self.projection(x)\n",
        "\n",
        "        return x\n",
        "\n",
        "@dataclass\n",
        "class GanClConfig:\n",
        "    \"\"\"Configuration for GAN-CL training\"\"\"\n",
        "    node_dim: int\n",
        "    edge_dim: int\n",
        "    hidden_dim: int = 128\n",
        "    output_dim: int = 128\n",
        "    queue_size: int = 65536\n",
        "    momentum: float = 0.999\n",
        "    temperature: float = 0.07\n",
        "    decay: float = 0.99999\n",
        "    dropout_ratio: float = 0.25\n",
        "\n",
        "class MolecularGANCL(nn.Module):\n",
        "    \"\"\"Combined GAN and Contrastive Learning framework\"\"\"\n",
        "    def __init__(self, config: GanClConfig):\n",
        "        super().__init__()\n",
        "        self.config = config\n",
        "\n",
        "        # Add weight initialization\n",
        "        def init_weights(m):\n",
        "            if isinstance(m, nn.Linear):\n",
        "                torch.nn.init.xavier_uniform_(m.weight)\n",
        "                m.bias.data.fill_(0.01)\n",
        "\n",
        "        # Initialize networks\n",
        "        self.generator = GraphGenerator(\n",
        "            config.node_dim,\n",
        "            config.edge_dim,\n",
        "            config.hidden_dim * 2\n",
        "        )\n",
        "\n",
        "        self.encoder = GraphDiscriminator(\n",
        "            config.node_dim,\n",
        "            config.edge_dim,\n",
        "            config.hidden_dim,\n",
        "            config.output_dim\n",
        "        )\n",
        "        self.encoder.apply(init_weights)\n",
        "\n",
        "        # Modified loss weights\n",
        "        self.contrastive_weight = 1.0\n",
        "        self.adversarial_weight = 0.1  # Increased from 0.05\n",
        "        self.similarity_weight = 0.01  # Decreased from 0.1\n",
        "\n",
        "        # Temperature annealing\n",
        "        self.initial_temperature = 0.1\n",
        "        self.min_temperature = 0.05\n",
        "\n",
        "        # Create momentum encoder\n",
        "        self.momentum_encoder = copy.deepcopy(self.encoder)\n",
        "        for param in self.momentum_encoder.parameters():\n",
        "            param.requires_grad = False\n",
        "\n",
        "        # Initialize memory queue\n",
        "        self.memory_queue = MemoryQueue(\n",
        "            config.queue_size,\n",
        "            config.output_dim,\n",
        "            config.decay\n",
        "        )\n",
        "\n",
        "    @torch.no_grad()\n",
        "    def _momentum_update(self):\n",
        "        \"\"\"Update momentum encoder\"\"\"\n",
        "        for param_q, param_k in zip(self.encoder.parameters(),\n",
        "                                  self.momentum_encoder.parameters()):\n",
        "            param_k.data = self.config.momentum * param_k.data + \\\n",
        "                          (1 - self.config.momentum) * param_q.data\n",
        "\n",
        "    def drop_graph_elements(self, data, node_scores: torch.Tensor,\n",
        "                          edge_scores: torch.Tensor) -> Data:\n",
        "        \"\"\"Apply dropout to graph based on importance scores\"\"\"\n",
        "        # Select elements to keep based on scores and dropout ratio\n",
        "#         node_mask = (node_scores < self.config.dropout_ratio).float()\n",
        "#         edge_mask = (edge_scores < self.config.dropout_ratio).float()\n",
        "\n",
        "        node_mask = (torch.rand_like(node_scores) > self.config.dropout_ratio).float()\n",
        "        edge_mask = (torch.rand_like(edge_scores) > self.config.dropout_ratio).float()\n",
        "\n",
        "        # Apply masks\n",
        "        x_cat_new = data.x_cat * node_mask\n",
        "        x_phys_new = data.x_phys * node_mask\n",
        "        edge_attr_new = data.edge_attr * edge_mask\n",
        "\n",
        "        # Create new graph data object\n",
        "        return Data(\n",
        "            x_cat=x_cat_new,\n",
        "            x_phys=x_phys_new,\n",
        "            edge_index=data.edge_index,\n",
        "            edge_attr=edge_attr_new,\n",
        "            batch=data.batch\n",
        "        )\n",
        "\n",
        "    def get_temperature(self, epoch, total_epochs):\n",
        "        \"\"\"Anneal temperature during training\"\"\"\n",
        "        progress = epoch / total_epochs\n",
        "        return max(self.initial_temperature * (1 - progress), self.min_temperature)\n",
        "\n",
        "    def forward(self, data, epoch=0, total_epochs=50):\n",
        "        # Get current temperature\n",
        "        temperature = self.get_temperature(epoch, total_epochs)\n",
        "\n",
        "        # Get importance scores from generator\n",
        "        node_scores, edge_scores = self.generator(data)\n",
        "\n",
        "        # Create perturbed graph\n",
        "        perturbed_data = self.drop_graph_elements(data, node_scores, edge_scores)\n",
        "\n",
        "        # Get embeddings\n",
        "        query_emb = self.encoder(perturbed_data)\n",
        "        with torch.no_grad():\n",
        "            key_emb = self.momentum_encoder(data)\n",
        "            original_emb = self.encoder(data).detach()\n",
        "\n",
        "        # Compute losses with modified weights\n",
        "        contrastive_loss = self.memory_queue.compute_contrastive_loss(\n",
        "            query_emb, key_emb, temperature\n",
        "        ) * self.contrastive_weight\n",
        "\n",
        "        adversarial_loss = -F.mse_loss(query_emb, original_emb) * self.adversarial_weight\n",
        "        similarity_loss = F.mse_loss(query_emb, original_emb) * self.similarity_weight\n",
        "\n",
        "        return contrastive_loss, adversarial_loss, similarity_loss\n",
        "\n",
        "    def get_embeddings(self, data) -> torch.Tensor:\n",
        "        \"\"\"Get embeddings for downstream tasks\"\"\"\n",
        "        with torch.no_grad():\n",
        "            return self.encoder(data)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "5a32fc5d",
      "metadata": {
        "id": "5a32fc5d"
      },
      "outputs": [],
      "source": [
        "class SMILESEmbeddingTracker:\n",
        "    \"\"\"Track the association between SMILES and embeddings during training\"\"\"\n",
        "\n",
        "    def __init__(self, output_dir='./embeddings'):\n",
        "        \"\"\"Initialize the tracker\n",
        "\n",
        "        Args:\n",
        "            output_dir: Directory to save embeddings and associated data\n",
        "        \"\"\"\n",
        "        self.output_dir = output_dir\n",
        "        os.makedirs(output_dir, exist_ok=True)\n",
        "        self.timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')\n",
        "        self.smiles_to_data = {}\n",
        "\n",
        "    def add_batch(self, batch, embeddings, smiles_list=None):\n",
        "        \"\"\"Add a batch of data to the tracker\n",
        "\n",
        "        Args:\n",
        "            batch: Batch of data from the dataloader\n",
        "            embeddings: Corresponding embeddings\n",
        "            smiles_list: List of SMILES strings (if available separately)\n",
        "        \"\"\"\n",
        "        if smiles_list is None:\n",
        "            # Extract SMILES from batch data\n",
        "            smiles_list = []\n",
        "            for data in batch:\n",
        "                if hasattr(data, 'smiles'):\n",
        "                    smiles_list.append(data.smiles)\n",
        "                else:\n",
        "                    # If no SMILES available, use a placeholder\n",
        "                    smiles_list.append(f\"mol_{len(self.smiles_to_data)}\")\n",
        "\n",
        "        # Map embeddings to SMILES\n",
        "        for smiles, emb in zip(smiles_list, embeddings):\n",
        "            self.smiles_to_data[smiles] = emb.detach().cpu().numpy()\n",
        "\n",
        "    def save_embeddings(self, epoch=None, is_final=False):\n",
        "        \"\"\"Save current embeddings with SMILES mapping\n",
        "\n",
        "        Args:\n",
        "            epoch: Current training epoch (if applicable)\n",
        "            is_final: Whether these are final embeddings\n",
        "\n",
        "        Returns:\n",
        "            Path to saved file\n",
        "        \"\"\"\n",
        "        if not self.smiles_to_data:\n",
        "            print(\"Warning: No embeddings to save\")\n",
        "            return None\n",
        "\n",
        "        # Prepare data for saving\n",
        "        smiles_list = list(self.smiles_to_data.keys())\n",
        "        embeddings_array = np.stack([self.smiles_to_data[s] for s in smiles_list])\n",
        "\n",
        "        # Create filename\n",
        "        if is_final:\n",
        "            filename = f\"final_embeddings_{self.timestamp}.npz\"\n",
        "        elif epoch is not None:\n",
        "            filename = f\"embeddings_epoch_{epoch}_{self.timestamp}.npz\"\n",
        "        else:\n",
        "            filename = f\"embeddings_{self.timestamp}.npz\"\n",
        "\n",
        "        filepath = os.path.join(self.output_dir, filename)\n",
        "\n",
        "        # Save as npz file\n",
        "        np.savez(filepath, embeddings=embeddings_array, smiles=smiles_list)\n",
        "\n",
        "        print(f\"Saved {len(smiles_list)} embeddings to {filepath}\")\n",
        "        return filepath\n",
        "\n",
        "    def reset(self):\n",
        "        \"\"\"Clear current data (e.g., between epochs)\"\"\"\n",
        "        self.smiles_to_data = {}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "4c5199ee",
      "metadata": {
        "id": "4c5199ee"
      },
      "outputs": [],
      "source": [
        "class MolecularAnalyzer:\n",
        "    \"\"\"Analyze molecules and their properties for bias detection\"\"\"\n",
        "\n",
        "    def __init__(self, output_dir='./analysis'):\n",
        "        \"\"\"Initialize the analyzer\n",
        "\n",
        "        Args:\n",
        "            output_dir: Directory to save analysis results\n",
        "        \"\"\"\n",
        "        self.output_dir = output_dir\n",
        "        os.makedirs(output_dir, exist_ok=True)\n",
        "        self.timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')\n",
        "\n",
        "    def extract_properties(self, mol):\n",
        "        \"\"\"Extract basic molecular properties\"\"\"\n",
        "        if mol is None:\n",
        "            return {\n",
        "                'MW': 0.0,\n",
        "                'LogP': 0.0,\n",
        "                'TPSA': 0.0,\n",
        "                'NumHAcceptors': 0,\n",
        "                'NumHDonors': 0,\n",
        "                'NumRotatableBonds': 0,\n",
        "                'NumAtoms': 0,\n",
        "                'NumHeavyAtoms': 0,\n",
        "                'NumBonds': 0\n",
        "            }\n",
        "\n",
        "        return {\n",
        "            'MW': Descriptors.ExactMolWt(mol),\n",
        "            'LogP': Descriptors.MolLogP(mol),\n",
        "            'TPSA': CalcTPSA(mol),\n",
        "            'NumHAcceptors': Lipinski.NumHAcceptors(mol),\n",
        "            'NumHDonors': Lipinski.NumHDonors(mol),\n",
        "            'NumRotatableBonds': Descriptors.NumRotatableBonds(mol),\n",
        "            'NumAtoms': mol.GetNumAtoms(),\n",
        "            'NumHeavyAtoms': mol.GetNumHeavyAtoms(),\n",
        "            'NumBonds': mol.GetNumBonds()\n",
        "        }\n",
        "\n",
        "    def extract_features(self, mol):\n",
        "        \"\"\"Extract structural features\"\"\"\n",
        "        if mol is None:\n",
        "            return {\n",
        "                'Aromatic': 0,\n",
        "                'Heterocycles': 0,\n",
        "                'FusedRings': 0,\n",
        "                'SpiroRings': 0,\n",
        "                'BridgedRings': 0,\n",
        "                'Macrocycles': 0,\n",
        "                'LinearChain': 0,\n",
        "                'Branched': 0\n",
        "            }\n",
        "\n",
        "        # Get ring information\n",
        "        ri = mol.GetRingInfo()\n",
        "        rings = ri.AtomRings()\n",
        "\n",
        "        # Count different ring types\n",
        "        num_aromatic = CalcNumAromaticRings(mol)\n",
        "        num_heterocycles = CalcNumHeterocycles(mol)\n",
        "\n",
        "        # Define feature presence (1 if present, 0 if not)\n",
        "        features = {\n",
        "            'Aromatic': 1 if num_aromatic > 0 else 0,\n",
        "            'Heterocycles': 1 if num_heterocycles > 0 else 0,\n",
        "            'FusedRings': 0,\n",
        "            'SpiroRings': 0,\n",
        "            'BridgedRings': 0,\n",
        "            'Macrocycles': 0,\n",
        "            'LinearChain': 0,\n",
        "            'Branched': 0\n",
        "        }\n",
        "\n",
        "        # Check for fused rings\n",
        "        if len(rings) >= 2:\n",
        "            # Check for fused rings (rings sharing atoms)\n",
        "            for i in range(len(rings)):\n",
        "                for j in range(i+1, len(rings)):\n",
        "                    if set(rings[i]).intersection(set(rings[j])):\n",
        "                        features['FusedRings'] = 1\n",
        "                        break\n",
        "                if features['FusedRings'] == 1:\n",
        "                    break\n",
        "\n",
        "        # Check for spiro rings (rings sharing exactly one atom)\n",
        "        for i in range(len(rings)):\n",
        "            for j in range(i+1, len(rings)):\n",
        "                if len(set(rings[i]).intersection(set(rings[j]))) == 1:\n",
        "                    features['SpiroRings'] = 1\n",
        "                    break\n",
        "            if features['SpiroRings'] == 1:\n",
        "                break\n",
        "\n",
        "        # Check for bridged rings (complex structures)\n",
        "        if mol.GetSubstructMatches(Chem.MolFromSmarts('[D4R]')):\n",
        "            features['BridgedRings'] = 1\n",
        "\n",
        "        # Check for macrocycles (rings with >= 8 atoms)\n",
        "        for ring in rings:\n",
        "            if len(ring) >= 8:\n",
        "                features['Macrocycles'] = 1\n",
        "                break\n",
        "\n",
        "        # Check for linear chains (molecules with no branches)\n",
        "        if mol.GetNumBonds() == mol.GetNumAtoms() - 1 and mol.GetNumRings() == 0:\n",
        "            features['LinearChain'] = 1\n",
        "\n",
        "        # Check for branched structures\n",
        "        branch_count = sum(1 for atom in mol.GetAtoms() if atom.GetDegree() > 2)\n",
        "        if branch_count > 0:\n",
        "            features['Branched'] = 1\n",
        "\n",
        "        return features\n",
        "\n",
        "    def extract_functional_groups(self, mol):\n",
        "        \"\"\"Extract functional group presence\"\"\"\n",
        "        if mol is None:\n",
        "            return {\n",
        "                'Alcohol': 0,\n",
        "                'Amine': 0,\n",
        "                'Carboxyl': 0,\n",
        "                'Carbonyl': 0,\n",
        "                'Ether': 0,\n",
        "                'Ester': 0,\n",
        "                'Amide': 0,\n",
        "                'Halogen': 0\n",
        "            }\n",
        "\n",
        "        # Count alcohols\n",
        "        alcohol_count = 0\n",
        "        for atom in mol.GetAtoms():\n",
        "            if atom.GetAtomicNum() == 8:  # Oxygen\n",
        "                if atom.GetTotalNumHs() > 0:  # Has at least one hydrogen\n",
        "                    # Check if connected to carbon\n",
        "                    for neighbor in atom.GetNeighbors():\n",
        "                        if neighbor.GetAtomicNum() == 6:  # Carbon\n",
        "                            alcohol_count += 1\n",
        "                            break\n",
        "\n",
        "        # Count amines using SMARTS patterns\n",
        "        amine_patt = Chem.MolFromSmarts('[NX3;H2,H1,H0]')\n",
        "        amine_count = len(mol.GetSubstructMatches(amine_patt)) if amine_patt else 0\n",
        "\n",
        "        # Count carboxylic acids\n",
        "        carboxylic_patt = Chem.MolFromSmarts('C(=O)[OH]')\n",
        "        carboxyl_count = len(mol.GetSubstructMatches(carboxylic_patt)) if carboxylic_patt else 0\n",
        "\n",
        "        # Count carbonyls (ketones and aldehydes)\n",
        "        ketone_patt = Chem.MolFromSmarts('[#6]C(=O)[#6]')\n",
        "        ketone_count = len(mol.GetSubstructMatches(ketone_patt)) if ketone_patt else 0\n",
        "\n",
        "        aldehyde_patt = Chem.MolFromSmarts('[#6]C(=O)[H]')\n",
        "        aldehyde_count = len(mol.GetSubstructMatches(aldehyde_patt)) if aldehyde_patt else 0\n",
        "\n",
        "        # Count ethers\n",
        "        ether_patt = Chem.MolFromSmarts('[#6]-[O]-[#6]')\n",
        "        ether_count = len(mol.GetSubstructMatches(ether_patt)) if ether_patt else 0\n",
        "\n",
        "        # Count esters\n",
        "        ester_patt = Chem.MolFromSmarts('[#6]C(=O)O[#6]')\n",
        "        ester_count = len(mol.GetSubstructMatches(ester_patt)) if ester_patt else 0\n",
        "\n",
        "        # Count amides\n",
        "        amide_patt = Chem.MolFromSmarts('C(=O)N')\n",
        "        amide_count = len(mol.GetSubstructMatches(amide_patt)) if amide_patt else 0\n",
        "\n",
        "        # Count halogens\n",
        "        halogen_count = sum(1 for atom in mol.GetAtoms()\n",
        "                          if atom.GetAtomicNum() in [9, 17, 35, 53])  # F, Cl, Br, I\n",
        "\n",
        "        return {\n",
        "            'Alcohol': alcohol_count,\n",
        "            'Amine': amine_count,\n",
        "            'Carboxyl': carboxyl_count,\n",
        "            'Carbonyl': ketone_count + aldehyde_count,\n",
        "            'Ether': ether_count,\n",
        "            'Ester': ester_count,\n",
        "            'Amide': amide_count,\n",
        "            'Halogen': halogen_count\n",
        "        }\n",
        "\n",
        "    def extract_ring_info(self, mol):\n",
        "        \"\"\"Extract ring information\"\"\"\n",
        "        if mol is None:\n",
        "            return {\n",
        "                'Spiro': {},\n",
        "                'Bridged': {},\n",
        "                'Fused': {},\n",
        "                'Single': {}\n",
        "            }\n",
        "\n",
        "        # Get ring information\n",
        "        ri = mol.GetRingInfo()\n",
        "        rings = ri.AtomRings()\n",
        "\n",
        "        # Initialize counts\n",
        "        ring_info = {\n",
        "            'Spiro': defaultdict(int),\n",
        "            'Bridged': defaultdict(int),\n",
        "            'Fused': defaultdict(int),\n",
        "            'Single': defaultdict(int)\n",
        "        }\n",
        "\n",
        "        # Process each ring\n",
        "        processed_rings = set()\n",
        "\n",
        "        # First identify spiro rings (rings sharing exactly one atom)\n",
        "        for i in range(len(rings)):\n",
        "            for j in range(i+1, len(rings)):\n",
        "                if len(set(rings[i]).intersection(set(rings[j]))) == 1:\n",
        "                    ring_info['Spiro'][len(rings[i])] += 1\n",
        "                    ring_info['Spiro'][len(rings[j])] += 1\n",
        "                    processed_rings.add(i)\n",
        "                    processed_rings.add(j)\n",
        "\n",
        "        # Identify fused rings (rings sharing more than one atom)\n",
        "        for i in range(len(rings)):\n",
        "            for j in range(i+1, len(rings)):\n",
        "                if i in processed_rings or j in processed_rings:\n",
        "                    continue\n",
        "\n",
        "                shared = len(set(rings[i]).intersection(set(rings[j])))\n",
        "                if shared > 1:\n",
        "                    ring_info['Fused'][len(rings[i])] += 1\n",
        "                    ring_info['Fused'][len(rings[j])] += 1\n",
        "                    processed_rings.add(i)\n",
        "                    processed_rings.add(j)\n",
        "\n",
        "        # Try to identify bridged rings\n",
        "        bridged_patt = Chem.MolFromSmarts('[D4R]')\n",
        "        if mol.HasSubstructMatch(bridged_patt):\n",
        "            for i in range(len(rings)):\n",
        "                if i in processed_rings:\n",
        "                    continue\n",
        "\n",
        "                if len(rings[i]) >= 6:\n",
        "                    ring_info['Bridged'][len(rings[i])] += 1\n",
        "                    processed_rings.add(i)\n",
        "\n",
        "        # Remaining rings are single\n",
        "        for i in range(len(rings)):\n",
        "            if i not in processed_rings:\n",
        "                ring_info['Single'][len(rings[i])] += 1\n",
        "\n",
        "        # Convert defaultdicts to regular dicts for JSON serialization\n",
        "        return {k: dict(v) for k, v in ring_info.items()}\n",
        "\n",
        "    def analyze_smiles_list(self, smiles_list, prefix=\"molecules\"):\n",
        "        \"\"\"Analyze a list of SMILES strings and extract properties\n",
        "\n",
        "        Args:\n",
        "            smiles_list: List of SMILES strings\n",
        "            prefix: Prefix for output files\n",
        "\n",
        "        Returns:\n",
        "            Tuple of DataFrames with properties, features, etc.\n",
        "        \"\"\"\n",
        "        print(f\"Analyzing {len(smiles_list)} molecules...\")\n",
        "\n",
        "        properties_list = []\n",
        "        features_list = []\n",
        "        func_groups_list = []\n",
        "        ring_info_list = []\n",
        "        valid_smiles = []\n",
        "\n",
        "        for smiles in tqdm(smiles_list):\n",
        "            mol = Chem.MolFromSmiles(smiles)\n",
        "            if mol is None:\n",
        "                print(f\"Warning: Could not parse SMILES: {smiles}\")\n",
        "                continue\n",
        "\n",
        "            valid_smiles.append(smiles)\n",
        "\n",
        "            # Extract and store properties\n",
        "            props = self.extract_properties(mol)\n",
        "            properties_list.append(props)\n",
        "\n",
        "            # Extract and store features\n",
        "            features = self.extract_features(mol)\n",
        "            features_list.append(features)\n",
        "\n",
        "            # Extract and store functional groups\n",
        "            func_groups = self.extract_functional_groups(mol)\n",
        "            func_groups_list.append(func_groups)\n",
        "\n",
        "            # Extract and store ring information\n",
        "            ring_info = self.extract_ring_info(mol)\n",
        "            ring_info_list.append(ring_info)\n",
        "\n",
        "        # Create DataFrames with SMILES as index\n",
        "        props_df = pd.DataFrame(properties_list, index=valid_smiles)\n",
        "        features_df = pd.DataFrame(features_list, index=valid_smiles)\n",
        "        func_groups_df = pd.DataFrame(func_groups_list, index=valid_smiles)\n",
        "\n",
        "        # Ring info requires special handling due to nested structure\n",
        "        ring_df = pd.DataFrame(index=valid_smiles)\n",
        "\n",
        "        # Flatten the ring info for DataFrame representation\n",
        "        for smiles, ring_data in zip(valid_smiles, ring_info_list):\n",
        "            for ring_type, size_dict in ring_data.items():\n",
        "                for size, count in size_dict.items():\n",
        "                    ring_df.at[smiles, f\"{ring_type}_Size{size}\"] = count\n",
        "\n",
        "        # Fill NaN values with 0\n",
        "        ring_df = ring_df.fillna(0)\n",
        "\n",
        "        # Save to CSV\n",
        "        output_prefix = os.path.join(self.output_dir, f\"{prefix}_{self.timestamp}\")\n",
        "\n",
        "        props_df.to_csv(f\"{output_prefix}_properties.csv\")\n",
        "        features_df.to_csv(f\"{output_prefix}_features.csv\")\n",
        "        func_groups_df.to_csv(f\"{output_prefix}_functional_groups.csv\")\n",
        "        ring_df.to_csv(f\"{output_prefix}_ring_info.csv\")\n",
        "\n",
        "        print(f\"Analysis saved to {output_prefix}_*.csv\")\n",
        "\n",
        "        return props_df, features_df, func_groups_df, ring_df, valid_smiles\n",
        "\n",
        "    def analyze_embeddings_file(self, embeddings_file, output_prefix=None):\n",
        "        \"\"\"Analyze embeddings from a file with SMILES mapping\n",
        "\n",
        "        Args:\n",
        "            embeddings_file: Path to .npz file with embeddings and SMILES\n",
        "            output_prefix: Prefix for output files (default: based on input filename)\n",
        "\n",
        "        Returns:\n",
        "            Tuple of DataFrames with properties, features, etc.\n",
        "        \"\"\"\n",
        "        # Load the embeddings file\n",
        "        try:\n",
        "            data = np.load(embeddings_file, allow_pickle=True)\n",
        "            embeddings = data['embeddings']\n",
        "            smiles_list = data['smiles']\n",
        "\n",
        "            print(f\"Loaded {len(smiles_list)} SMILES with embeddings of shape {embeddings.shape}\")\n",
        "\n",
        "            # Generate output prefix if not provided\n",
        "            if output_prefix is None:\n",
        "                base_name = os.path.splitext(os.path.basename(embeddings_file))[0]\n",
        "                output_prefix = os.path.join(self.output_dir, base_name)\n",
        "\n",
        "            # Analyze the SMILES\n",
        "            return self.analyze_smiles_list(smiles_list, prefix=output_prefix)\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"Error analyzing embeddings file: {e}\")\n",
        "            return None, None, None, None, None"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "0f22bbb1",
      "metadata": {
        "id": "0f22bbb1"
      },
      "outputs": [],
      "source": [
        "class MolecularBiasAnalyzer:\n",
        "    \"\"\"Molecular bias analysis for comparing raw molecules and embeddings\"\"\"\n",
        "\n",
        "    def __init__(self, output_dir: str = './bias_analysis'):\n",
        "        \"\"\"Initialize the bias analyzer\n",
        "\n",
        "        Args:\n",
        "            output_dir: Directory to save analysis results\n",
        "        \"\"\"\n",
        "        self.output_dir = output_dir\n",
        "        os.makedirs(output_dir, exist_ok=True)\n",
        "        self.timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')\n",
        "\n",
        "        # Setup SMILES to property mapping for quick lookup\n",
        "        self.smiles_to_props = {}\n",
        "        self.smiles_to_features = {}\n",
        "        self.smiles_to_funcs = {}\n",
        "        self.smiles_to_rings = {}\n",
        "\n",
        "    def extract_properties(self, mol: Chem.Mol) -> Dict[str, float]:\n",
        "        \"\"\"Extract basic molecular properties\n",
        "\n",
        "        Args:\n",
        "            mol: RDKit molecule object\n",
        "\n",
        "        Returns:\n",
        "            Dictionary of properties\n",
        "        \"\"\"\n",
        "        if mol is None:\n",
        "            return {\n",
        "                'MW': 0.0,\n",
        "                'LogP': 0.0,\n",
        "                'TPSA': 0.0,\n",
        "                'NumHAcceptors': 0,\n",
        "                'NumHDonors': 0,\n",
        "                'NumRotatableBonds': 0,\n",
        "                'NumAtoms': 0,\n",
        "                'NumHeavyAtoms': 0,\n",
        "                'NumBonds': 0\n",
        "            }\n",
        "\n",
        "        return {\n",
        "            'MW': Descriptors.ExactMolWt(mol),\n",
        "            'LogP': Descriptors.MolLogP(mol),\n",
        "            'TPSA': CalcTPSA(mol),\n",
        "            'NumHAcceptors': Lipinski.NumHAcceptors(mol),\n",
        "            'NumHDonors': Lipinski.NumHDonors(mol),\n",
        "            'NumRotatableBonds': Descriptors.NumRotatableBonds(mol),\n",
        "            'NumAtoms': mol.GetNumAtoms(),\n",
        "            'NumHeavyAtoms': mol.GetNumHeavyAtoms(),\n",
        "            'NumBonds': mol.GetNumBonds()\n",
        "        }\n",
        "\n",
        "    def extract_features(self, mol: Chem.Mol) -> Dict[str, int]:\n",
        "        \"\"\"Extract structural features\n",
        "\n",
        "        Args:\n",
        "            mol: RDKit molecule object\n",
        "\n",
        "        Returns:\n",
        "            Dictionary of feature presence (0/1)\n",
        "        \"\"\"\n",
        "        if mol is None:\n",
        "            return {\n",
        "                'Aromatic': 0,\n",
        "                'Heterocycles': 0,\n",
        "                'FusedRings': 0,\n",
        "                'SpiroRings': 0,\n",
        "                'BridgedRings': 0,\n",
        "                'Macrocycles': 0,\n",
        "                'LinearChain': 0,\n",
        "                'Branched': 0\n",
        "            }\n",
        "\n",
        "        # Get ring information\n",
        "        ri = mol.GetRingInfo()\n",
        "        rings = ri.AtomRings()\n",
        "\n",
        "        # Count different ring types\n",
        "        num_aromatic = CalcNumAromaticRings(mol)\n",
        "        num_heterocycles = CalcNumHeterocycles(mol)\n",
        "\n",
        "        # Define feature presence (1 if present, 0 if not)\n",
        "        features = {\n",
        "            'Aromatic': 1 if num_aromatic > 0 else 0,\n",
        "            'Heterocycles': 1 if num_heterocycles > 0 else 0,\n",
        "            'FusedRings': 0,  # Will be set below if detected\n",
        "            'SpiroRings': 0,  # Will be set below if detected\n",
        "            'BridgedRings': 0,  # Will be set below if detected\n",
        "            'Macrocycles': 0,  # Will be set below if detected\n",
        "            'LinearChain': 0,  # Will be set below if detected\n",
        "            'Branched': 0      # Will be set below if detected\n",
        "        }\n",
        "\n",
        "        # Check for fused rings\n",
        "        if len(rings) >= 2:\n",
        "            # Check for fused rings (rings sharing atoms)\n",
        "            for i in range(len(rings)):\n",
        "                for j in range(i+1, len(rings)):\n",
        "                    if set(rings[i]).intersection(set(rings[j])):\n",
        "                        features['FusedRings'] = 1\n",
        "                        break\n",
        "                if features['FusedRings'] == 1:\n",
        "                    break\n",
        "\n",
        "        # Check for spiro rings (rings sharing exactly one atom)\n",
        "        for i in range(len(rings)):\n",
        "            for j in range(i+1, len(rings)):\n",
        "                if len(set(rings[i]).intersection(set(rings[j]))) == 1:\n",
        "                    features['SpiroRings'] = 1\n",
        "                    break\n",
        "            if features['SpiroRings'] == 1:\n",
        "                break\n",
        "\n",
        "        # Check for bridged rings (complex structures)\n",
        "        # This is a simplification - detailed analysis would require more complex algorithms\n",
        "        if Fragments.fr_bicyclic(mol) > 0:\n",
        "            features['BridgedRings'] = 1\n",
        "\n",
        "        # Check for macrocycles (rings with >= 8 atoms)\n",
        "        for ring in rings:\n",
        "            if len(ring) >= 8:\n",
        "                features['Macrocycles'] = 1\n",
        "                break\n",
        "\n",
        "        # Check for linear chains (molecules with no branches)\n",
        "        if mol.GetNumBonds() == mol.GetNumAtoms() - 1 and mol.GetNumRings() == 0:\n",
        "            features['LinearChain'] = 1\n",
        "\n",
        "        # Check for branched structures\n",
        "        # Simplification: if multiple atoms have degree > 2, it's branched\n",
        "        branch_count = sum(1 for atom in mol.GetAtoms() if atom.GetDegree() > 2)\n",
        "        if branch_count > 0:\n",
        "            features['Branched'] = 1\n",
        "\n",
        "        return features\n",
        "\n",
        "    def extract_functional_groups(self, mol: Chem.Mol) -> Dict[str, int]:\n",
        "        \"\"\"Extract functional group presence\n",
        "\n",
        "        Args:\n",
        "            mol: RDKit molecule object\n",
        "\n",
        "        Returns:\n",
        "            Dictionary of functional group counts\n",
        "        \"\"\"\n",
        "        if mol is None:\n",
        "            return {\n",
        "                'Alcohol': 0,\n",
        "                'Amine': 0,\n",
        "                'Carboxyl': 0,\n",
        "                'Carbonyl': 0,\n",
        "                'Ether': 0,\n",
        "                'Ester': 0,\n",
        "                'Amide': 0,\n",
        "                'Halogen': 0\n",
        "            }\n",
        "\n",
        "        # Count alcohols\n",
        "        alcohol_count = 0\n",
        "        for atom in mol.GetAtoms():\n",
        "            if atom.GetAtomicNum() == 8:  # Oxygen\n",
        "                if atom.GetTotalNumHs() > 0:  # Has at least one hydrogen\n",
        "                    # Check if connected to carbon\n",
        "                    for neighbor in atom.GetNeighbors():\n",
        "                        if neighbor.GetAtomicNum() == 6:  # Carbon\n",
        "                            alcohol_count += 1\n",
        "                            break\n",
        "\n",
        "        # Count various functional groups using RDKit's built-in fragment counts\n",
        "        try:\n",
        "            amine_count = Fragments.fr_NH2(mol) + Fragments.fr_NH1(mol) + Fragments.fr_NH0(mol)\n",
        "        except:\n",
        "            # Fallback if fragment functions are not available\n",
        "            amine_count = sum(1 for atom in mol.GetAtoms()\n",
        "                             if atom.GetAtomicNum() == 7 and atom.GetTotalNumHs() > 0)\n",
        "\n",
        "        try:\n",
        "            carboxyl_count = Fragments.fr_COO(mol) + Fragments.fr_COOH(mol)\n",
        "        except:\n",
        "            # Fallback using SMARTS patterns\n",
        "            carboxyl_patt = Chem.MolFromSmarts('C(=O)[OH]')\n",
        "            carboxyl_count = len(mol.GetSubstructMatches(carboxyl_patt)) if carboxyl_patt else 0\n",
        "\n",
        "            ester_patt = Chem.MolFromSmarts('C(=O)O[#6]')\n",
        "            carboxyl_count += len(mol.GetSubstructMatches(ester_patt)) if ester_patt else 0\n",
        "\n",
        "        try:\n",
        "            carbonyl_count = Fragments.fr_ketone(mol) + Fragments.fr_aldehyde(mol)\n",
        "        except:\n",
        "            # Fallback using SMARTS patterns\n",
        "            ketone_patt = Chem.MolFromSmarts('[#6]C(=O)[#6]')\n",
        "            carbonyl_count = len(mol.GetSubstructMatches(ketone_patt)) if ketone_patt else 0\n",
        "\n",
        "            aldehyde_patt = Chem.MolFromSmarts('[#6]C(=O)[H]')\n",
        "            carbonyl_count += len(mol.GetSubstructMatches(aldehyde_patt)) if aldehyde_patt else 0\n",
        "\n",
        "        # Count ethers\n",
        "        try:\n",
        "            ether_count = len(mol.GetSubstructMatches(Chem.MolFromSmarts('[#6]-[O]-[#6]')))\n",
        "        except:\n",
        "            ether_count = 0\n",
        "\n",
        "        # Count esters\n",
        "        try:\n",
        "            ester_count = len(mol.GetSubstructMatches(Chem.MolFromSmarts('[#6]-C(=O)-O-[#6]')))\n",
        "        except:\n",
        "            ester_count = 0\n",
        "\n",
        "        # Count amides\n",
        "        try:\n",
        "            amide_count = len(mol.GetSubstructMatches(Chem.MolFromSmarts('C(=O)-N')))\n",
        "        except:\n",
        "            amide_count = 0\n",
        "\n",
        "        # Count halogens\n",
        "        halogen_count = sum(1 for atom in mol.GetAtoms()\n",
        "                           if atom.GetAtomicNum() in [9, 17, 35, 53])  # F, Cl, Br, I\n",
        "\n",
        "        return {\n",
        "            'Alcohol': alcohol_count,\n",
        "            'Amine': amine_count,\n",
        "            'Carboxyl': carboxyl_count,\n",
        "            'Carbonyl': carbonyl_count,\n",
        "            'Ether': ether_count,\n",
        "            'Ester': ester_count,\n",
        "            'Amide': amide_count,\n",
        "            'Halogen': halogen_count\n",
        "        }\n",
        "\n",
        "    def extract_ring_info(self, mol: Chem.Mol) -> Dict[str, Dict[int, int]]:\n",
        "        \"\"\"Extract ring information\n",
        "\n",
        "        Args:\n",
        "            mol: RDKit molecule object\n",
        "\n",
        "        Returns:\n",
        "            Dictionary of ring types and their size distributions\n",
        "        \"\"\"\n",
        "        if mol is None:\n",
        "            return {\n",
        "                'Spiro': {},\n",
        "                'Bridged': {},\n",
        "                'Fused': {},\n",
        "                'Single': {}\n",
        "            }\n",
        "\n",
        "        # Get ring information\n",
        "        ri = mol.GetRingInfo()\n",
        "        rings = ri.AtomRings()\n",
        "\n",
        "        # Initialize counts\n",
        "        ring_info = {\n",
        "            'Spiro': defaultdict(int),\n",
        "            'Bridged': defaultdict(int),\n",
        "            'Fused': defaultdict(int),\n",
        "            'Single': defaultdict(int)\n",
        "        }\n",
        "\n",
        "        # Process each ring\n",
        "        processed_rings = set()\n",
        "\n",
        "        # First identify spiro rings (rings sharing exactly one atom)\n",
        "        for i in range(len(rings)):\n",
        "            for j in range(i+1, len(rings)):\n",
        "                if len(set(rings[i]).intersection(set(rings[j]))) == 1:\n",
        "                    ring_info['Spiro'][len(rings[i])] += 1\n",
        "                    ring_info['Spiro'][len(rings[j])] += 1\n",
        "                    processed_rings.add(i)\n",
        "                    processed_rings.add(j)\n",
        "\n",
        "        # Identify fused rings (rings sharing more than one atom)\n",
        "        for i in range(len(rings)):\n",
        "            for j in range(i+1, len(rings)):\n",
        "                if i in processed_rings or j in processed_rings:\n",
        "                    continue\n",
        "\n",
        "                shared = len(set(rings[i]).intersection(set(rings[j])))\n",
        "                if shared > 1:\n",
        "                    ring_info['Fused'][len(rings[i])] += 1\n",
        "                    ring_info['Fused'][len(rings[j])] += 1\n",
        "                    processed_rings.add(i)\n",
        "                    processed_rings.add(j)\n",
        "\n",
        "        # Identify bridged rings (simplification)\n",
        "        if Fragments.fr_bicyclic(mol) > 0:\n",
        "            for i in range(len(rings)):\n",
        "                if i in processed_rings:\n",
        "                    continue\n",
        "\n",
        "                # This is a simplification - true bridged detection is complex\n",
        "                if len(rings[i]) >= 6:\n",
        "                    ring_info['Bridged'][len(rings[i])] += 1\n",
        "                    processed_rings.add(i)\n",
        "\n",
        "        # Remaining rings are single\n",
        "        for i in range(len(rings)):\n",
        "            if i not in processed_rings:\n",
        "                ring_info['Single'][len(rings[i])] += 1\n",
        "\n",
        "        # Convert defaultdicts to regular dicts for JSON serialization\n",
        "        return {k: dict(v) for k, v in ring_info.items()}\n",
        "\n",
        "    def analyze_raw_molecules(self, smiles_list: List[str]) -> Tuple[pd.DataFrame, pd.DataFrame, pd.DataFrame, pd.DataFrame]:\n",
        "        \"\"\"Analyze raw molecules from SMILES strings\n",
        "\n",
        "        Args:\n",
        "            smiles_list: List of SMILES strings\n",
        "\n",
        "        Returns:\n",
        "            DataFrames with properties, features, functional groups, and ring info\n",
        "        \"\"\"\n",
        "        print(\"Analyzing raw molecules...\")\n",
        "\n",
        "        properties_list = []\n",
        "        features_list = []\n",
        "        func_groups_list = []\n",
        "        ring_info_list = []\n",
        "        valid_smiles = []\n",
        "\n",
        "        for smiles in tqdm(smiles_list):\n",
        "            mol = Chem.MolFromSmiles(smiles)\n",
        "            if mol is None:\n",
        "                continue\n",
        "\n",
        "            valid_smiles.append(smiles)\n",
        "\n",
        "            # Extract and store properties\n",
        "            props = self.extract_properties(mol)\n",
        "            properties_list.append(props)\n",
        "            self.smiles_to_props[smiles] = props\n",
        "\n",
        "            # Extract and store features\n",
        "            features = self.extract_features(mol)\n",
        "            features_list.append(features)\n",
        "            self.smiles_to_features[smiles] = features\n",
        "\n",
        "            # Extract and store functional groups\n",
        "            func_groups = self.extract_functional_groups(mol)\n",
        "            func_groups_list.append(func_groups)\n",
        "            self.smiles_to_funcs[smiles] = func_groups\n",
        "\n",
        "            # Extract and store ring information\n",
        "            ring_info = self.extract_ring_info(mol)\n",
        "            ring_info_list.append(ring_info)\n",
        "            self.smiles_to_rings[smiles] = ring_info\n",
        "\n",
        "        # Create DataFrames with SMILES as index\n",
        "        props_df = pd.DataFrame(properties_list, index=valid_smiles)\n",
        "        features_df = pd.DataFrame(features_list, index=valid_smiles)\n",
        "        func_groups_df = pd.DataFrame(func_groups_list, index=valid_smiles)\n",
        "\n",
        "        # Ring info requires special handling due to nested structure\n",
        "        ring_df = pd.DataFrame(index=valid_smiles)\n",
        "\n",
        "        # Flatten the ring info for DataFrame representation\n",
        "        for smiles, ring_data in zip(valid_smiles, ring_info_list):\n",
        "            for ring_type, size_dict in ring_data.items():\n",
        "                for size, count in size_dict.items():\n",
        "                    ring_df.at[smiles, f\"{ring_type}_Size{size}\"] = count\n",
        "\n",
        "        # Fill NaN values with 0\n",
        "        ring_df = ring_df.fillna(0)\n",
        "\n",
        "        return props_df, features_df, func_groups_df, ring_df\n",
        "\n",
        "    def save_raw_analysis(self, props_df, features_df, func_groups_df, ring_df, prefix=\"raw\"):\n",
        "        \"\"\"Save raw molecule analysis to CSV files\"\"\"\n",
        "        output_prefix = os.path.join(self.output_dir, f\"{prefix}_{self.timestamp}\")\n",
        "\n",
        "        props_df.to_csv(f\"{output_prefix}_properties.csv\")\n",
        "        features_df.to_csv(f\"{output_prefix}_features.csv\")\n",
        "        func_groups_df.to_csv(f\"{output_prefix}_functional_groups.csv\")\n",
        "        ring_df.to_csv(f\"{output_prefix}_ring_info.csv\")\n",
        "\n",
        "        print(f\"Analysis saved to {output_prefix}_*.csv\")\n",
        "\n",
        "        # Also save the lookup mappings for later use\n",
        "        with open(f\"{output_prefix}_lookups.pkl\", 'wb') as f:\n",
        "            pickle.dump({\n",
        "                'smiles_to_props': self.smiles_to_props,\n",
        "                'smiles_to_features': self.smiles_to_features,\n",
        "                'smiles_to_funcs': self.smiles_to_funcs,\n",
        "                'smiles_to_rings': self.smiles_to_rings\n",
        "            }, f)\n",
        "\n",
        "        return output_prefix"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "c83a055b",
      "metadata": {
        "id": "c83a055b"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import torch\n",
        "from rdkit import Chem\n",
        "from rdkit.Chem import Descriptors, Fragments, Lipinski\n",
        "from rdkit.Chem.rdMolDescriptors import CalcNumRings, CalcNumAromaticRings, CalcNumHeterocycles\n",
        "from rdkit.Chem.rdMolDescriptors import CalcNumAliphaticRings, CalcTPSA\n",
        "from typing import List, Dict, Any, Tuple\n",
        "import os\n",
        "import json\n",
        "import pickle\n",
        "from tqdm import tqdm\n",
        "from datetime import datetime\n",
        "from collections import defaultdict\n",
        "\n",
        "\n",
        "class DirectSMILESTracker:\n",
        "    \"\"\"A simpler tracker that directly stores SMILES with their embeddings\"\"\"\n",
        "\n",
        "    def __init__(self, output_dir='./embeddings'):\n",
        "        \"\"\"Initialize the tracker\n",
        "\n",
        "        Args:\n",
        "            output_dir: Directory to save embeddings and associated data\n",
        "        \"\"\"\n",
        "        self.output_dir = output_dir\n",
        "        os.makedirs(output_dir, exist_ok=True)\n",
        "        self.timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')\n",
        "        # Simple list storage instead of dict\n",
        "        self.embeddings = []\n",
        "        self.smiles = []\n",
        "\n",
        "    def add_batch(self, batch_data, batch_embeddings):\n",
        "        \"\"\"Add batch data to tracker, ensuring SMILES are properly associated with embeddings\n",
        "\n",
        "        Args:\n",
        "            batch_data: Batch of molecular graph data\n",
        "            batch_embeddings: Tensor of embeddings\n",
        "        \"\"\"\n",
        "        # Convert embeddings to numpy\n",
        "        embeddings_np = batch_embeddings.detach().cpu().numpy()\n",
        "\n",
        "        # Extract SMILES from each data object\n",
        "        batch_smiles = []\n",
        "        for data in batch_data:\n",
        "            if hasattr(data, 'smiles'):\n",
        "                batch_smiles.append(data.smiles)\n",
        "            else:\n",
        "                # If no SMILES available, use a placeholder\n",
        "                batch_smiles.append(f\"unknown_molecule_{len(self.smiles)}\")\n",
        "\n",
        "        # Verify dimensions match\n",
        "        if len(batch_smiles) != len(embeddings_np):\n",
        "            print(f\"Warning: SMILES count ({len(batch_smiles)}) doesn't match embeddings count ({len(embeddings_np)})\")\n",
        "            # Use the smaller size\n",
        "            min_size = min(len(batch_smiles), len(embeddings_np))\n",
        "            batch_smiles = batch_smiles[:min_size]\n",
        "            embeddings_np = embeddings_np[:min_size]\n",
        "\n",
        "        # Append to lists\n",
        "        self.embeddings.extend(embeddings_np)\n",
        "        self.smiles.extend(batch_smiles)\n",
        "\n",
        "        print(f\"Added {len(batch_smiles)} SMILES-embedding pairs. Total now: {len(self.smiles)}\")\n",
        "\n",
        "    def save_embeddings(self, prefix=\"embeddings\"):\n",
        "        \"\"\"Save current embeddings with SMILES mapping\n",
        "\n",
        "        Args:\n",
        "            prefix: Prefix for the filename\n",
        "\n",
        "        Returns:\n",
        "            Path to saved file\n",
        "        \"\"\"\n",
        "        if not self.smiles or not self.embeddings:\n",
        "            print(\"Warning: No embeddings to save\")\n",
        "            return None\n",
        "\n",
        "        # Convert to numpy arrays\n",
        "        embeddings_array = np.array(self.embeddings)\n",
        "        smiles_array = np.array(self.smiles)\n",
        "\n",
        "        # Create filename\n",
        "        filename = f\"{prefix}_{self.timestamp}.npz\"\n",
        "        filepath = os.path.join(self.output_dir, filename)\n",
        "\n",
        "        # Save as npz file\n",
        "        np.savez(filepath, embeddings=embeddings_array, smiles=smiles_array)\n",
        "\n",
        "        print(f\"Saved {len(self.smiles)} embeddings to {filepath}\")\n",
        "        return filepath\n",
        "\n",
        "    def reset(self):\n",
        "        \"\"\"Clear current data\"\"\"\n",
        "        self.embeddings = []\n",
        "        self.smiles = []\n",
        "\n",
        "\n",
        "def modified_train_gan_cl(train_loader, config, device='cuda',\n",
        "                         save_dir='./checkpoints',\n",
        "                         embedding_dir='./embeddings'):\n",
        "    \"\"\"Modified training function with SMILES-embedding tracking\"\"\"\n",
        "\n",
        "    # Create directories\n",
        "    os.makedirs(save_dir, exist_ok=True)\n",
        "    os.makedirs(embedding_dir, exist_ok=True)\n",
        "\n",
        "    # Initialize tracker\n",
        "    tracker = DirectSMILESTracker(output_dir=embedding_dir)\n",
        "\n",
        "    # Extract all SMILES from the dataset for the \"before\" analysis\n",
        "    all_smiles = []\n",
        "    for batch in train_loader:\n",
        "        for data in batch:\n",
        "            if hasattr(data, 'smiles'):\n",
        "                all_smiles.append(data.smiles)\n",
        "\n",
        "    print(f\"Found {len(all_smiles)} molecules for 'before training' analysis\")\n",
        "\n",
        "    # Save the raw SMILES list for before-training analysis\n",
        "    smiles_path = os.path.join(embedding_dir, f\"before_training_smiles_{tracker.timestamp}.txt\")\n",
        "    with open(smiles_path, 'w') as f:\n",
        "        for smiles in all_smiles:\n",
        "            f.write(f\"{smiles}\\n\")\n",
        "\n",
        "    print(f\"Saved 'before training' SMILES to {smiles_path}\")\n",
        "\n",
        "    # Initialize model\n",
        "    model = MolecularGANCL(config).to(device)\n",
        "\n",
        "    # Initialize optimizers\n",
        "    optimizer_encoder = torch.optim.Adam(model.encoder.parameters(), lr=3e-4)\n",
        "    optimizer_generator = torch.optim.Adam(model.generator.parameters(), lr=1e-4)\n",
        "\n",
        "    # Training metrics\n",
        "    metrics = {\n",
        "        'contrastive_losses': [],\n",
        "        'adversarial_losses': [],\n",
        "        'similarity_losses': [],\n",
        "        'total_losses': []\n",
        "    }\n",
        "\n",
        "    # Get initial embeddings\n",
        "    model.eval()\n",
        "    print(\"Extracting initial embeddings...\")\n",
        "    with torch.no_grad():\n",
        "        for batch in tqdm(train_loader, desc=\"Initial embeddings\"):\n",
        "            batch = batch.to(device)\n",
        "            initial_embeddings = model.encoder(batch)\n",
        "            tracker.add_batch(batch, initial_embeddings)\n",
        "\n",
        "    # Save initial embeddings\n",
        "    initial_embeddings_path = tracker.save_embeddings(prefix=\"before_training\")\n",
        "    tracker.reset()\n",
        "    model.train()\n",
        "\n",
        "    # Training phases\n",
        "    print(\"Phase 1: Pretraining Contrastive Learning...\")\n",
        "    pretrain_epochs = 10\n",
        "    for epoch in range(pretrain_epochs):\n",
        "        contrastive_epoch_loss = 0\n",
        "\n",
        "        for batch in tqdm(train_loader, desc=f'Pretrain Epoch {epoch+1}/{pretrain_epochs}'):\n",
        "            batch = batch.to(device)\n",
        "\n",
        "            # Forward pass (without generator)\n",
        "            query_emb = model.encoder(batch)\n",
        "            with torch.no_grad():\n",
        "                key_emb = model.momentum_encoder(batch)\n",
        "\n",
        "            # Compute contrastive loss\n",
        "            contrastive_loss = model.memory_queue.compute_contrastive_loss(\n",
        "                query_emb, key_emb, model.config.temperature\n",
        "            )\n",
        "\n",
        "            # Update encoder\n",
        "            optimizer_encoder.zero_grad()\n",
        "            contrastive_loss.backward()\n",
        "            optimizer_encoder.step()\n",
        "\n",
        "            # Update momentum encoder\n",
        "            model._momentum_update()\n",
        "\n",
        "            # Update memory queue\n",
        "            model.memory_queue.update_queue(key_emb.detach())\n",
        "\n",
        "            contrastive_epoch_loss += contrastive_loss.item()\n",
        "\n",
        "        avg_loss = contrastive_epoch_loss / len(train_loader)\n",
        "        metrics['contrastive_losses'].append(avg_loss)\n",
        "        print(f'Pretrain Epoch {epoch+1}, Avg Loss: {avg_loss:.4f}')\n",
        "\n",
        "        # Save pretrained checkpoint\n",
        "        if (epoch + 1) % 5 == 0:\n",
        "            torch.save({\n",
        "                'epoch': epoch,\n",
        "                'model_state_dict': model.state_dict(),\n",
        "                'optimizer_encoder_state_dict': optimizer_encoder.state_dict(),\n",
        "                'loss': avg_loss,\n",
        "            }, os.path.join(save_dir, f'pretrain_checkpoint_{epoch+1}.pt'))\n",
        "\n",
        "    print(\"\\nPhase 2: Training GAN-CL...\")\n",
        "    train_epochs = 50\n",
        "    best_loss = float('inf')\n",
        "\n",
        "    for epoch in range(train_epochs):\n",
        "        epoch_losses = {\n",
        "            'contrastive': 0,\n",
        "            'adversarial': 0,\n",
        "            'similarity': 0,\n",
        "            'total': 0\n",
        "        }\n",
        "\n",
        "        for batch in tqdm(train_loader, desc=f'Train Epoch {epoch+1}/{train_epochs}'):\n",
        "            batch = batch.to(device)\n",
        "\n",
        "            # Step 1: Train Encoder\n",
        "            optimizer_encoder.zero_grad()\n",
        "\n",
        "            # Get importance scores from generator\n",
        "            with torch.no_grad():\n",
        "                node_scores, edge_scores = model.generator(batch)\n",
        "\n",
        "            # Create perturbed graph\n",
        "            perturbed_data = model.drop_graph_elements(batch, node_scores, edge_scores)\n",
        "\n",
        "            # Get embeddings\n",
        "            query_emb = model.encoder(perturbed_data)\n",
        "            with torch.no_grad():\n",
        "                key_emb = model.momentum_encoder(batch)\n",
        "                original_emb = model.encoder(batch).detach()\n",
        "\n",
        "            # Compute losses for encoder\n",
        "            contrastive_loss = model.memory_queue.compute_contrastive_loss(\n",
        "                query_emb, key_emb, model.config.temperature\n",
        "            )\n",
        "            similarity_loss = torch.nn.functional.mse_loss(query_emb, original_emb)\n",
        "\n",
        "            # Total loss for encoder\n",
        "            encoder_loss = contrastive_loss + 0.1 * similarity_loss\n",
        "\n",
        "            # Update encoder\n",
        "            encoder_loss.backward()\n",
        "            optimizer_encoder.step()\n",
        "\n",
        "            # Update momentum encoder\n",
        "            model._momentum_update()\n",
        "\n",
        "            # Step 2: Train Generator\n",
        "            optimizer_generator.zero_grad()\n",
        "\n",
        "            # Get new embeddings for adversarial loss\n",
        "            node_scores, edge_scores = model.generator(batch)\n",
        "            perturbed_data = model.drop_graph_elements(batch, node_scores, edge_scores)\n",
        "\n",
        "            with torch.no_grad():\n",
        "                original_emb = model.encoder(batch)\n",
        "            perturbed_emb = model.encoder(perturbed_data)\n",
        "\n",
        "            # Compute adversarial loss\n",
        "            adversarial_loss = -torch.nn.functional.mse_loss(perturbed_emb, original_emb)\n",
        "\n",
        "            # Update generator\n",
        "            adversarial_loss.backward()\n",
        "            optimizer_generator.step()\n",
        "\n",
        "            # Update memory queue\n",
        "            model.memory_queue.update_queue(key_emb.detach())\n",
        "\n",
        "            # Update metrics\n",
        "            epoch_losses['contrastive'] += contrastive_loss.item()\n",
        "            epoch_losses['adversarial'] += adversarial_loss.item()\n",
        "            epoch_losses['similarity'] += similarity_loss.item()\n",
        "            epoch_losses['total'] += encoder_loss.item()\n",
        "\n",
        "        # Average losses\n",
        "        for k in epoch_losses:\n",
        "            epoch_losses[k] /= len(train_loader)\n",
        "            metrics[f'{k}_losses'].append(epoch_losses[k])\n",
        "\n",
        "        print(f'Epoch {epoch+1}, Losses: {epoch_losses}')\n",
        "\n",
        "        # Save checkpoint\n",
        "        if (epoch + 1) % 10 == 0:\n",
        "            torch.save({\n",
        "                'epoch': epoch,\n",
        "                'model_state_dict': model.state_dict(),\n",
        "                'optimizer_encoder_state_dict': optimizer_encoder.state_dict(),\n",
        "                'optimizer_generator_state_dict': optimizer_generator.state_dict(),\n",
        "                'losses': epoch_losses,\n",
        "            }, os.path.join(save_dir, f'gan_cl_checkpoint_{epoch+1}.pt'))\n",
        "\n",
        "    # Extract final embeddings after training\n",
        "    model.eval()\n",
        "    tracker.reset()  # Clear previous embeddings\n",
        "\n",
        "    print(\"Extracting final embeddings...\")\n",
        "    with torch.no_grad():\n",
        "        for batch in tqdm(train_loader, desc=\"Extracting final embeddings\"):\n",
        "            batch = batch.to(device)\n",
        "            final_embeddings = model.get_embeddings(batch)\n",
        "            tracker.add_batch(batch, final_embeddings)\n",
        "\n",
        "    # Save final embeddings\n",
        "    final_embeddings_path = tracker.save_embeddings(prefix=\"after_training\")\n",
        "\n",
        "    # Save final metrics\n",
        "    with open(os.path.join(save_dir, 'training_metrics.json'), 'w') as f:\n",
        "        json.dump(metrics, f)\n",
        "\n",
        "    # Make sure we have a valid path to return\n",
        "    if final_embeddings_path is None:\n",
        "        print(\"WARNING: Failed to save final embeddings. Using empty arrays.\")\n",
        "        return model, metrics, np.array([]), np.array([])\n",
        "\n",
        "    # Load the final embeddings to return\n",
        "    try:\n",
        "        data = np.load(final_embeddings_path)\n",
        "        final_embeddings = data['embeddings']\n",
        "        final_smiles = data['smiles']\n",
        "        print(f\"Loaded {len(final_smiles)} final embeddings from {final_embeddings_path}\")\n",
        "    except Exception as e:\n",
        "        print(f\"Error loading final embeddings: {e}\")\n",
        "        print(\"Using empty arrays for return values\")\n",
        "        final_embeddings = np.array([])\n",
        "        final_smiles = np.array([])\n",
        "\n",
        "    return model, metrics, final_embeddings, final_smiles\n",
        "\n",
        "\n",
        "def analyze_smiles_file(smiles_file, output_dir='./analysis', prefix='analysis'):\n",
        "    \"\"\"Analyze a file of SMILES strings for molecular properties\n",
        "\n",
        "    Args:\n",
        "        smiles_file: Path to text file with SMILES strings (one per line)\n",
        "        output_dir: Directory to save analysis results\n",
        "        prefix: Prefix for output files\n",
        "    \"\"\"\n",
        "    # Make sure directory exists\n",
        "    os.makedirs(output_dir, exist_ok=True)\n",
        "\n",
        "    # Load SMILES\n",
        "    smiles_list = []\n",
        "    with open(smiles_file, 'r') as f:\n",
        "        for line in f:\n",
        "            smiles = line.strip()\n",
        "            if smiles:\n",
        "                smiles_list.append(smiles)\n",
        "\n",
        "    print(f\"Loaded {len(smiles_list)} SMILES strings for analysis\")\n",
        "\n",
        "    # Analyze properties\n",
        "    analyze_smiles_list(smiles_list, output_dir, prefix)\n",
        "\n",
        "\n",
        "def analyze_embedding_file(embedding_file, output_dir='./analysis', prefix=None):\n",
        "    \"\"\"Analyze an embedding file with SMILES mapping\n",
        "\n",
        "    Args:\n",
        "        embedding_file: Path to .npz file with embeddings and SMILES\n",
        "        output_dir: Directory to save analysis results\n",
        "        prefix: Prefix for output files (default: derived from embedding filename)\n",
        "    \"\"\"\n",
        "    # Make sure directory exists\n",
        "    os.makedirs(output_dir, exist_ok=True)\n",
        "\n",
        "    # Determine prefix if not provided\n",
        "    if prefix is None:\n",
        "        base_name = os.path.splitext(os.path.basename(embedding_file))[0]\n",
        "        prefix = base_name\n",
        "\n",
        "    # Load embeddings and SMILES\n",
        "    try:\n",
        "        data = np.load(embedding_file)\n",
        "        embeddings = data['embeddings']\n",
        "        smiles_list = data['smiles']\n",
        "\n",
        "        print(f\"Loaded {len(smiles_list)} SMILES strings from embedding file\")\n",
        "\n",
        "        # Analyze properties\n",
        "        analyze_smiles_list(smiles_list, output_dir, prefix)\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"Error loading embedding file: {e}\")\n",
        "\n",
        "\n",
        "def analyze_smiles_list(smiles_list, output_dir='./analysis', prefix='analysis'):\n",
        "    \"\"\"Analyze a list of SMILES strings for molecular properties\n",
        "\n",
        "    Args:\n",
        "        smiles_list: List of SMILES strings\n",
        "        output_dir: Directory to save analysis results\n",
        "        prefix: Prefix for output files\n",
        "    \"\"\"\n",
        "    from rdkit import Chem\n",
        "    from rdkit.Chem import Descriptors, Lipinski\n",
        "    from rdkit.Chem.rdMolDescriptors import CalcTPSA\n",
        "    from collections import defaultdict\n",
        "    import pandas as pd\n",
        "\n",
        "    print(f\"Analyzing {len(smiles_list)} molecules...\")\n",
        "\n",
        "    # Prepare data storage\n",
        "    properties = []\n",
        "    features = []\n",
        "    func_groups = []\n",
        "    ring_info = []\n",
        "    valid_smiles = []\n",
        "\n",
        "    timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')\n",
        "\n",
        "    # Process each SMILES\n",
        "    for smiles in tqdm(smiles_list):\n",
        "        mol = Chem.MolFromSmiles(smiles)\n",
        "        if mol is None:\n",
        "            print(f\"Warning: Invalid SMILES: {smiles}\")\n",
        "            continue\n",
        "\n",
        "        valid_smiles.append(smiles)\n",
        "\n",
        "        # Extract basic properties\n",
        "        prop = {\n",
        "            'MW': Descriptors.ExactMolWt(mol),\n",
        "            'LogP': Descriptors.MolLogP(mol),\n",
        "            'TPSA': CalcTPSA(mol),\n",
        "            'NumHAcceptors': Lipinski.NumHAcceptors(mol),\n",
        "            'NumHDonors': Lipinski.NumHDonors(mol),\n",
        "            'NumRotatableBonds': Descriptors.NumRotatableBonds(mol),\n",
        "            'NumAtoms': mol.GetNumAtoms(),\n",
        "            'NumHeavyAtoms': mol.GetNumHeavyAtoms(),\n",
        "            'NumBonds': mol.GetNumBonds(),\n",
        "            'NumRings': Chem.GetSSSR(mol)\n",
        "        }\n",
        "        properties.append(prop)\n",
        "\n",
        "        # Extract structural features\n",
        "        ri = mol.GetRingInfo()\n",
        "        rings = ri.AtomRings()\n",
        "\n",
        "        num_aromatic = sum(1 for atom in mol.GetAtoms() if atom.GetIsAromatic())\n",
        "\n",
        "        feat = {\n",
        "            'Aromatic': 1 if num_aromatic > 0 else 0,\n",
        "            'Heterocycles': 1 if any(any(mol.GetAtomWithIdx(idx).GetAtomicNum() != 6 for idx in ring) for ring in rings) else 0,\n",
        "            'FusedRings': 0,\n",
        "            'SpiroRings': 0,\n",
        "            'BridgedRings': 0,\n",
        "            'Macrocycles': 0,\n",
        "            'LinearChain': 1 if mol.GetNumBonds() == mol.GetNumAtoms() - 1 and len(rings) == 0 else 0,\n",
        "            'Branched': 1 if sum(1 for atom in mol.GetAtoms() if atom.GetDegree() > 2) > 0 else 0\n",
        "        }\n",
        "\n",
        "        # Check for fused rings\n",
        "        if len(rings) >= 2:\n",
        "            for i in range(len(rings)):\n",
        "                for j in range(i+1, len(rings)):\n",
        "                    if len(set(rings[i]).intersection(set(rings[j]))) > 1:\n",
        "                        feat['FusedRings'] = 1\n",
        "                        break\n",
        "                if feat['FusedRings'] == 1:\n",
        "                    break\n",
        "\n",
        "        # Check for spiro rings\n",
        "        if len(rings) >= 2:\n",
        "            for i in range(len(rings)):\n",
        "                for j in range(i+1, len(rings)):\n",
        "                    if len(set(rings[i]).intersection(set(rings[j]))) == 1:\n",
        "                        feat['SpiroRings'] = 1\n",
        "                        break\n",
        "                if feat['SpiroRings'] == 1:\n",
        "                    break\n",
        "\n",
        "        # Check for bridged rings\n",
        "        bridged_patt = Chem.MolFromSmarts('[D4R]')\n",
        "        if bridged_patt and mol.HasSubstructMatch(bridged_patt):\n",
        "            feat['BridgedRings'] = 1\n",
        "\n",
        "        # Check for macrocycles\n",
        "        for ring in rings:\n",
        "            if len(ring) >= 8:\n",
        "                feat['Macrocycles'] = 1\n",
        "                break\n",
        "\n",
        "        features.append(feat)\n",
        "\n",
        "        # Extract functional groups\n",
        "        fg = {\n",
        "            'Alcohol': len(mol.GetSubstructMatches(Chem.MolFromSmarts('CO[H]'))) if Chem.MolFromSmarts('CO[H]') else 0,\n",
        "            'Amine': len(mol.GetSubstructMatches(Chem.MolFromSmarts('[NX3]'))) if Chem.MolFromSmarts('[NX3]') else 0,\n",
        "            'Carboxyl': len(mol.GetSubstructMatches(Chem.MolFromSmarts('C(=O)[OH]'))) if Chem.MolFromSmarts('C(=O)[OH]') else 0,\n",
        "            'Carbonyl': len(mol.GetSubstructMatches(Chem.MolFromSmarts('C=O'))) if Chem.MolFromSmarts('C=O') else 0,\n",
        "            'Ether': len(mol.GetSubstructMatches(Chem.MolFromSmarts('COC'))) if Chem.MolFromSmarts('COC') else 0,\n",
        "            'Ester': len(mol.GetSubstructMatches(Chem.MolFromSmarts('C(=O)OC'))) if Chem.MolFromSmarts('C(=O)OC') else 0,\n",
        "            'Amide': len(mol.GetSubstructMatches(Chem.MolFromSmarts('C(=O)N'))) if Chem.MolFromSmarts('C(=O)N') else 0,\n",
        "            'Halogen': sum(1 for atom in mol.GetAtoms() if atom.GetAtomicNum() in [9, 17, 35, 53])\n",
        "        }\n",
        "        func_groups.append(fg)\n",
        "\n",
        "        # Extract ring information\n",
        "        rings_data = {\n",
        "            'Spiro': defaultdict(int),\n",
        "            'Bridged': defaultdict(int),\n",
        "            'Fused': defaultdict(int),\n",
        "            'Single': defaultdict(int)\n",
        "        }\n",
        "\n",
        "        # Process ring information\n",
        "        processed = set()\n",
        "\n",
        "        # First identify spiro rings\n",
        "        for i in range(len(rings)):\n",
        "            for j in range(i+1, len(rings)):\n",
        "                if len(set(rings[i]).intersection(set(rings[j]))) == 1:\n",
        "                    rings_data['Spiro'][len(rings[i])] += 1\n",
        "                    rings_data['Spiro'][len(rings[j])] += 1\n",
        "                    processed.add(i)\n",
        "                    processed.add(j)\n",
        "\n",
        "        # Identify fused rings\n",
        "        for i in range(len(rings)):\n",
        "            for j in range(i+1, len(rings)):\n",
        "                if i in processed or j in processed:\n",
        "                    continue\n",
        "\n",
        "                shared = len(set(rings[i]).intersection(set(rings[j])))\n",
        "                if shared > 1:\n",
        "                    rings_data['Fused'][len(rings[i])] += 1\n",
        "                    rings_data['Fused'][len(rings[j])] += 1\n",
        "                    processed.add(i)\n",
        "                    processed.add(j)\n",
        "\n",
        "        # Try to identify bridged rings\n",
        "        if mol.HasSubstructMatch(Chem.MolFromSmarts('[D4R]')):\n",
        "            for i in range(len(rings)):\n",
        "                if i in processed:\n",
        "                    continue\n",
        "\n",
        "                if len(rings[i]) >= 6:\n",
        "                    rings_data['Bridged'][len(rings[i])] += 1\n",
        "                    processed.add(i)\n",
        "\n",
        "        # Remaining rings are single\n",
        "        for i in range(len(rings)):\n",
        "            if i not in processed:\n",
        "                rings_data['Single'][len(rings[i])] += 1\n",
        "\n",
        "        # Convert defaultdicts to regular dicts\n",
        "        rings_data = {k: dict(v) for k, v in rings_data.items()}\n",
        "        ring_info.append(rings_data)\n",
        "\n",
        "    # Create DataFrames\n",
        "    props_df = pd.DataFrame(properties, index=valid_smiles)\n",
        "    features_df = pd.DataFrame(features, index=valid_smiles)\n",
        "    func_groups_df = pd.DataFrame(func_groups, index=valid_smiles)\n",
        "\n",
        "    # Ring info requires special handling\n",
        "    ring_df = pd.DataFrame(index=valid_smiles)\n",
        "\n",
        "    # Flatten ring info\n",
        "    for smiles, rings_data in zip(valid_smiles, ring_info):\n",
        "        for ring_type, size_dict in rings_data.items():\n",
        "            for size, count in size_dict.items():\n",
        "                ring_df.at[smiles, f\"{ring_type}_Size{size}\"] = count\n",
        "\n",
        "    # Fill NaN values with 0\n",
        "    ring_df = ring_df.fillna(0)\n",
        "\n",
        "    # Save to CSV\n",
        "    output_prefix = os.path.join(output_dir, f\"{prefix}_{timestamp}\")\n",
        "\n",
        "    props_df.to_csv(f\"{output_prefix}_properties.csv\")\n",
        "    features_df.to_csv(f\"{output_prefix}_features.csv\")\n",
        "    func_groups_df.to_csv(f\"{output_prefix}_functional_groups.csv\")\n",
        "    ring_df.to_csv(f\"{output_prefix}_ring_info.csv\")\n",
        "\n",
        "    print(f\"Analysis complete. Results saved to {output_prefix}_*.csv\")\n",
        "\n",
        "    return props_df, features_df, func_groups_df, ring_df"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "4c840793",
      "metadata": {
        "id": "4c840793"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "3dd5f4cc",
      "metadata": {
        "id": "3dd5f4cc"
      },
      "outputs": [],
      "source": [
        "# Function to modify the original train_gan_cl function\n",
        "def train_gan_cl_with_bias_analysis(train_loader, config, device='cuda',\n",
        "                                   save_dir='./checkpoints',\n",
        "                                   embedding_dir='./embeddings'):\n",
        "    \"\"\"Main training function for GAN-CL with bias analysis\"\"\"\n",
        "\n",
        "    # Create directories\n",
        "    os.makedirs(save_dir, exist_ok=True)\n",
        "    os.makedirs(embedding_dir, exist_ok=True)\n",
        "    encoder_dir = os.path.join(save_dir, 'encoders')\n",
        "    os.makedirs(encoder_dir, exist_ok=True)\n",
        "\n",
        "    # Create bias analyzer\n",
        "    analyzer = MolecularBiasAnalyzer(output_dir='./bias_analysis')\n",
        "\n",
        "    # Extract and analyze SMILES from the dataset\n",
        "    smiles_list = []\n",
        "    for batch in train_loader:\n",
        "        for data in batch:\n",
        "            if hasattr(data, 'smiles'):\n",
        "                smiles_list.append(data.smiles)\n",
        "\n",
        "    print(f\"Found {len(smiles_list)} SMILES strings for analysis\")\n",
        "\n",
        "    # Analyze raw molecules (before training)\n",
        "    props_df, features_df, func_groups_df, ring_df = analyzer.analyze_raw_molecules(smiles_list)\n",
        "    before_prefix = analyzer.save_raw_analysis(props_df, features_df, func_groups_df, ring_df, \"before_training\")\n",
        "\n",
        "    # Initialize model\n",
        "    model = MolecularGANCL(config).to(device)\n",
        "\n",
        "    # Initialize optimizers\n",
        "    optimizer_encoder = torch.optim.Adam(model.encoder.parameters(), lr=3e-4)\n",
        "    optimizer_generator = torch.optim.Adam(model.generator.parameters(), lr=1e-4)\n",
        "\n",
        "    # Save initial model info\n",
        "    model_info = {\n",
        "        'node_dim': config.node_dim,\n",
        "        'edge_dim': config.edge_dim,\n",
        "        'hidden_dim': config.hidden_dim,\n",
        "        'output_dim': config.output_dim,\n",
        "        'training_config': config.__dict__\n",
        "    }\n",
        "\n",
        "    # Training phases as before...\n",
        "    best_loss = float('inf')\n",
        "\n",
        "    # Training metrics\n",
        "    metrics = {\n",
        "        'contrastive_losses': [],\n",
        "        'adversarial_losses': [],\n",
        "        'similarity_losses': [],\n",
        "        'total_losses': []\n",
        "    }\n",
        "\n",
        "    # Training phases\n",
        "    print(\"Phase 1: Pretraining Contrastive Learning...\")\n",
        "    pretrain_epochs = 10\n",
        "    for epoch in range(pretrain_epochs):\n",
        "        contrastive_epoch_loss = 0\n",
        "\n",
        "        for batch in tqdm(train_loader, desc=f'Pretrain Epoch {epoch+1}/{pretrain_epochs}'):\n",
        "            batch = batch.to(device)\n",
        "\n",
        "            # Forward pass (without generator)\n",
        "            query_emb = model.encoder(batch)\n",
        "            with torch.no_grad():\n",
        "                key_emb = model.momentum_encoder(batch)\n",
        "\n",
        "            # Compute contrastive loss\n",
        "            contrastive_loss = model.memory_queue.compute_contrastive_loss(\n",
        "                query_emb, key_emb, model.config.temperature\n",
        "            )\n",
        "\n",
        "            # Update encoder\n",
        "            optimizer_encoder.zero_grad()\n",
        "            contrastive_loss.backward()\n",
        "            optimizer_encoder.step()\n",
        "\n",
        "            # Update momentum encoder\n",
        "            model._momentum_update()\n",
        "\n",
        "            # Update memory queue\n",
        "            model.memory_queue.update_queue(key_emb.detach())\n",
        "\n",
        "            contrastive_epoch_loss += contrastive_loss.item()\n",
        "\n",
        "        avg_loss = contrastive_epoch_loss / len(train_loader)\n",
        "        metrics['contrastive_losses'].append(avg_loss)\n",
        "        print(f'Pretrain Epoch {epoch+1}, Avg Loss: {avg_loss:.4f}')\n",
        "\n",
        "        # Save pretrained checkpoint\n",
        "        if (epoch + 1) % 5 == 0:\n",
        "            torch.save({\n",
        "                'epoch': epoch,\n",
        "                'model_state_dict': model.state_dict(),\n",
        "                'optimizer_encoder_state_dict': optimizer_encoder.state_dict(),\n",
        "                'loss': avg_loss,\n",
        "            }, os.path.join(save_dir, f'pretrain_checkpoint_{epoch+1}.pt'))\n",
        "\n",
        "    print(\"\\nPhase 2: Training GAN-CL...\")\n",
        "    train_epochs = 50\n",
        "    for epoch in range(train_epochs):\n",
        "        epoch_losses = {\n",
        "            'contrastive': 0,\n",
        "            'adversarial': 0,\n",
        "            'similarity': 0,\n",
        "            'total': 0\n",
        "        }\n",
        "\n",
        "        for batch in tqdm(train_loader, desc=f'Train Epoch {epoch+1}/{train_epochs}'):\n",
        "            batch = batch.to(device)\n",
        "\n",
        "            # Step 1: Train Encoder\n",
        "            optimizer_encoder.zero_grad()\n",
        "\n",
        "            # Get importance scores from generator\n",
        "            with torch.no_grad():\n",
        "                node_scores, edge_scores = model.generator(batch)\n",
        "\n",
        "            # Create perturbed graph\n",
        "            perturbed_data = model.drop_graph_elements(batch, node_scores, edge_scores)\n",
        "\n",
        "            # Get embeddings\n",
        "            query_emb = model.encoder(perturbed_data)\n",
        "            with torch.no_grad():\n",
        "                key_emb = model.momentum_encoder(batch)\n",
        "                original_emb = model.encoder(batch).detach()\n",
        "\n",
        "            # Compute losses for encoder\n",
        "            contrastive_loss = model.memory_queue.compute_contrastive_loss(\n",
        "                query_emb, key_emb, model.config.temperature\n",
        "            )\n",
        "            similarity_loss = torch.nn.functional.mse_loss(query_emb, original_emb)\n",
        "\n",
        "            # Total loss for encoder\n",
        "            encoder_loss = contrastive_loss + 0.1 * similarity_loss\n",
        "\n",
        "            # Update encoder\n",
        "            encoder_loss.backward()\n",
        "            optimizer_encoder.step()\n",
        "\n",
        "            # Update momentum encoder\n",
        "            model._momentum_update()\n",
        "\n",
        "            # Step 2: Train Generator\n",
        "            optimizer_generator.zero_grad()\n",
        "\n",
        "            # Get new embeddings for adversarial loss\n",
        "            node_scores, edge_scores = model.generator(batch)\n",
        "            perturbed_data = model.drop_graph_elements(batch, node_scores, edge_scores)\n",
        "\n",
        "            with torch.no_grad():\n",
        "                original_emb = model.encoder(batch)\n",
        "            perturbed_emb = model.encoder(perturbed_data)\n",
        "\n",
        "            # Compute adversarial loss\n",
        "            adversarial_loss = -torch.nn.functional.mse_loss(perturbed_emb, original_emb)\n",
        "\n",
        "            # Update generator\n",
        "            adversarial_loss.backward()\n",
        "            optimizer_generator.step()\n",
        "\n",
        "            # Update memory queue\n",
        "            model.memory_queue.update_queue(key_emb.detach())\n",
        "\n",
        "            # Update metrics\n",
        "            epoch_losses['contrastive'] += contrastive_loss.item()\n",
        "            epoch_losses['adversarial'] += adversarial_loss.item()\n",
        "            epoch_losses['similarity'] += similarity_loss.item()\n",
        "            epoch_losses['total'] += encoder_loss.item()\n",
        "\n",
        "        # Average losses\n",
        "        for k in epoch_losses:\n",
        "            epoch_losses[k] /= len(train_loader)\n",
        "            metrics[f'{k}_losses'].append(epoch_losses[k])\n",
        "\n",
        "        print(f'Epoch {epoch+1}, Losses: {epoch_losses}')\n",
        "\n",
        "        # Save checkpoint\n",
        "        if (epoch + 1) % 10 == 0:\n",
        "            torch.save({\n",
        "                'epoch': epoch,\n",
        "                'model_state_dict': model.state_dict(),\n",
        "                'optimizer_encoder_state_dict': optimizer_encoder.state_dict(),\n",
        "                'optimizer_generator_state_dict': optimizer_generator.state_dict(),\n",
        "                'losses': epoch_losses,\n",
        "            }, os.path.join(save_dir, f'gan_cl_checkpoint_{epoch+1}.pt'))\n",
        "\n",
        "        # Extract and save embeddings periodically\n",
        "        if (epoch + 1) % 10 == 0 or epoch == train_epochs - 1:\n",
        "            model.eval()\n",
        "            all_embeddings = []\n",
        "            all_smiles = []\n",
        "\n",
        "            with torch.no_grad():\n",
        "                for batch in train_loader:\n",
        "                    batch = batch.to(device)\n",
        "                    embeddings = model.get_embeddings(batch)\n",
        "                    all_embeddings.append(embeddings.cpu().numpy())\n",
        "\n",
        "                    # Extract SMILES for each data point\n",
        "                    for data in batch:\n",
        "                        if hasattr(data, 'smiles'):\n",
        "                            all_smiles.append(data.smiles)\n",
        "\n",
        "            # Convert to numpy arrays\n",
        "            all_embeddings = np.vstack(all_embeddings)\n",
        "\n",
        "            # Save embeddings with SMILES mapping\n",
        "            embedding_file = os.path.join(embedding_dir, f'embeddings_epoch_{epoch+1}.npz')\n",
        "            np.savez(embedding_file, embeddings=all_embeddings, smiles=all_smiles)\n",
        "\n",
        "            print(f\"Saved embeddings for epoch {epoch+1} with {len(all_smiles)} SMILES mappings\")\n",
        "\n",
        "            model.train()\n",
        "\n",
        "        # Save encoder periodically\n",
        "        if (epoch + 1) % 10 == 0:\n",
        "            epoch_info = {\n",
        "                **model_info,\n",
        "                'epoch': epoch + 1,\n",
        "                'loss': epoch_losses['total']\n",
        "            }\n",
        "            save_encoder(\n",
        "                model.encoder,\n",
        "                os.path.join(encoder_dir, f'encoder_epoch_{epoch+1}.pt'),\n",
        "                epoch_info\n",
        "            )\n",
        "\n",
        "        # Save best encoder based on total loss\n",
        "        if epoch_losses['total'] < best_loss:\n",
        "            best_loss = epoch_losses['total']\n",
        "            save_encoder(\n",
        "                model.encoder,\n",
        "                os.path.join(encoder_dir, 'best_encoder.pt'),\n",
        "                {**model_info, 'epoch': epoch + 1, 'loss': best_loss}\n",
        "            )\n",
        "\n",
        "    # Save final encoder\n",
        "    timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')\n",
        "    save_encoder(\n",
        "        model.encoder,\n",
        "        os.path.join(encoder_dir, f'final_encoder_{timestamp}.pt'),\n",
        "        {**model_info, 'epoch': train_epochs, 'loss': epoch_losses['total']}\n",
        "    )\n",
        "\n",
        "    # Extract and analyze final embeddings\n",
        "    model.eval()\n",
        "    final_embeddings = []\n",
        "    final_smiles = []\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for batch in tqdm(train_loader, desc=\"Extracting final embeddings\"):\n",
        "            batch = batch.to(device)\n",
        "            embeddings = model.get_embeddings(batch)\n",
        "            final_embeddings.append(embeddings.cpu().numpy())\n",
        "\n",
        "            # Extract SMILES for each data point\n",
        "            for data in batch:\n",
        "                if hasattr(data, 'smiles'):\n",
        "                    final_smiles.append(data.smiles)\n",
        "\n",
        "    # Convert to numpy arrays\n",
        "    final_embeddings = np.vstack(final_embeddings)\n",
        "\n",
        "    # Save final embeddings with SMILES mapping\n",
        "    final_embedding_file = os.path.join(embedding_dir, f'final_embeddings_{timestamp}.npz')\n",
        "    np.savez(final_embedding_file, embeddings=final_embeddings, smiles=final_smiles)\n",
        "\n",
        "    print(f\"Saved final embeddings with {len(final_smiles)} SMILES mappings\")\n",
        "\n",
        "    # Save final metrics\n",
        "    with open(os.path.join(save_dir, 'training_metrics.json'), 'w') as f:\n",
        "        json.dump(metrics, f)\n",
        "\n",
        "    return model, metrics, final_embeddings, final_smiles\n",
        "\n",
        "\n",
        "def save_embeddings_with_properties(embeddings, smiles_list, analyzer, output_dir, prefix=\"after_training\"):\n",
        "    \"\"\"Save embeddings with molecular properties for further analysis\n",
        "\n",
        "    Args:\n",
        "        embeddings: Numpy array of embeddings\n",
        "        smiles_list: List of SMILES strings corresponding to embeddings\n",
        "        analyzer: MolecularBiasAnalyzer instance with property data\n",
        "        output_dir: Directory to save results\n",
        "        prefix: Prefix for output filenames\n",
        "    \"\"\"\n",
        "    timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')\n",
        "\n",
        "    # Extract properties for each SMILES\n",
        "    properties_list = []\n",
        "    features_list = []\n",
        "    func_groups_list = []\n",
        "    ring_info_list = []\n",
        "    valid_indices = []\n",
        "    valid_smiles = []\n",
        "\n",
        "    for i, smiles in enumerate(smiles_list):\n",
        "        mol = Chem.MolFromSmiles(smiles)\n",
        "        if mol is None:\n",
        "            continue\n",
        "\n",
        "        valid_indices.append(i)\n",
        "        valid_smiles.append(smiles)\n",
        "\n",
        "        # Extract properties\n",
        "        properties_list.append(analyzer.extract_properties(mol))\n",
        "        features_list.append(analyzer.extract_features(mol))\n",
        "        func_groups_list.append(analyzer.extract_functional_groups(mol))\n",
        "        ring_info_list.append(analyzer.extract_ring_info(mol))\n",
        "\n",
        "    # Filter embeddings to match valid SMILES\n",
        "    valid_embeddings = embeddings[valid_indices]\n",
        "\n",
        "    # Create DataFrames\n",
        "    props_df = pd.DataFrame(properties_list, index=valid_smiles)\n",
        "    features_df = pd.DataFrame(features_list, index=valid_smiles)\n",
        "    func_groups_df = pd.DataFrame(func_groups_list, index=valid_smiles)\n",
        "\n",
        "    # Ring info requires special handling due to nested structure\n",
        "    ring_df = pd.DataFrame(index=valid_smiles)\n",
        "\n",
        "    # Flatten the ring info for DataFrame representation\n",
        "    for smiles, ring_data in zip(valid_smiles, ring_info_list):\n",
        "        for ring_type, size_dict in ring_data.items():\n",
        "            for size, count in size_dict.items():\n",
        "                ring_df.at[smiles, f\"{ring_type}_Size{size}\"] = count\n",
        "\n",
        "    # Fill NaN values with 0\n",
        "    ring_df = ring_df.fillna(0)\n",
        "\n",
        "    # Save all data\n",
        "    output_prefix = os.path.join(output_dir, f\"{prefix}_{timestamp}\")\n",
        "\n",
        "    # Save DataFrames\n",
        "    props_df.to_csv(f\"{output_prefix}_properties.csv\")\n",
        "    features_df.to_csv(f\"{output_prefix}_features.csv\")\n",
        "    func_groups_df.to_csv(f\"{output_prefix}_functional_groups.csv\")\n",
        "    ring_df.to_csv(f\"{output_prefix}_ring_info.csv\")\n",
        "\n",
        "    # Save embeddings with SMILES mapping\n",
        "    np.savez(f\"{output_prefix}_embeddings.npz\",\n",
        "             embeddings=valid_embeddings,\n",
        "             smiles=valid_smiles)\n",
        "\n",
        "    print(f\"Saved {prefix} analysis to {output_prefix}_*.csv/npz\")\n",
        "\n",
        "    return output_prefix"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "8e90813e",
      "metadata": {
        "id": "8e90813e"
      },
      "outputs": [],
      "source": [
        "def save_embeddings(embeddings, labels, filepath):\n",
        "    \"\"\"Save embeddings and corresponding labels\"\"\"\n",
        "    with open(filepath, 'wb') as f:\n",
        "        pickle.dump({\n",
        "            'embeddings': embeddings,\n",
        "            'labels': labels\n",
        "        }, f)\n",
        "\n",
        "def save_encoder(encoder, save_path, info=None):\n",
        "    \"\"\"Save encoder model for downstream tasks\"\"\"\n",
        "    save_dict = {\n",
        "        'encoder_state_dict': encoder.state_dict(),\n",
        "        'model_info': info or {}\n",
        "    }\n",
        "    torch.save(save_dict, save_path)\n",
        "\n",
        "def load_encoder(model_path, device='cpu'):\n",
        "    \"\"\"Load saved encoder model\"\"\"\n",
        "    checkpoint = torch.load(model_path, map_location=device)\n",
        "    encoder = GraphDiscriminator(\n",
        "        node_dim=checkpoint['model_info'].get('node_dim'),\n",
        "        edge_dim=checkpoint['model_info'].get('edge_dim'),\n",
        "        hidden_dim=checkpoint['model_info'].get('hidden_dim', 128),\n",
        "        output_dim=checkpoint['model_info'].get('output_dim', 128)\n",
        "    )\n",
        "    encoder.load_state_dict(checkpoint['encoder_state_dict'])\n",
        "    return encoder\n",
        "\n",
        "def train_gan_cl(train_loader, config, device='cuda',\n",
        "                save_dir='./checkpoints',\n",
        "                embedding_dir='./embeddings'):\n",
        "    \"\"\"Main training function for GAN-CL with fixed gradient computation\"\"\"\n",
        "\n",
        "    # Create directories\n",
        "    os.makedirs(save_dir, exist_ok=True)\n",
        "    os.makedirs(embedding_dir, exist_ok=True)\n",
        "    encoder_dir = os.path.join(save_dir, 'encoders')\n",
        "    os.makedirs(encoder_dir, exist_ok=True)\n",
        "\n",
        "    # Initialize model\n",
        "    model = MolecularGANCL(config).to(device)\n",
        "\n",
        "    # Initialize optimizers\n",
        "    optimizer_encoder = torch.optim.Adam(model.encoder.parameters(), lr=3e-4)\n",
        "    optimizer_generator = torch.optim.Adam(model.generator.parameters(), lr=1e-4)\n",
        "\n",
        "    # Save initial model info\n",
        "    model_info = {\n",
        "        'node_dim': config.node_dim,\n",
        "        'edge_dim': config.edge_dim,\n",
        "        'hidden_dim': config.hidden_dim,\n",
        "        'output_dim': config.output_dim,\n",
        "        'training_config': config.__dict__\n",
        "    }\n",
        "\n",
        "    # Training phases as before...\n",
        "    best_loss = float('inf')\n",
        "\n",
        "    # Training metrics\n",
        "    metrics = {\n",
        "        'contrastive_losses': [],\n",
        "        'adversarial_losses': [],\n",
        "        'similarity_losses': [],\n",
        "        'total_losses': []\n",
        "    }\n",
        "\n",
        "    # Training phases\n",
        "    print(\"Phase 1: Pretraining Contrastive Learning...\")\n",
        "    pretrain_epochs = 10\n",
        "    for epoch in range(pretrain_epochs):\n",
        "        contrastive_epoch_loss = 0\n",
        "\n",
        "        for batch in tqdm(train_loader, desc=f'Pretrain Epoch {epoch+1}/{pretrain_epochs}'):\n",
        "            batch = batch.to(device)\n",
        "\n",
        "            # Forward pass (without generator)\n",
        "            query_emb = model.encoder(batch)\n",
        "            with torch.no_grad():\n",
        "                key_emb = model.momentum_encoder(batch)\n",
        "\n",
        "            # Compute contrastive loss\n",
        "            contrastive_loss = model.memory_queue.compute_contrastive_loss(\n",
        "                query_emb, key_emb, model.config.temperature\n",
        "            )\n",
        "\n",
        "            # Update encoder\n",
        "            optimizer_encoder.zero_grad()\n",
        "            contrastive_loss.backward()\n",
        "            optimizer_encoder.step()\n",
        "\n",
        "            # Update momentum encoder\n",
        "            model._momentum_update()\n",
        "\n",
        "            # Update memory queue\n",
        "            model.memory_queue.update_queue(key_emb.detach())\n",
        "\n",
        "            contrastive_epoch_loss += contrastive_loss.item()\n",
        "\n",
        "        avg_loss = contrastive_epoch_loss / len(train_loader)\n",
        "        metrics['contrastive_losses'].append(avg_loss)\n",
        "        print(f'Pretrain Epoch {epoch+1}, Avg Loss: {avg_loss:.4f}')\n",
        "\n",
        "        # Save pretrained checkpoint\n",
        "        if (epoch + 1) % 5 == 0:\n",
        "            torch.save({\n",
        "                'epoch': epoch,\n",
        "                'model_state_dict': model.state_dict(),\n",
        "                'optimizer_encoder_state_dict': optimizer_encoder.state_dict(),\n",
        "                'loss': avg_loss,\n",
        "            }, os.path.join(save_dir, f'pretrain_checkpoint_{epoch+1}.pt'))\n",
        "\n",
        "    print(\"\\nPhase 2: Training GAN-CL...\")\n",
        "    train_epochs = 50\n",
        "#     train_epochs = 10\n",
        "    for epoch in range(train_epochs):\n",
        "        epoch_losses = {\n",
        "            'contrastive': 0,\n",
        "            'adversarial': 0,\n",
        "            'similarity': 0,\n",
        "            'total': 0\n",
        "        }\n",
        "\n",
        "        for batch in tqdm(train_loader, desc=f'Train Epoch {epoch+1}/{train_epochs}'):\n",
        "            batch = batch.to(device)\n",
        "\n",
        "            # Step 1: Train Encoder\n",
        "            optimizer_encoder.zero_grad()\n",
        "\n",
        "            # Get importance scores from generator\n",
        "            with torch.no_grad():\n",
        "                node_scores, edge_scores = model.generator(batch)\n",
        "\n",
        "            # Create perturbed graph\n",
        "            perturbed_data = model.drop_graph_elements(batch, node_scores, edge_scores)\n",
        "\n",
        "            # Get embeddings\n",
        "            query_emb = model.encoder(perturbed_data)\n",
        "            with torch.no_grad():\n",
        "                key_emb = model.momentum_encoder(batch)\n",
        "                original_emb = model.encoder(batch).detach()\n",
        "\n",
        "            # Compute losses for encoder\n",
        "            contrastive_loss = model.memory_queue.compute_contrastive_loss(\n",
        "                query_emb, key_emb, model.config.temperature\n",
        "            )\n",
        "            similarity_loss = F.mse_loss(query_emb, original_emb)\n",
        "\n",
        "            # Total loss for encoder\n",
        "            encoder_loss = contrastive_loss + 0.1 * similarity_loss\n",
        "\n",
        "            # Update encoder\n",
        "            encoder_loss.backward()\n",
        "            optimizer_encoder.step()\n",
        "\n",
        "            # Update momentum encoder\n",
        "            model._momentum_update()\n",
        "\n",
        "            # Step 2: Train Generator\n",
        "            optimizer_generator.zero_grad()\n",
        "\n",
        "            # Get new embeddings for adversarial loss\n",
        "            node_scores, edge_scores = model.generator(batch)\n",
        "            perturbed_data = model.drop_graph_elements(batch, node_scores, edge_scores)\n",
        "\n",
        "            with torch.no_grad():\n",
        "                original_emb = model.encoder(batch)\n",
        "            perturbed_emb = model.encoder(perturbed_data)\n",
        "\n",
        "            # Compute adversarial loss\n",
        "            adversarial_loss = -F.mse_loss(perturbed_emb, original_emb)\n",
        "\n",
        "            # Update generator\n",
        "            adversarial_loss.backward()\n",
        "            optimizer_generator.step()\n",
        "\n",
        "            # Update memory queue\n",
        "            model.memory_queue.update_queue(key_emb.detach())\n",
        "\n",
        "            # Update metrics\n",
        "            epoch_losses['contrastive'] += contrastive_loss.item()\n",
        "            epoch_losses['adversarial'] += adversarial_loss.item()\n",
        "            epoch_losses['similarity'] += similarity_loss.item()\n",
        "            epoch_losses['total'] += encoder_loss.item()\n",
        "\n",
        "        # Average losses\n",
        "        for k in epoch_losses:\n",
        "            epoch_losses[k] /= len(train_loader)\n",
        "            metrics[f'{k}_losses'].append(epoch_losses[k])\n",
        "\n",
        "        print(f'Epoch {epoch+1}, Losses: {epoch_losses}')\n",
        "\n",
        "        # Save checkpoint\n",
        "        if (epoch + 1) % 10 == 0:\n",
        "            torch.save({\n",
        "                'epoch': epoch,\n",
        "                'model_state_dict': model.state_dict(),\n",
        "                'optimizer_encoder_state_dict': optimizer_encoder.state_dict(),\n",
        "                'optimizer_generator_state_dict': optimizer_generator.state_dict(),\n",
        "                'losses': epoch_losses,\n",
        "            }, os.path.join(save_dir, f'gan_cl_checkpoint_{epoch+1}.pt'))\n",
        "\n",
        "        # Extract and save embeddings periodically\n",
        "        if (epoch + 1) % 10 == 0:\n",
        "            model.eval()\n",
        "            all_embeddings = []\n",
        "            all_graphs = []\n",
        "\n",
        "            with torch.no_grad():\n",
        "                for batch in train_loader:\n",
        "                    batch = batch.to(device)\n",
        "                    embeddings = model.get_embeddings(batch)\n",
        "                    all_embeddings.append(embeddings.cpu())\n",
        "                    all_graphs.extend([data for data in batch])\n",
        "\n",
        "            all_embeddings = torch.cat(all_embeddings, dim=0)\n",
        "\n",
        "            # Save embeddings\n",
        "#             timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')\n",
        "            save_embeddings(\n",
        "                all_embeddings.numpy(),\n",
        "                all_graphs,\n",
        "                os.path.join(embedding_dir, f'embeddings_epoch_{epoch+1}_{timestamp}.pkl')\n",
        "            )\n",
        "\n",
        "            model.train()\n",
        "\n",
        "    # Save final metrics\n",
        "    with open(os.path.join(save_dir, 'training_metrics.json'), 'w') as f:\n",
        "        json.dump(metrics, f)\n",
        "\n",
        "            # Save encoder periodically\n",
        "        if (epoch + 1) % 10 == 0:\n",
        "            epoch_info = {\n",
        "                **model_info,\n",
        "                'epoch': epoch + 1,\n",
        "                'loss': epoch_losses['total']\n",
        "            }\n",
        "            save_encoder(\n",
        "                model.encoder,\n",
        "                os.path.join(encoder_dir, f'encoder_epoch_{epoch+1}.pt'),\n",
        "                epoch_info\n",
        "            )\n",
        "\n",
        "        # Save best encoder based on total loss\n",
        "        if epoch_losses['total'] < best_loss:\n",
        "            best_loss = epoch_losses['total']\n",
        "            save_encoder(\n",
        "                model.encoder,\n",
        "                os.path.join(encoder_dir, f'best_encoder_{timestamp}.pt'),\n",
        "                {**model_info, 'epoch': epoch + 1, 'loss': best_loss}\n",
        "            )\n",
        "\n",
        "    # Save final encoder\n",
        "    save_encoder(\n",
        "        model.encoder,\n",
        "        os.path.join(encoder_dir, f'final_encoder_{timestamp}.pt'),\n",
        "        {**model_info, 'epoch': train_epochs, 'loss': epoch_losses['total']}\n",
        "    )\n",
        "\n",
        "    return model, metrics\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "4dbc0517",
      "metadata": {
        "scrolled": false,
        "id": "4dbc0517"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import numpy as np\n",
        "from torch_geometric.data import DataLoader\n",
        "import os\n",
        "import json\n",
        "import pickle\n",
        "from tqdm import tqdm\n",
        "from datetime import datetime\n",
        "from rdkit import Chem\n",
        "import pandas as pd\n",
        "\n",
        "\n",
        "def main():\n",
        "    \"\"\"Main function with integrated bias analysis\"\"\"\n",
        "    # Enable anomaly detection during development\n",
        "    torch.autograd.set_detect_anomaly(True)\n",
        "    # Your existing data loading code here\n",
        "    torch.manual_seed(42)\n",
        "    np.random.seed(42)\n",
        "\n",
        "    print(\"Starting data loading...\")\n",
        "    extractor = MolecularFeatureExtractor()\n",
        "    smiles_file = \"D:\\\\PhD\\\\Chapter3\\\\Unsupervised_GAN_Code\\\\pubchem-41-clean.txt\"\n",
        "\n",
        "    dataset = []\n",
        "    failed_smiles = []\n",
        "\n",
        "    with open(smiles_file, 'r') as f:\n",
        "        for i, line in enumerate(f):\n",
        "            smiles = line.strip()\n",
        "            data = extractor.process_molecule(smiles)\n",
        "            if data is not None:\n",
        "                # Store original SMILES in the data object\n",
        "                data.smiles = smiles\n",
        "                dataset.append(data)\n",
        "            else:\n",
        "                failed_smiles.append(smiles)\n",
        "\n",
        "            # Limit dataset size for testing\n",
        "            if i >= 10000:  # Adjust as needed\n",
        "                break\n",
        "\n",
        "    print(f\"1. Loaded dataset with {len(dataset)} graphs.\")\n",
        "    print(f\"2. Failed SMILES count: {len(failed_smiles)}\")\n",
        "\n",
        "    if not dataset:\n",
        "        print(\"No valid graphs generated.\")\n",
        "        return None\n",
        "\n",
        "    # Setup training\n",
        "    batch_size = 32\n",
        "    train_loader = DataLoader(dataset, batch_size=batch_size, shuffle=True)\n",
        "    print(f\"3. Created DataLoader with {len(train_loader.dataset)} graphs\")\n",
        "\n",
        "    # Set device\n",
        "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "    print(f\"4. Using device: {device}\")\n",
        "\n",
        "    # Get configuration based on dataset\n",
        "    config = get_model_config(dataset)\n",
        "\n",
        "    # Train model with bias analysis\n",
        "    print(\"5. Starting GAN-CL training with bias analysis...\")\n",
        "    model, metrics, final_embeddings, final_smiles = modified_train_gan_cl(\n",
        "        train_loader,\n",
        "        config,\n",
        "        device=device,\n",
        "        save_dir='./checkpoints',\n",
        "        embedding_dir='./embeddings'\n",
        "    )\n",
        "\n",
        "    print(\"6. Training completed!\")\n",
        "\n",
        "    # At this point, we've already saved and analyzed the embeddings during training\n",
        "    # But let's do one final sanity check to make sure the analysis was completed\n",
        "\n",
        "    # Check if after_training_*.csv files exist\n",
        "    analysis_dir = './analysis'\n",
        "    found_analysis = False\n",
        "\n",
        "    for filename in os.listdir(analysis_dir):\n",
        "        if filename.startswith('after_training_') and filename.endswith('.csv'):\n",
        "            found_analysis = True\n",
        "            break\n",
        "\n",
        "    if not found_analysis:\n",
        "        print(\"7. Warning: After-training analysis files not found!\")\n",
        "        print(\"   Running final analysis on embeddings...\")\n",
        "\n",
        "        # Instead of importing, use the analyze_smiles_list function directly\n",
        "        # This function should be defined in your script or properly imported\n",
        "\n",
        "        # Create a mapping from embeddings to SMILES\n",
        "        valid_smiles = []\n",
        "        for smiles in final_smiles:\n",
        "            if Chem.MolFromSmiles(smiles) is not None:\n",
        "                valid_smiles.append(smiles)\n",
        "\n",
        "        # Use analyze_smiles_list function (this should be defined elsewhere in your code)\n",
        "        props_df, features_df, func_groups_df, ring_df = analyze_smiles_list(\n",
        "            valid_smiles, output_dir=analysis_dir, prefix=\"after_training\")\n",
        "\n",
        "        print(f\"8. Final analysis completed with {len(valid_smiles)} valid molecules\")\n",
        "    else:\n",
        "        print(\"7. After-training analysis found!\")\n",
        "\n",
        "    print(\"8. Analysis complete. The data can now be used for visualization and comparison.\")\n",
        "\n",
        "    return model, metrics, final_embeddings, final_smiles\n",
        "\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    model, metrics, embeddings, smiles = main()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "27d928ef",
      "metadata": {
        "id": "27d928ef"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.21"
    },
    "colab": {
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}