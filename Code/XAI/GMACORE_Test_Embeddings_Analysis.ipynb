{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "5379f427",
      "metadata": {
        "id": "5379f427"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import numpy as np\n",
        "import random\n",
        "import os\n",
        "import json\n",
        "import pickle\n",
        "import copy\n",
        "import pandas as pd\n",
        "from tqdm import tqdm\n",
        "from datetime import datetime\n",
        "from typing import Tuple, List, Dict, Any, Optional\n",
        "from collections import defaultdict\n",
        "from dataclasses import dataclass\n",
        "\n",
        "from rdkit import Chem, RDLogger\n",
        "from rdkit.Chem import (\n",
        "    AllChem, Descriptors, MolSurf, Fragments, Lipinski, RemoveHs\n",
        ")\n",
        "from rdkit.Chem.rdMolDescriptors import (\n",
        "    CalcNumRings, CalcNumAromaticRings, CalcNumHeterocycles,\n",
        "    CalcNumAliphaticRings, CalcTPSA\n",
        ")\n",
        "\n",
        "from torch_geometric.data import Data, DataLoader\n",
        "from torch_geometric.nn import GCNConv, global_mean_pool, MessagePassing\n",
        "\n",
        "# Suppress RDKit warnings\n",
        "RDLogger.DisableLog('rdApp.warning')\n",
        "\n",
        "# Working timestamp\n",
        "timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')\n",
        "\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import torch\n",
        "from rdkit import Chem\n",
        "from rdkit.Chem import Descriptors, Fragments, Lipinski\n",
        "from rdkit.Chem.rdMolDescriptors import CalcNumRings, CalcNumAromaticRings, CalcNumHeterocycles\n",
        "from rdkit.Chem.rdMolDescriptors import CalcNumAliphaticRings, CalcTPSA\n",
        "from typing import List, Dict, Any, Tuple\n",
        "import os\n",
        "import json\n",
        "import pickle\n",
        "from tqdm import tqdm\n",
        "from datetime import datetime\n",
        "from collections import defaultdict\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "8de17bc6",
      "metadata": {
        "id": "8de17bc6"
      },
      "outputs": [],
      "source": [
        "class MolecularFeatureExtractor:\n",
        "    def __init__(self):\n",
        "        self.atom_list = list(range(1, 119))\n",
        "        self.chirality_list = [\n",
        "            Chem.rdchem.ChiralType.CHI_UNSPECIFIED,\n",
        "            Chem.rdchem.ChiralType.CHI_TETRAHEDRAL_CW,\n",
        "            Chem.rdchem.ChiralType.CHI_TETRAHEDRAL_CCW,\n",
        "            Chem.rdchem.ChiralType.CHI_OTHER\n",
        "        ]\n",
        "        self.bond_list = [\n",
        "            Chem.rdchem.BondType.SINGLE,\n",
        "            Chem.rdchem.BondType.DOUBLE,\n",
        "            Chem.rdchem.BondType.TRIPLE,\n",
        "            Chem.rdchem.BondType.AROMATIC\n",
        "        ]\n",
        "        self.bonddir_list = [\n",
        "            Chem.rdchem.BondDir.NONE,\n",
        "            Chem.rdchem.BondDir.ENDUPRIGHT,\n",
        "            Chem.rdchem.BondDir.ENDDOWNRIGHT\n",
        "        ]\n",
        "\n",
        "    def calc_atom_features(self, atom: Chem.Atom) -> Tuple[list, list]:\n",
        "        \"\"\"Calculate atom features with better error handling\"\"\"\n",
        "        try:\n",
        "            # Basic features\n",
        "            atom_feat = [\n",
        "                self.atom_list.index(atom.GetAtomicNum()),\n",
        "                self.chirality_list.index(atom.GetChiralTag())\n",
        "            ]\n",
        "\n",
        "            # Physical features with error handling\n",
        "            phys_feat = []\n",
        "\n",
        "            # Molecular weight contribution\n",
        "            try:\n",
        "                contrib_mw = Descriptors.ExactMolWt(Chem.MolFromSmiles(f'[{atom.GetSymbol()}]'))\n",
        "                phys_feat.append(contrib_mw)\n",
        "            except:\n",
        "                phys_feat.append(0.0)\n",
        "\n",
        "            # LogP contribution\n",
        "            try:\n",
        "                contrib_logp = Descriptors.MolLogP(Chem.MolFromSmiles(f'[{atom.GetSymbol()}]'))\n",
        "                phys_feat.append(contrib_logp)\n",
        "            except:\n",
        "                phys_feat.append(0.0)\n",
        "\n",
        "            # Add other physical properties\n",
        "            phys_feat.extend([\n",
        "                atom.GetFormalCharge(),\n",
        "                int(atom.GetHybridization()),\n",
        "                int(atom.GetIsAromatic()),\n",
        "                atom.GetTotalNumHs(),\n",
        "                atom.GetTotalValence(),\n",
        "                atom.GetDegree()\n",
        "            ])\n",
        "\n",
        "            return atom_feat, phys_feat\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"Error calculating atom features: {e}\")\n",
        "            return [0, 0], [0.0] * 9\n",
        "\n",
        "    def get_atom_features(self, mol: Chem.Mol) -> Tuple[torch.Tensor, torch.Tensor]:\n",
        "        \"\"\"Extract atom features for the whole molecule\"\"\"\n",
        "        atom_feats = []\n",
        "        phys_feats = []\n",
        "\n",
        "        if mol is None:\n",
        "            return torch.tensor([[0, 0]], dtype=torch.long), torch.tensor([[0.0] * 9], dtype=torch.float)\n",
        "\n",
        "        for atom in mol.GetAtoms():\n",
        "            atom_feat, phys_feat = self.calc_atom_features(atom)\n",
        "            atom_feats.append(atom_feat)\n",
        "            phys_feats.append(phys_feat)\n",
        "\n",
        "        x = torch.tensor(atom_feats, dtype=torch.long)\n",
        "        phys = torch.tensor(phys_feats, dtype=torch.float)\n",
        "\n",
        "        return x, phys\n",
        "\n",
        "    def remove_unbonded_hydrogens(mol):\n",
        "        params = Chem.RemoveHsParameters()\n",
        "        params.removeDegreeZero = True\n",
        "        mol = Chem.RemoveHs(mol, params)\n",
        "        return mol\n",
        "\n",
        "\n",
        "    def get_bond_features(self, mol: Chem.Mol) -> Tuple[torch.Tensor, torch.Tensor]:\n",
        "        \"\"\"Extract bond features with better error handling\"\"\"\n",
        "        if mol is None:\n",
        "            return torch.tensor([[0], [0]], dtype=torch.long), torch.tensor([[0.0] * 5], dtype=torch.float)\n",
        "\n",
        "        row, col, edge_feat = [], [], []\n",
        "\n",
        "        for bond in mol.GetBonds():\n",
        "            try:\n",
        "                start, end = bond.GetBeginAtomIdx(), bond.GetEndAtomIdx()\n",
        "\n",
        "                # Add edges in both directions\n",
        "                row += [start, end]\n",
        "                col += [end, start]\n",
        "\n",
        "                # Bond features\n",
        "                bond_type = self.bond_list.index(bond.GetBondType())\n",
        "                bond_dir = self.bonddir_list.index(bond.GetBondDir())\n",
        "\n",
        "                # Calculate additional properties\n",
        "                feat = [\n",
        "                    bond_type,\n",
        "                    bond_dir,\n",
        "                    int(bond.GetIsConjugated()),\n",
        "                    int(self._is_rotatable(bond)),\n",
        "                    self._get_bond_length(mol, start, end)\n",
        "                ]\n",
        "\n",
        "                edge_feat.extend([feat, feat])\n",
        "\n",
        "            except Exception as e:\n",
        "                print(f\"Error processing bond: {e}\")\n",
        "                continue\n",
        "\n",
        "        if not row:  # If no valid bonds were processed\n",
        "            return torch.tensor([[0], [0]], dtype=torch.long), torch.tensor([[0.0] * 5], dtype=torch.float)\n",
        "\n",
        "        edge_index = torch.tensor([row, col], dtype=torch.long)\n",
        "        edge_attr = torch.tensor(edge_feat, dtype=torch.float)\n",
        "\n",
        "        return edge_index, edge_attr\n",
        "\n",
        "    def _is_rotatable(self, bond: Chem.Bond) -> bool:\n",
        "        \"\"\"Check if bond is rotatable\"\"\"\n",
        "        return (bond.GetBondType() == Chem.rdchem.BondType.SINGLE and\n",
        "                not bond.IsInRing() and\n",
        "                len(bond.GetBeginAtom().GetNeighbors()) > 1 and\n",
        "                len(bond.GetEndAtom().GetNeighbors()) > 1)\n",
        "\n",
        "    def _get_bond_length(self, mol: Chem.Mol, start: int, end: int) -> float:\n",
        "        \"\"\"Get bond length with error handling\"\"\"\n",
        "        try:\n",
        "            conf = mol.GetConformer()\n",
        "            if conf.Is3D():\n",
        "                return Chem.rdMolTransforms.GetBondLength(conf, start, end)\n",
        "        except:\n",
        "            pass\n",
        "        return 0.0\n",
        "\n",
        "    def process_molecule(self, smiles: str) -> Data:\n",
        "        \"\"\"Process SMILES string to graph data\"\"\"\n",
        "        try:\n",
        "            mol = Chem.MolFromSmiles(smiles)\n",
        "            if mol is None:\n",
        "                print(f\"Invalid SMILES: {smiles}\")\n",
        "                return None  # Skip invalid molecules\n",
        "            mol = RemoveHs(mol)\n",
        "\n",
        "            # Add explicit hydrogens\n",
        "            mol = Chem.AddHs(mol, addCoords=True)\n",
        "\n",
        "            # Sanitize molecule\n",
        "            Chem.SanitizeMol(mol)\n",
        "\n",
        "            # Check if the molecule has atoms\n",
        "            if mol.GetNumAtoms() == 0:\n",
        "                print(\"Molecule has no atoms, skipping.\")\n",
        "                return None\n",
        "\n",
        "            # Generate 3D coordinates\n",
        "            if not mol.GetNumConformers():\n",
        "                status = AllChem.EmbedMolecule(mol, AllChem.ETKDG())\n",
        "                if status != 0:\n",
        "                    print(\"Failed to generate 3D conformer\")\n",
        "                    return None  # Skip failed molecules\n",
        "\n",
        "                # Try MMFF or UFF optimization\n",
        "                try:\n",
        "                    AllChem.MMFFOptimizeMolecule(mol)\n",
        "                except:\n",
        "                    AllChem.UFFOptimizeMolecule(mol)\n",
        "\n",
        "            # Extract features\n",
        "            x_cat, x_phys = self.get_atom_features(mol)\n",
        "            edge_index, edge_attr = self.get_bond_features(mol)\n",
        "\n",
        "            return Data(\n",
        "                x_cat=x_cat,\n",
        "                x_phys=x_phys,\n",
        "                edge_index=edge_index,\n",
        "                edge_attr=edge_attr,\n",
        "                num_nodes=x_cat.size(0)\n",
        "            )\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"Error processing molecule {smiles}: {e}\")\n",
        "            return None\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "35a311ed",
      "metadata": {
        "id": "35a311ed"
      },
      "outputs": [],
      "source": [
        "class MemoryQueue:\n",
        "    \"\"\"Memory queue with temporal decay for contrastive learning\"\"\"\n",
        "    def __init__(self, size: int, dim: int, decay: float = 0.99999):\n",
        "        self.size = size\n",
        "        self.dim = dim\n",
        "        self.decay = decay\n",
        "        self.ptr = 0\n",
        "        self.full = False\n",
        "\n",
        "        # Initialize queue\n",
        "        self.queue = nn.Parameter(F.normalize(torch.randn(size, dim), dim=1), requires_grad=False)\n",
        "        self.queue_age = nn.Parameter(torch.zeros(size), requires_grad=False)\n",
        "\n",
        "#         self.register_buffer(\"queue\", torch.randn(size, dim))\n",
        "#         self.register_buffer(\"queue_age\", torch.zeros(size))  # Track age of each entry\n",
        "        self.queue = F.normalize(self.queue, dim=1)\n",
        "\n",
        "    def update_queue(self, keys: torch.Tensor):\n",
        "        \"\"\"Update queue with new keys\"\"\"\n",
        "        batch_size = keys.shape[0]\n",
        "\n",
        "        # Increment age of all entries\n",
        "        self.queue_age += 1\n",
        "\n",
        "        # Add new keys\n",
        "        if self.ptr + batch_size <= self.size:\n",
        "            self.queue[self.ptr:self.ptr + batch_size] = keys\n",
        "            self.queue_age[self.ptr:self.ptr + batch_size] = 0\n",
        "        else:\n",
        "            # Handle overflow\n",
        "            rem = self.size - self.ptr\n",
        "            self.queue[self.ptr:] = keys[:rem]\n",
        "            self.queue[:batch_size-rem] = keys[rem:]\n",
        "            self.queue_age[self.ptr:] = 0\n",
        "            self.queue_age[:batch_size-rem] = 0\n",
        "            self.full = True\n",
        "\n",
        "        self.ptr = (self.ptr + batch_size) % self.size\n",
        "\n",
        "    def get_decay_weights(self) -> torch.Tensor:\n",
        "        \"\"\"Get temporal decay weights for queue entries\"\"\"\n",
        "        return self.decay ** self.queue_age\n",
        "\n",
        "    def compute_contrastive_loss(self, query: torch.Tensor, positive_key: torch.Tensor,\n",
        "                                temperature: float = 0.07) -> torch.Tensor:\n",
        "        \"\"\"Compute contrastive loss with temporal decay\"\"\"\n",
        "        # Normalize embeddings\n",
        "        query = F.normalize(query, dim=1)\n",
        "        positive_key = F.normalize(positive_key, dim=1)\n",
        "        queue = F.normalize(self.queue, dim=1)\n",
        "\n",
        "        # Compute logits\n",
        "        l_pos = torch.einsum('nc,nc->n', [query, positive_key]).unsqueeze(-1)\n",
        "        l_neg = torch.einsum('nc,ck->nk', [query, queue.T])\n",
        "\n",
        "        # Apply temporal decay to negative samples\n",
        "        decay_weights = self.get_decay_weights()\n",
        "        l_neg = l_neg * decay_weights.unsqueeze(0)\n",
        "\n",
        "        # Temperature scaling\n",
        "        logits = torch.cat([l_pos, l_neg], dim=1) / temperature\n",
        "        labels = torch.zeros(logits.shape[0], dtype=torch.long, device=query.device)\n",
        "\n",
        "        return F.cross_entropy(logits, labels)\n",
        "\n",
        "class GraphGenerator(nn.Module):\n",
        "    \"\"\"Generator network with proper feature handling\"\"\"\n",
        "    def __init__(self, node_dim: int, edge_dim: int, hidden_dim: int = 128):\n",
        "        super().__init__()\n",
        "\n",
        "        # Node feature processing\n",
        "        self.node_encoder = nn.Sequential(\n",
        "            nn.Linear(node_dim, hidden_dim),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(hidden_dim, hidden_dim)\n",
        "        )\n",
        "\n",
        "        # Edge feature processing\n",
        "        self.edge_encoder = nn.Sequential(\n",
        "            nn.Linear(edge_dim, hidden_dim),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(hidden_dim, hidden_dim)\n",
        "        )\n",
        "\n",
        "        # Graph convolution layers\n",
        "        self.conv1 = GCNConv(hidden_dim, hidden_dim)\n",
        "        self.conv2 = GCNConv(hidden_dim, hidden_dim)\n",
        "        self.conv3 = GCNConv(hidden_dim, hidden_dim)\n",
        "\n",
        "        # Importance prediction layers\n",
        "        self.node_importance = nn.Sequential(\n",
        "            nn.Linear(hidden_dim, hidden_dim // 2),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(hidden_dim // 2, 1),\n",
        "            nn.Sigmoid()\n",
        "        )\n",
        "\n",
        "        self.edge_importance = nn.Sequential(\n",
        "            nn.Linear(hidden_dim * 2, hidden_dim),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(hidden_dim, 1),\n",
        "            nn.Sigmoid()\n",
        "        )\n",
        "\n",
        "    def normalize_features(self, x_cat, x_phys):\n",
        "        \"\"\"Normalize categorical and physical features separately\"\"\"\n",
        "        # Convert categorical features to one-hot\n",
        "        x_cat = x_cat.float()\n",
        "\n",
        "        # Normalize physical features\n",
        "        x_phys = x_phys.float()\n",
        "        if x_phys.size(0) > 1:  # Only normalize if we have more than one sample\n",
        "            x_phys = (x_phys - x_phys.mean(0)) / (x_phys.std(0) + 1e-5)\n",
        "\n",
        "        return x_cat, x_phys\n",
        "\n",
        "    def forward(self, data) -> Tuple[torch.Tensor, torch.Tensor]:\n",
        "        # Normalize features\n",
        "        x_cat, x_phys = self.normalize_features(data.x_cat, data.x_phys)\n",
        "\n",
        "        # Concatenate features\n",
        "        x = torch.cat([x_cat, x_phys], dim=-1)\n",
        "\n",
        "        edge_index = data.edge_index\n",
        "        edge_attr = data.edge_attr.float()  # Ensure float type\n",
        "\n",
        "        # Initial feature encoding\n",
        "        x = self.node_encoder(x)\n",
        "        edge_attr = self.edge_encoder(edge_attr)\n",
        "\n",
        "        # Graph convolutions\n",
        "        x = F.relu(self.conv1(x, edge_index))  # Removed edge_attr from GCNConv\n",
        "        x = F.relu(self.conv2(x, edge_index))\n",
        "        x = self.conv3(x, edge_index)\n",
        "\n",
        "        # Predict importance scores\n",
        "        node_scores = self.node_importance(x)\n",
        "\n",
        "        # Edge scores using both connected nodes\n",
        "        edge_features = torch.cat([\n",
        "            x[edge_index[0]],\n",
        "            x[edge_index[1]]\n",
        "        ], dim=-1)\n",
        "        edge_scores = self.edge_importance(edge_features)\n",
        "\n",
        "        return node_scores, edge_scores\n",
        "\n",
        "def get_model_config(dataset):\n",
        "    \"\"\"Get model configuration based on dataset features\"\"\"\n",
        "    sample_data = dataset[0]\n",
        "\n",
        "    # Calculate input dimensions\n",
        "    node_dim = sample_data.x_cat.shape[1] + sample_data.x_phys.shape[1]\n",
        "    edge_dim = sample_data.edge_attr.shape[1]\n",
        "\n",
        "    config = GanClConfig(\n",
        "        node_dim=node_dim,\n",
        "        edge_dim=edge_dim,\n",
        "        hidden_dim=128,\n",
        "        output_dim=128,\n",
        "        queue_size=65536,\n",
        "        momentum=0.999,\n",
        "        temperature=0.07,\n",
        "        decay=0.99999,\n",
        "        dropout_ratio=0.25\n",
        "    )\n",
        "\n",
        "    return config\n",
        "\n",
        "class GraphDiscriminator(nn.Module):\n",
        "    \"\"\"Discriminator/Encoder network\"\"\"\n",
        "    def __init__(self, node_dim: int, edge_dim: int, hidden_dim: int = 128, output_dim: int = 128):\n",
        "        super().__init__()\n",
        "\n",
        "        # Feature encoding\n",
        "        self.node_encoder = nn.Sequential(\n",
        "            nn.Linear(node_dim, hidden_dim),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(hidden_dim, hidden_dim)\n",
        "        )\n",
        "\n",
        "        self.edge_encoder = nn.Sequential(\n",
        "            nn.Linear(edge_dim, hidden_dim),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(hidden_dim, hidden_dim)\n",
        "        )\n",
        "\n",
        "        # Graph convolution layers\n",
        "        self.conv1 = GCNConv(hidden_dim, hidden_dim)\n",
        "        self.conv2 = GCNConv(hidden_dim, hidden_dim)\n",
        "        self.conv3 = GCNConv(hidden_dim, output_dim)\n",
        "\n",
        "        # Projection head for contrastive learning\n",
        "        self.projection = nn.Sequential(\n",
        "            nn.Linear(output_dim, hidden_dim),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(hidden_dim, output_dim)\n",
        "        )\n",
        "\n",
        "    def normalize_features(self, x_cat, x_phys):\n",
        "        \"\"\"Normalize categorical and physical features separately\"\"\"\n",
        "        # Convert categorical features to one-hot\n",
        "        x_cat = x_cat.float()\n",
        "\n",
        "        # Normalize physical features\n",
        "        x_phys = x_phys.float()\n",
        "        if x_phys.size(0) > 1:  # Only normalize if we have more than one sample\n",
        "            x_phys = (x_phys - x_phys.mean(0)) / (x_phys.std(0) + 1e-5)\n",
        "\n",
        "        return x_cat, x_phys\n",
        "\n",
        "    def forward(self, data):\n",
        "        # Normalize features\n",
        "        x_cat, x_phys = self.normalize_features(data.x_cat, data.x_phys)\n",
        "\n",
        "        # Concatenate features\n",
        "        x = torch.cat([x_cat, x_phys], dim=-1)\n",
        "\n",
        "        edge_index = data.edge_index\n",
        "        edge_attr = data.edge_attr.float()  # Ensure float type\n",
        "        batch = data.batch\n",
        "\n",
        "        # Initial feature encoding\n",
        "        x = self.node_encoder(x)\n",
        "        edge_attr = self.edge_encoder(edge_attr)\n",
        "\n",
        "        # Graph convolutions\n",
        "        x = F.relu(self.conv1(x, edge_index))  # Removed edge_attr from GCNConv\n",
        "        x = F.relu(self.conv2(x, edge_index))\n",
        "        x = self.conv3(x, edge_index)\n",
        "\n",
        "        # Global pooling\n",
        "        x = global_mean_pool(x, batch)\n",
        "\n",
        "        # Projection\n",
        "        x = self.projection(x)\n",
        "\n",
        "        return x\n",
        "\n",
        "@dataclass\n",
        "class GanClConfig:\n",
        "    \"\"\"Configuration for GAN-CL training\"\"\"\n",
        "    node_dim: int\n",
        "    edge_dim: int\n",
        "    hidden_dim: int = 128\n",
        "    output_dim: int = 128\n",
        "    queue_size: int = 65536\n",
        "    momentum: float = 0.999\n",
        "    temperature: float = 0.07\n",
        "    decay: float = 0.99999\n",
        "    dropout_ratio: float = 0.25\n",
        "\n",
        "class MolecularGANCL(nn.Module):\n",
        "    \"\"\"Combined GAN and Contrastive Learning framework\"\"\"\n",
        "    def __init__(self, config: GanClConfig):\n",
        "        super().__init__()\n",
        "        self.config = config\n",
        "\n",
        "        # Add weight initialization\n",
        "        def init_weights(m):\n",
        "            if isinstance(m, nn.Linear):\n",
        "                torch.nn.init.xavier_uniform_(m.weight)\n",
        "                m.bias.data.fill_(0.01)\n",
        "\n",
        "        # Initialize networks\n",
        "        self.generator = GraphGenerator(\n",
        "            config.node_dim,\n",
        "            config.edge_dim,\n",
        "            config.hidden_dim * 2\n",
        "        )\n",
        "\n",
        "        self.encoder = GraphDiscriminator(\n",
        "            config.node_dim,\n",
        "            config.edge_dim,\n",
        "            config.hidden_dim,\n",
        "            config.output_dim\n",
        "        )\n",
        "        self.encoder.apply(init_weights)\n",
        "\n",
        "        # Modified loss weights\n",
        "        self.contrastive_weight = 1.0\n",
        "        self.adversarial_weight = 0.1  # Increased from 0.05\n",
        "        self.similarity_weight = 0.01  # Decreased from 0.1\n",
        "\n",
        "        # Temperature annealing\n",
        "        self.initial_temperature = 0.1\n",
        "        self.min_temperature = 0.05\n",
        "\n",
        "        # Create momentum encoder\n",
        "        self.momentum_encoder = copy.deepcopy(self.encoder)\n",
        "        for param in self.momentum_encoder.parameters():\n",
        "            param.requires_grad = False\n",
        "\n",
        "        # Initialize memory queue\n",
        "        self.memory_queue = MemoryQueue(\n",
        "            config.queue_size,\n",
        "            config.output_dim,\n",
        "            config.decay\n",
        "        )\n",
        "\n",
        "    @torch.no_grad()\n",
        "    def _momentum_update(self):\n",
        "        \"\"\"Update momentum encoder\"\"\"\n",
        "        for param_q, param_k in zip(self.encoder.parameters(),\n",
        "                                  self.momentum_encoder.parameters()):\n",
        "            param_k.data = self.config.momentum * param_k.data + \\\n",
        "                          (1 - self.config.momentum) * param_q.data\n",
        "\n",
        "    def drop_graph_elements(self, data, node_scores: torch.Tensor,\n",
        "                          edge_scores: torch.Tensor) -> Data:\n",
        "        \"\"\"Apply dropout to graph based on importance scores\"\"\"\n",
        "        # Select elements to keep based on scores and dropout ratio\n",
        "#         node_mask = (node_scores < self.config.dropout_ratio).float()\n",
        "#         edge_mask = (edge_scores < self.config.dropout_ratio).float()\n",
        "\n",
        "        node_mask = (torch.rand_like(node_scores) > self.config.dropout_ratio).float()\n",
        "        edge_mask = (torch.rand_like(edge_scores) > self.config.dropout_ratio).float()\n",
        "\n",
        "        # Apply masks\n",
        "        x_cat_new = data.x_cat * node_mask\n",
        "        x_phys_new = data.x_phys * node_mask\n",
        "        edge_attr_new = data.edge_attr * edge_mask\n",
        "\n",
        "        # Create new graph data object\n",
        "        return Data(\n",
        "            x_cat=x_cat_new,\n",
        "            x_phys=x_phys_new,\n",
        "            edge_index=data.edge_index,\n",
        "            edge_attr=edge_attr_new,\n",
        "            batch=data.batch\n",
        "        )\n",
        "\n",
        "    def get_temperature(self, epoch, total_epochs):\n",
        "        \"\"\"Anneal temperature during training\"\"\"\n",
        "        progress = epoch / total_epochs\n",
        "        return max(self.initial_temperature * (1 - progress), self.min_temperature)\n",
        "\n",
        "    def forward(self, data, epoch=0, total_epochs=50):\n",
        "        # Get current temperature\n",
        "        temperature = self.get_temperature(epoch, total_epochs)\n",
        "\n",
        "        # Get importance scores from generator\n",
        "        node_scores, edge_scores = self.generator(data)\n",
        "\n",
        "        # Create perturbed graph\n",
        "        perturbed_data = self.drop_graph_elements(data, node_scores, edge_scores)\n",
        "\n",
        "        # Get embeddings\n",
        "        query_emb = self.encoder(perturbed_data)\n",
        "        with torch.no_grad():\n",
        "            key_emb = self.momentum_encoder(data)\n",
        "            original_emb = self.encoder(data).detach()\n",
        "\n",
        "        # Compute losses with modified weights\n",
        "        contrastive_loss = self.memory_queue.compute_contrastive_loss(\n",
        "            query_emb, key_emb, temperature\n",
        "        ) * self.contrastive_weight\n",
        "\n",
        "        adversarial_loss = -F.mse_loss(query_emb, original_emb) * self.adversarial_weight\n",
        "        similarity_loss = F.mse_loss(query_emb, original_emb) * self.similarity_weight\n",
        "\n",
        "        return contrastive_loss, adversarial_loss, similarity_loss\n",
        "\n",
        "    def get_embeddings(self, data) -> torch.Tensor:\n",
        "        \"\"\"Get embeddings for downstream tasks\"\"\"\n",
        "        with torch.no_grad():\n",
        "            return self.encoder(data)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "f5dcc705",
      "metadata": {
        "id": "f5dcc705"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "c83a055b",
      "metadata": {
        "id": "c83a055b"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import torch\n",
        "from rdkit import Chem\n",
        "from tqdm import tqdm\n",
        "import os\n",
        "from datetime import datetime\n",
        "import json\n",
        "\n",
        "\n",
        "class SMILESTracker:\n",
        "    \"\"\"A simplified tracker that stores original SMILES strings during training\"\"\"\n",
        "\n",
        "    def __init__(self, output_dir='./embeddings'):\n",
        "        \"\"\"Initialize the tracker\"\"\"\n",
        "        self.output_dir = output_dir\n",
        "        os.makedirs(output_dir, exist_ok=True)\n",
        "        self.timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')\n",
        "        self.original_smiles = {}  # Maps dataset index to SMILES\n",
        "        self.embeddings = []\n",
        "        self.batch_indices = []\n",
        "\n",
        "    def store_dataset_smiles(self, train_loader):\n",
        "        \"\"\"Extract and store all SMILES strings from the dataset\n",
        "\n",
        "        This should be called once at the start of training\n",
        "        \"\"\"\n",
        "        print(\"Storing original SMILES strings from dataset...\")\n",
        "        dataset_idx = 0\n",
        "\n",
        "        for batch in train_loader:\n",
        "            for data in batch:\n",
        "                if hasattr(data, 'smiles'):\n",
        "                    self.original_smiles[dataset_idx] = data.smiles\n",
        "                dataset_idx += 1\n",
        "\n",
        "        print(f\"Stored {len(self.original_smiles)} SMILES strings from dataset\")\n",
        "\n",
        "        # Save the SMILES as a reference\n",
        "        smiles_file = os.path.join(self.output_dir, f\"dataset_smiles_{self.timestamp}.txt\")\n",
        "        with open(smiles_file, 'w') as f:\n",
        "            for idx, smiles in sorted(self.original_smiles.items()):\n",
        "                f.write(f\"{idx},{smiles}\\n\")\n",
        "\n",
        "        print(f\"Saved original SMILES to {smiles_file}\")\n",
        "\n",
        "    def add_batch(self, batch, embeddings):\n",
        "        \"\"\"Store embeddings and their batch indices\n",
        "\n",
        "        Args:\n",
        "            batch: Batch data with batch.batch containing the batch indices\n",
        "            embeddings: Embeddings tensor\n",
        "        \"\"\"\n",
        "        # Convert embeddings to numpy\n",
        "        embeddings_np = embeddings.detach().cpu().numpy()\n",
        "\n",
        "        # Extract batch indices\n",
        "        if hasattr(batch, 'batch'):\n",
        "            # For batched graph data\n",
        "            indices = batch.batch.cpu().numpy()\n",
        "        else:\n",
        "            # Fallback: just use sequential indices\n",
        "            indices = np.arange(len(embeddings_np))\n",
        "\n",
        "        # Verify dimensions match\n",
        "        if len(indices) != len(embeddings_np):\n",
        "            print(f\"Warning: Indices count ({len(indices)}) doesn't match embeddings count ({len(embeddings_np)})\")\n",
        "            # Use the smaller size\n",
        "            min_size = min(len(indices), len(embeddings_np))\n",
        "            indices = indices[:min_size]\n",
        "            embeddings_np = embeddings_np[:min_size]\n",
        "\n",
        "        # Store\n",
        "        self.embeddings.append(embeddings_np)\n",
        "        self.batch_indices.append(indices)\n",
        "\n",
        "    def save_embeddings(self, prefix=\"embeddings\"):\n",
        "        \"\"\"Save embeddings with their original SMILES strings\n",
        "\n",
        "        Args:\n",
        "            prefix: Filename prefix\n",
        "\n",
        "        Returns:\n",
        "            Path to saved file\n",
        "        \"\"\"\n",
        "        if not self.embeddings:\n",
        "            print(\"Warning: No embeddings to save\")\n",
        "            return None\n",
        "\n",
        "        # Concatenate all embeddings and indices\n",
        "        all_embeddings = np.vstack(self.embeddings)\n",
        "        all_indices = np.concatenate(self.batch_indices)\n",
        "\n",
        "        # Map indices back to SMILES\n",
        "        all_smiles = []\n",
        "        for idx in all_indices:\n",
        "            if idx in self.original_smiles:\n",
        "                all_smiles.append(self.original_smiles[idx])\n",
        "            else:\n",
        "                all_smiles.append(f\"unknown_{idx}\")\n",
        "\n",
        "        # Create filename\n",
        "        filename = f\"{prefix}_{self.timestamp}.npz\"\n",
        "        filepath = os.path.join(self.output_dir, filename)\n",
        "\n",
        "        # Save as npz file\n",
        "        np.savez(filepath, embeddings=all_embeddings, smiles=all_smiles)\n",
        "\n",
        "        print(f\"Saved {len(all_smiles)} embeddings with SMILES to {filepath}\")\n",
        "        return filepath\n",
        "\n",
        "    def reset_embeddings(self):\n",
        "        \"\"\"Clear current embeddings (keeping original SMILES)\"\"\"\n",
        "        self.embeddings = []\n",
        "        self.batch_indices = []\n",
        "\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import torch\n",
        "from rdkit import Chem\n",
        "from rdkit.Chem import Descriptors, Lipinski\n",
        "from rdkit.Chem.rdMolDescriptors import CalcTPSA\n",
        "from tqdm import tqdm\n",
        "import os\n",
        "from datetime import datetime\n",
        "import json\n",
        "from torch_geometric.data import Batch, Data\n",
        "\n",
        "\n",
        "def process_and_save_dataset(train_loader, output_dir='./analysis', prefix='before_training'):\n",
        "    \"\"\"Extract and analyze SMILES from a dataloader directly\n",
        "\n",
        "    Args:\n",
        "        train_loader: PyTorch Geometric DataLoader\n",
        "        output_dir: Directory to save analysis results\n",
        "        prefix: Prefix for output files\n",
        "    \"\"\"\n",
        "    # Extract SMILES from the dataset\n",
        "    all_smiles = []\n",
        "    for batch in train_loader:\n",
        "        for i in range(len(batch)):\n",
        "            # Extract individual data items from the batch\n",
        "            if hasattr(batch, 'smiles'):\n",
        "                # If the batch has a smiles attribute (list)\n",
        "                all_smiles.append(batch.smiles[i])\n",
        "            elif hasattr(batch[i], 'smiles'):\n",
        "                # If individual items have smiles attributes\n",
        "                all_smiles.append(batch[i].smiles)\n",
        "\n",
        "    print(f\"Extracted {len(all_smiles)} SMILES strings from the dataset\")\n",
        "\n",
        "    # Save SMILES for reference\n",
        "    timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')\n",
        "    smiles_path = os.path.join(output_dir, f\"{prefix}_smiles_{timestamp}.txt\")\n",
        "    os.makedirs(os.path.dirname(smiles_path), exist_ok=True)\n",
        "\n",
        "    with open(smiles_path, 'w') as f:\n",
        "        for smiles in all_smiles:\n",
        "            f.write(f\"{smiles}\\n\")\n",
        "\n",
        "    print(f\"Saved SMILES to {smiles_path}\")\n",
        "\n",
        "    # Analyze SMILES\n",
        "    analyze_smiles_list(all_smiles, output_dir=output_dir, prefix=prefix)\n",
        "\n",
        "    return all_smiles\n",
        "\n",
        "\n",
        "# Utility function to ensure Data objects have SMILES attributes\n",
        "def ensure_smiles_in_batch(batch):\n",
        "    \"\"\"Ensure that SMILES strings are available in a batch\n",
        "\n",
        "    This modifies the batch in-place to make sure SMILES strings are preserved\n",
        "    for later tracking.\n",
        "\n",
        "    Args:\n",
        "        batch: PyTorch Geometric batch\n",
        "\n",
        "    Returns:\n",
        "        Modified batch with smiles attribute\n",
        "    \"\"\"\n",
        "    if not hasattr(batch, 'smiles'):\n",
        "        # Check if individual data items have smiles\n",
        "        smiles_list = []\n",
        "        for i in range(len(batch)):\n",
        "            if hasattr(batch[i], 'smiles'):\n",
        "                smiles_list.append(batch[i].smiles)\n",
        "            else:\n",
        "                smiles_list.append(f\"unknown_{i}\")\n",
        "\n",
        "        # Add smiles list as an attribute of the batch\n",
        "        batch.smiles = smiles_list\n",
        "\n",
        "    return batch\n",
        "\n",
        "\n",
        "def modified_train_gan_cl(train_loader, config, device='cuda',\n",
        "                         save_dir='./checkpoints',\n",
        "                         embedding_dir='./embeddings'):\n",
        "    \"\"\"Modified training function with direct SMILES tracking\"\"\"\n",
        "\n",
        "    # Create directories\n",
        "    os.makedirs(save_dir, exist_ok=True)\n",
        "    os.makedirs(embedding_dir, exist_ok=True)\n",
        "    os.makedirs('./analysis', exist_ok=True)\n",
        "\n",
        "    # Process and analyze original dataset\n",
        "    print(\"Analyzing dataset before training...\")\n",
        "    original_smiles = process_and_save_dataset(train_loader, output_dir='./analysis', prefix='before_training')\n",
        "\n",
        "    # Initialize model\n",
        "    model = MolecularGANCL(config).to(device)\n",
        "\n",
        "    # Initialize optimizers\n",
        "    optimizer_encoder = torch.optim.Adam(model.encoder.parameters(), lr=3e-4)\n",
        "    optimizer_generator = torch.optim.Adam(model.generator.parameters(), lr=1e-4)\n",
        "\n",
        "    # Training metrics\n",
        "    metrics = {\n",
        "        'contrastive_losses': [],\n",
        "        'adversarial_losses': [],\n",
        "        'similarity_losses': [],\n",
        "        'total_losses': []\n",
        "    }\n",
        "\n",
        "    # Training phases\n",
        "    print(\"Phase 1: Pretraining Contrastive Learning...\")\n",
        "    pretrain_epochs = 10\n",
        "    for epoch in range(pretrain_epochs):\n",
        "        contrastive_epoch_loss = 0\n",
        "\n",
        "        for batch in tqdm(train_loader, desc=f'Pretrain Epoch {epoch+1}/{pretrain_epochs}'):\n",
        "            batch = batch.to(device)\n",
        "\n",
        "            # Forward pass (without generator)\n",
        "            query_emb = model.encoder(batch)\n",
        "            with torch.no_grad():\n",
        "                key_emb = model.momentum_encoder(batch)\n",
        "\n",
        "            # Compute contrastive loss\n",
        "            contrastive_loss = model.memory_queue.compute_contrastive_loss(\n",
        "                query_emb, key_emb, model.config.temperature\n",
        "            )\n",
        "\n",
        "            # Update encoder\n",
        "            optimizer_encoder.zero_grad()\n",
        "            contrastive_loss.backward()\n",
        "            optimizer_encoder.step()\n",
        "\n",
        "            # Update momentum encoder\n",
        "            model._momentum_update()\n",
        "\n",
        "            # Update memory queue\n",
        "            model.memory_queue.update_queue(key_emb.detach())\n",
        "\n",
        "            contrastive_epoch_loss += contrastive_loss.item()\n",
        "\n",
        "        avg_loss = contrastive_epoch_loss / len(train_loader)\n",
        "        metrics['contrastive_losses'].append(avg_loss)\n",
        "        print(f'Pretrain Epoch {epoch+1}, Avg Loss: {avg_loss:.4f}')\n",
        "\n",
        "        # Save pretrained checkpoint\n",
        "        if (epoch + 1) % 5 == 0:\n",
        "            torch.save({\n",
        "                'epoch': epoch,\n",
        "                'model_state_dict': model.state_dict(),\n",
        "                'optimizer_encoder_state_dict': optimizer_encoder.state_dict(),\n",
        "                'loss': avg_loss,\n",
        "            }, os.path.join(save_dir, f'pretrain_checkpoint_{epoch+1}.pt'))\n",
        "\n",
        "    print(\"\\nPhase 2: Training GAN-CL...\")\n",
        "    train_epochs = 50\n",
        "    best_loss = float('inf')\n",
        "\n",
        "    for epoch in range(train_epochs):\n",
        "        epoch_losses = {\n",
        "            'contrastive': 0,\n",
        "            'adversarial': 0,\n",
        "            'similarity': 0,\n",
        "            'total': 0\n",
        "        }\n",
        "\n",
        "        for batch in tqdm(train_loader, desc=f'Train Epoch {epoch+1}/{train_epochs}'):\n",
        "            batch = batch.to(device)\n",
        "\n",
        "            # Step 1: Train Encoder\n",
        "            optimizer_encoder.zero_grad()\n",
        "\n",
        "            # Get importance scores from generator\n",
        "            with torch.no_grad():\n",
        "                node_scores, edge_scores = model.generator(batch)\n",
        "\n",
        "            # Create perturbed graph\n",
        "            perturbed_data = model.drop_graph_elements(batch, node_scores, edge_scores)\n",
        "\n",
        "            # Get embeddings\n",
        "            query_emb = model.encoder(perturbed_data)\n",
        "            with torch.no_grad():\n",
        "                key_emb = model.momentum_encoder(batch)\n",
        "                original_emb = model.encoder(batch).detach()\n",
        "\n",
        "            # Compute losses for encoder\n",
        "            contrastive_loss = model.memory_queue.compute_contrastive_loss(\n",
        "                query_emb, key_emb, model.config.temperature\n",
        "            )\n",
        "            similarity_loss = torch.nn.functional.mse_loss(query_emb, original_emb)\n",
        "\n",
        "            # Total loss for encoder\n",
        "            encoder_loss = contrastive_loss + 0.1 * similarity_loss\n",
        "\n",
        "            # Update encoder\n",
        "            encoder_loss.backward()\n",
        "            optimizer_encoder.step()\n",
        "\n",
        "            # Update momentum encoder\n",
        "            model._momentum_update()\n",
        "\n",
        "            # Step 2: Train Generator\n",
        "            optimizer_generator.zero_grad()\n",
        "\n",
        "            # Get new embeddings for adversarial loss\n",
        "            node_scores, edge_scores = model.generator(batch)\n",
        "            perturbed_data = model.drop_graph_elements(batch, node_scores, edge_scores)\n",
        "\n",
        "            with torch.no_grad():\n",
        "                original_emb = model.encoder(batch)\n",
        "            perturbed_emb = model.encoder(perturbed_data)\n",
        "\n",
        "            # Compute adversarial loss\n",
        "            adversarial_loss = -torch.nn.functional.mse_loss(perturbed_emb, original_emb)\n",
        "\n",
        "            # Update generator\n",
        "            adversarial_loss.backward()\n",
        "            optimizer_generator.step()\n",
        "\n",
        "            # Update memory queue\n",
        "            model.memory_queue.update_queue(key_emb.detach())\n",
        "\n",
        "            # Update metrics\n",
        "            epoch_losses['contrastive'] += contrastive_loss.item()\n",
        "            epoch_losses['adversarial'] += adversarial_loss.item()\n",
        "            epoch_losses['similarity'] += similarity_loss.item()\n",
        "            epoch_losses['total'] += encoder_loss.item()\n",
        "\n",
        "        # Average losses\n",
        "        for k in epoch_losses:\n",
        "            epoch_losses[k] /= len(train_loader)\n",
        "            metrics[f'{k}_losses'].append(epoch_losses[k])\n",
        "\n",
        "        print(f'Epoch {epoch+1}, Losses: {epoch_losses}')\n",
        "\n",
        "        # Save checkpoint\n",
        "        if (epoch + 1) % 10 == 0:\n",
        "            torch.save({\n",
        "                'epoch': epoch,\n",
        "                'model_state_dict': model.state_dict(),\n",
        "                'optimizer_encoder_state_dict': optimizer_encoder.state_dict(),\n",
        "                'optimizer_generator_state_dict': optimizer_generator.state_dict(),\n",
        "                'losses': epoch_losses,\n",
        "            }, os.path.join(save_dir, f'gan_cl_checkpoint_{epoch+1}.pt'))\n",
        "\n",
        "    # Extract and save final embeddings\n",
        "    timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')\n",
        "    all_final_embeddings = []\n",
        "    all_final_smiles = []\n",
        "\n",
        "    # Extract embeddings after training\n",
        "    model.eval()\n",
        "    print(\"Extracting final embeddings...\")\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for batch in tqdm(train_loader, desc=\"Final embeddings\"):\n",
        "            # Process batch and add SMILES\n",
        "            batch = batch.to(device)\n",
        "\n",
        "            # Extract SMILES from batch\n",
        "            batch_smiles = []\n",
        "            for i in range(len(batch)):\n",
        "                if hasattr(batch[i], 'smiles'):\n",
        "                    batch_smiles.append(batch[i].smiles)\n",
        "                else:\n",
        "                    batch_smiles.append(f\"unknown_{i}\")\n",
        "\n",
        "            # Get embeddings\n",
        "            final_embeddings = model.get_embeddings(batch)\n",
        "\n",
        "            # Store embeddings and SMILES\n",
        "            all_final_embeddings.append(final_embeddings.cpu().numpy())\n",
        "            all_final_smiles.extend(batch_smiles)\n",
        "\n",
        "    # Concatenate embeddings\n",
        "    if all_final_embeddings:\n",
        "        all_final_embeddings = np.vstack(all_final_embeddings)\n",
        "\n",
        "        # Save embeddings with SMILES\n",
        "        final_embeddings_path = os.path.join(embedding_dir, f\"after_training_{timestamp}.npz\")\n",
        "        np.savez(final_embeddings_path, embeddings=all_final_embeddings, smiles=all_final_smiles)\n",
        "        print(f\"Saved {len(all_final_smiles)} embeddings with SMILES to {final_embeddings_path}\")\n",
        "\n",
        "        # Analyze final embeddings\n",
        "        print(\"Analyzing final embeddings...\")\n",
        "        analyze_smiles_list(all_final_smiles, output_dir='./analysis', prefix=\"after_training\")\n",
        "    else:\n",
        "        print(\"Warning: No final embeddings to save\")\n",
        "        all_final_embeddings = np.array([])\n",
        "        all_final_smiles = []\n",
        "\n",
        "    # Save final metrics\n",
        "    with open(os.path.join(save_dir, 'training_metrics.json'), 'w') as f:\n",
        "        json.dump(metrics, f)\n",
        "\n",
        "    return model, metrics, all_final_embeddings, all_final_smiles\n",
        "\n",
        "\n",
        "def analyze_smiles_list(smiles_list, output_dir='./analysis', prefix='analysis'):\n",
        "    \"\"\"Analyze a list of SMILES strings for molecular properties\n",
        "\n",
        "    Args:\n",
        "        smiles_list: List of SMILES strings\n",
        "        output_dir: Directory to save analysis results\n",
        "        prefix: Prefix for output files\n",
        "\n",
        "    Returns:\n",
        "        Tuple of DataFrames with properties, features, functional groups\n",
        "    \"\"\"\n",
        "    # Make sure directory exists\n",
        "    os.makedirs(output_dir, exist_ok=True)\n",
        "\n",
        "    print(f\"Analyzing {len(smiles_list)} molecules...\")\n",
        "\n",
        "    # Prepare data storage\n",
        "    properties_list = []\n",
        "    features_list = []\n",
        "    func_groups_list = []\n",
        "    valid_smiles = []\n",
        "\n",
        "    timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')\n",
        "\n",
        "    # Process each SMILES\n",
        "    for smiles in tqdm(smiles_list):\n",
        "        # Skip if not a valid SMILES string (e.g., \"unknown_0\")\n",
        "        if not isinstance(smiles, str) or smiles.startswith(\"unknown_\"):\n",
        "            print(f\"Skipping invalid SMILES placeholder: {smiles}\")\n",
        "            continue\n",
        "\n",
        "        try:\n",
        "            mol = Chem.MolFromSmiles(smiles)\n",
        "            if mol is None:\n",
        "                print(f\"Warning: Invalid SMILES: {smiles}\")\n",
        "                continue\n",
        "\n",
        "            valid_smiles.append(smiles)\n",
        "\n",
        "            # Extract basic properties\n",
        "            prop = {\n",
        "                'MW': Descriptors.ExactMolWt(mol),\n",
        "                'LogP': Descriptors.MolLogP(mol),\n",
        "                'TPSA': CalcTPSA(mol),\n",
        "                'NumHAcceptors': Lipinski.NumHAcceptors(mol),\n",
        "                'NumHDonors': Lipinski.NumHDonors(mol),\n",
        "                'NumRotatableBonds': Descriptors.NumRotatableBonds(mol),\n",
        "                'NumAtoms': mol.GetNumAtoms(),\n",
        "                'NumHeavyAtoms': mol.GetNumHeavyAtoms(),\n",
        "                'NumBonds': mol.GetNumBonds(),\n",
        "                'NumRings': Chem.GetSSSR(mol)\n",
        "            }\n",
        "            properties_list.append(prop)\n",
        "\n",
        "            # Extract structural features\n",
        "            ri = mol.GetRingInfo()\n",
        "            rings = ri.AtomRings()\n",
        "\n",
        "            num_aromatic = sum(1 for atom in mol.GetAtoms() if atom.GetIsAromatic())\n",
        "\n",
        "            feat = {\n",
        "                'Aromatic': 1 if num_aromatic > 0 else 0,\n",
        "                'Heterocycles': 1 if any(any(mol.GetAtomWithIdx(idx).GetAtomicNum() != 6 for idx in ring) for ring in rings) else 0,\n",
        "                'FusedRings': 0,\n",
        "                'SpiroRings': 0,\n",
        "                'BridgedRings': 0,\n",
        "                'Macrocycles': 0,\n",
        "                'LinearChain': 1 if mol.GetNumBonds() == mol.GetNumAtoms() - 1 and len(rings) == 0 else 0,\n",
        "                'Branched': 1 if sum(1 for atom in mol.GetAtoms() if atom.GetDegree() > 2) > 0 else 0\n",
        "            }\n",
        "\n",
        "            # Check for fused rings\n",
        "            if len(rings) >= 2:\n",
        "                for i in range(len(rings)):\n",
        "                    for j in range(i+1, len(rings)):\n",
        "                        if len(set(rings[i]).intersection(set(rings[j]))) > 1:\n",
        "                            feat['FusedRings'] = 1\n",
        "                            break\n",
        "                    if feat['FusedRings'] == 1:\n",
        "                        break\n",
        "\n",
        "            # Check for spiro rings\n",
        "            if len(rings) >= 2:\n",
        "                for i in range(len(rings)):\n",
        "                    for j in range(i+1, len(rings)):\n",
        "                        if len(set(rings[i]).intersection(set(rings[j]))) == 1:\n",
        "                            feat['SpiroRings'] = 1\n",
        "                            break\n",
        "                    if feat['SpiroRings'] == 1:\n",
        "                        break\n",
        "\n",
        "            # Check for bridged rings\n",
        "            bridged_patt = Chem.MolFromSmarts('[D4R]')\n",
        "            if bridged_patt and mol.HasSubstructMatch(bridged_patt):\n",
        "                feat['BridgedRings'] = 1\n",
        "\n",
        "            # Check for macrocycles\n",
        "            for ring in rings:\n",
        "                if len(ring) >= 8:\n",
        "                    feat['Macrocycles'] = 1\n",
        "                    break\n",
        "\n",
        "            features_list.append(feat)\n",
        "\n",
        "            # Extract functional groups\n",
        "            fg = {}\n",
        "\n",
        "            # Alcohols (explicit check for -OH group)\n",
        "            alcohol_count = 0\n",
        "            for atom in mol.GetAtoms():\n",
        "                if atom.GetAtomicNum() == 8:  # Oxygen\n",
        "                    if atom.GetTotalNumHs() > 0:  # Has hydrogen\n",
        "                        # Check if connected to carbon\n",
        "                        for neighbor in atom.GetNeighbors():\n",
        "                            if neighbor.GetAtomicNum() == 6:  # Carbon\n",
        "                                alcohol_count += 1\n",
        "                                break\n",
        "            fg['Alcohol'] = alcohol_count\n",
        "\n",
        "            # Check amines (N with hydrogens)\n",
        "            amine_count = 0\n",
        "            for atom in mol.GetAtoms():\n",
        "                if atom.GetAtomicNum() == 7:  # Nitrogen\n",
        "                    if atom.GetTotalNumHs() > 0:  # Has hydrogen\n",
        "                        amine_count += 1\n",
        "            fg['Amine'] = amine_count\n",
        "\n",
        "            # Other functional groups via SMARTS patterns\n",
        "            patterns = {\n",
        "                'Carboxyl': 'C(=O)[OH]',\n",
        "                'Carbonyl': 'C=O',\n",
        "                'Ether': 'COC',\n",
        "                'Ester': 'C(=O)OC',\n",
        "                'Amide': 'C(=O)N'\n",
        "            }\n",
        "\n",
        "            for name, smarts in patterns.items():\n",
        "                patt = Chem.MolFromSmarts(smarts)\n",
        "                if patt:\n",
        "                    fg[name] = len(mol.GetSubstructMatches(patt))\n",
        "                else:\n",
        "                    fg[name] = 0\n",
        "\n",
        "            # Count halogens\n",
        "            fg['Halogen'] = sum(1 for atom in mol.GetAtoms()\n",
        "                            if atom.GetAtomicNum() in [9, 17, 35, 53])  # F, Cl, Br, I\n",
        "\n",
        "            func_groups_list.append(fg)\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"Error processing SMILES {smiles}: {e}\")\n",
        "\n",
        "    # If no valid SMILES were found, create empty DataFrames\n",
        "    if not valid_smiles:\n",
        "        print(\"Warning: No valid SMILES found for analysis\")\n",
        "        # Create empty DataFrames\n",
        "        props_df = pd.DataFrame()\n",
        "        features_df = pd.DataFrame()\n",
        "        func_groups_df = pd.DataFrame()\n",
        "    else:\n",
        "        # Create DataFrames\n",
        "        props_df = pd.DataFrame(properties_list, index=valid_smiles)\n",
        "        features_df = pd.DataFrame(features_list, index=valid_smiles)\n",
        "        func_groups_df = pd.DataFrame(func_groups_list, index=valid_smiles)\n",
        "\n",
        "    # Save to CSV\n",
        "    output_prefix = os.path.join(output_dir, f\"{prefix}_{timestamp}\")\n",
        "\n",
        "    props_df.to_csv(f\"{output_prefix}_properties.csv\")\n",
        "    features_df.to_csv(f\"{output_prefix}_features.csv\")\n",
        "    func_groups_df.to_csv(f\"{output_prefix}_functional_groups.csv\")\n",
        "\n",
        "    # Save the valid SMILES list for reference\n",
        "    with open(f\"{output_prefix}_valid_smiles.txt\", 'w') as f:\n",
        "        for smiles in valid_smiles:\n",
        "            f.write(f\"{smiles}\\n\")\n",
        "\n",
        "    print(f\"Analysis saved to {output_prefix}_*.csv\")\n",
        "    print(f\"Found {len(valid_smiles)} valid molecules out of {len(smiles_list)} SMILES\")\n",
        "\n",
        "    return props_df, features_df, func_groups_df"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "4c840793",
      "metadata": {
        "id": "4c840793"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "8e90813e",
      "metadata": {
        "id": "8e90813e"
      },
      "outputs": [],
      "source": [
        "def save_embeddings(embeddings, labels, filepath):\n",
        "    \"\"\"Save embeddings and corresponding labels\"\"\"\n",
        "    with open(filepath, 'wb') as f:\n",
        "        pickle.dump({\n",
        "            'embeddings': embeddings,\n",
        "            'labels': labels\n",
        "        }, f)\n",
        "\n",
        "def save_encoder(encoder, save_path, info=None):\n",
        "    \"\"\"Save encoder model for downstream tasks\"\"\"\n",
        "    save_dict = {\n",
        "        'encoder_state_dict': encoder.state_dict(),\n",
        "        'model_info': info or {}\n",
        "    }\n",
        "    torch.save(save_dict, save_path)\n",
        "\n",
        "def load_encoder(model_path, device='cpu'):\n",
        "    \"\"\"Load saved encoder model\"\"\"\n",
        "    checkpoint = torch.load(model_path, map_location=device)\n",
        "    encoder = GraphDiscriminator(\n",
        "        node_dim=checkpoint['model_info'].get('node_dim'),\n",
        "        edge_dim=checkpoint['model_info'].get('edge_dim'),\n",
        "        hidden_dim=checkpoint['model_info'].get('hidden_dim', 128),\n",
        "        output_dim=checkpoint['model_info'].get('output_dim', 128)\n",
        "    )\n",
        "    encoder.load_state_dict(checkpoint['encoder_state_dict'])\n",
        "    return encoder\n",
        "\n",
        "def train_gan_cl(train_loader, config, device='cuda',\n",
        "                save_dir='./checkpoints',\n",
        "                embedding_dir='./embeddings'):\n",
        "    \"\"\"Main training function for GAN-CL with fixed gradient computation\"\"\"\n",
        "\n",
        "    # Create directories\n",
        "    os.makedirs(save_dir, exist_ok=True)\n",
        "    os.makedirs(embedding_dir, exist_ok=True)\n",
        "    encoder_dir = os.path.join(save_dir, 'encoders')\n",
        "    os.makedirs(encoder_dir, exist_ok=True)\n",
        "\n",
        "    # Initialize model\n",
        "    model = MolecularGANCL(config).to(device)\n",
        "\n",
        "    # Initialize optimizers\n",
        "    optimizer_encoder = torch.optim.Adam(model.encoder.parameters(), lr=3e-4)\n",
        "    optimizer_generator = torch.optim.Adam(model.generator.parameters(), lr=1e-4)\n",
        "\n",
        "    # Save initial model info\n",
        "    model_info = {\n",
        "        'node_dim': config.node_dim,\n",
        "        'edge_dim': config.edge_dim,\n",
        "        'hidden_dim': config.hidden_dim,\n",
        "        'output_dim': config.output_dim,\n",
        "        'training_config': config.__dict__\n",
        "    }\n",
        "\n",
        "    # Training phases as before...\n",
        "    best_loss = float('inf')\n",
        "\n",
        "    # Training metrics\n",
        "    metrics = {\n",
        "        'contrastive_losses': [],\n",
        "        'adversarial_losses': [],\n",
        "        'similarity_losses': [],\n",
        "        'total_losses': []\n",
        "    }\n",
        "\n",
        "    # Training phases\n",
        "    print(\"Phase 1: Pretraining Contrastive Learning...\")\n",
        "    pretrain_epochs = 10\n",
        "    for epoch in range(pretrain_epochs):\n",
        "        contrastive_epoch_loss = 0\n",
        "\n",
        "        for batch in tqdm(train_loader, desc=f'Pretrain Epoch {epoch+1}/{pretrain_epochs}'):\n",
        "            batch = batch.to(device)\n",
        "\n",
        "            # Forward pass (without generator)\n",
        "            query_emb = model.encoder(batch)\n",
        "            with torch.no_grad():\n",
        "                key_emb = model.momentum_encoder(batch)\n",
        "\n",
        "            # Compute contrastive loss\n",
        "            contrastive_loss = model.memory_queue.compute_contrastive_loss(\n",
        "                query_emb, key_emb, model.config.temperature\n",
        "            )\n",
        "\n",
        "            # Update encoder\n",
        "            optimizer_encoder.zero_grad()\n",
        "            contrastive_loss.backward()\n",
        "            optimizer_encoder.step()\n",
        "\n",
        "            # Update momentum encoder\n",
        "            model._momentum_update()\n",
        "\n",
        "            # Update memory queue\n",
        "            model.memory_queue.update_queue(key_emb.detach())\n",
        "\n",
        "            contrastive_epoch_loss += contrastive_loss.item()\n",
        "\n",
        "        avg_loss = contrastive_epoch_loss / len(train_loader)\n",
        "        metrics['contrastive_losses'].append(avg_loss)\n",
        "        print(f'Pretrain Epoch {epoch+1}, Avg Loss: {avg_loss:.4f}')\n",
        "\n",
        "        # Save pretrained checkpoint\n",
        "        if (epoch + 1) % 5 == 0:\n",
        "            torch.save({\n",
        "                'epoch': epoch,\n",
        "                'model_state_dict': model.state_dict(),\n",
        "                'optimizer_encoder_state_dict': optimizer_encoder.state_dict(),\n",
        "                'loss': avg_loss,\n",
        "            }, os.path.join(save_dir, f'pretrain_checkpoint_{epoch+1}.pt'))\n",
        "\n",
        "    print(\"\\nPhase 2: Training GAN-CL...\")\n",
        "    train_epochs = 50\n",
        "#     train_epochs = 10\n",
        "    for epoch in range(train_epochs):\n",
        "        epoch_losses = {\n",
        "            'contrastive': 0,\n",
        "            'adversarial': 0,\n",
        "            'similarity': 0,\n",
        "            'total': 0\n",
        "        }\n",
        "\n",
        "        for batch in tqdm(train_loader, desc=f'Train Epoch {epoch+1}/{train_epochs}'):\n",
        "            batch = batch.to(device)\n",
        "\n",
        "            # Step 1: Train Encoder\n",
        "            optimizer_encoder.zero_grad()\n",
        "\n",
        "            # Get importance scores from generator\n",
        "            with torch.no_grad():\n",
        "                node_scores, edge_scores = model.generator(batch)\n",
        "\n",
        "            # Create perturbed graph\n",
        "            perturbed_data = model.drop_graph_elements(batch, node_scores, edge_scores)\n",
        "\n",
        "            # Get embeddings\n",
        "            query_emb = model.encoder(perturbed_data)\n",
        "            with torch.no_grad():\n",
        "                key_emb = model.momentum_encoder(batch)\n",
        "                original_emb = model.encoder(batch).detach()\n",
        "\n",
        "            # Compute losses for encoder\n",
        "            contrastive_loss = model.memory_queue.compute_contrastive_loss(\n",
        "                query_emb, key_emb, model.config.temperature\n",
        "            )\n",
        "            similarity_loss = F.mse_loss(query_emb, original_emb)\n",
        "\n",
        "            # Total loss for encoder\n",
        "            encoder_loss = contrastive_loss + 0.1 * similarity_loss\n",
        "\n",
        "            # Update encoder\n",
        "            encoder_loss.backward()\n",
        "            optimizer_encoder.step()\n",
        "\n",
        "            # Update momentum encoder\n",
        "            model._momentum_update()\n",
        "\n",
        "            # Step 2: Train Generator\n",
        "            optimizer_generator.zero_grad()\n",
        "\n",
        "            # Get new embeddings for adversarial loss\n",
        "            node_scores, edge_scores = model.generator(batch)\n",
        "            perturbed_data = model.drop_graph_elements(batch, node_scores, edge_scores)\n",
        "\n",
        "            with torch.no_grad():\n",
        "                original_emb = model.encoder(batch)\n",
        "            perturbed_emb = model.encoder(perturbed_data)\n",
        "\n",
        "            # Compute adversarial loss\n",
        "            adversarial_loss = -F.mse_loss(perturbed_emb, original_emb)\n",
        "\n",
        "            # Update generator\n",
        "            adversarial_loss.backward()\n",
        "            optimizer_generator.step()\n",
        "\n",
        "            # Update memory queue\n",
        "            model.memory_queue.update_queue(key_emb.detach())\n",
        "\n",
        "            # Update metrics\n",
        "            epoch_losses['contrastive'] += contrastive_loss.item()\n",
        "            epoch_losses['adversarial'] += adversarial_loss.item()\n",
        "            epoch_losses['similarity'] += similarity_loss.item()\n",
        "            epoch_losses['total'] += encoder_loss.item()\n",
        "\n",
        "        # Average losses\n",
        "        for k in epoch_losses:\n",
        "            epoch_losses[k] /= len(train_loader)\n",
        "            metrics[f'{k}_losses'].append(epoch_losses[k])\n",
        "\n",
        "        print(f'Epoch {epoch+1}, Losses: {epoch_losses}')\n",
        "\n",
        "        # Save checkpoint\n",
        "        if (epoch + 1) % 10 == 0:\n",
        "            torch.save({\n",
        "                'epoch': epoch,\n",
        "                'model_state_dict': model.state_dict(),\n",
        "                'optimizer_encoder_state_dict': optimizer_encoder.state_dict(),\n",
        "                'optimizer_generator_state_dict': optimizer_generator.state_dict(),\n",
        "                'losses': epoch_losses,\n",
        "            }, os.path.join(save_dir, f'gan_cl_checkpoint_{epoch+1}.pt'))\n",
        "\n",
        "        # Extract and save embeddings periodically\n",
        "        if (epoch + 1) % 10 == 0:\n",
        "            model.eval()\n",
        "            all_embeddings = []\n",
        "            all_graphs = []\n",
        "\n",
        "            with torch.no_grad():\n",
        "                for batch in train_loader:\n",
        "                    batch = batch.to(device)\n",
        "                    embeddings = model.get_embeddings(batch)\n",
        "                    all_embeddings.append(embeddings.cpu())\n",
        "                    all_graphs.extend([data for data in batch])\n",
        "\n",
        "            all_embeddings = torch.cat(all_embeddings, dim=0)\n",
        "\n",
        "            # Save embeddings\n",
        "#             timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')\n",
        "            save_embeddings(\n",
        "                all_embeddings.numpy(),\n",
        "                all_graphs,\n",
        "                os.path.join(embedding_dir, f'embeddings_epoch_{epoch+1}_{timestamp}.pkl')\n",
        "            )\n",
        "\n",
        "            model.train()\n",
        "\n",
        "    # Save final metrics\n",
        "    with open(os.path.join(save_dir, 'training_metrics.json'), 'w') as f:\n",
        "        json.dump(metrics, f)\n",
        "\n",
        "            # Save encoder periodically\n",
        "        if (epoch + 1) % 10 == 0:\n",
        "            epoch_info = {\n",
        "                **model_info,\n",
        "                'epoch': epoch + 1,\n",
        "                'loss': epoch_losses['total']\n",
        "            }\n",
        "            save_encoder(\n",
        "                model.encoder,\n",
        "                os.path.join(encoder_dir, f'encoder_epoch_{epoch+1}.pt'),\n",
        "                epoch_info\n",
        "            )\n",
        "\n",
        "        # Save best encoder based on total loss\n",
        "        if epoch_losses['total'] < best_loss:\n",
        "            best_loss = epoch_losses['total']\n",
        "            save_encoder(\n",
        "                model.encoder,\n",
        "                os.path.join(encoder_dir, f'best_encoder_{timestamp}.pt'),\n",
        "                {**model_info, 'epoch': epoch + 1, 'loss': best_loss}\n",
        "            )\n",
        "\n",
        "    # Save final encoder\n",
        "    save_encoder(\n",
        "        model.encoder,\n",
        "        os.path.join(encoder_dir, f'final_encoder_{timestamp}.pt'),\n",
        "        {**model_info, 'epoch': train_epochs, 'loss': epoch_losses['total']}\n",
        "    )\n",
        "\n",
        "    return model, metrics\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "4dbc0517",
      "metadata": {
        "scrolled": false,
        "id": "4dbc0517"
      },
      "outputs": [],
      "source": [
        "def main():\n",
        "    \"\"\"Main function\"\"\"\n",
        "    # Enable anomaly detection during development\n",
        "    torch.autograd.set_detect_anomaly(True)\n",
        "    # Your existing data loading code here\n",
        "    torch.manual_seed(42)\n",
        "    np.random.seed(42)\n",
        "\n",
        "    print(\"Starting data loading...\")\n",
        "    extractor = MolecularFeatureExtractor()\n",
        "    smiles_file = \"D:\\\\PhD\\\\Chapter3\\\\Unsupervised_GAN_Code\\\\pubchem-41-clean.txt\"\n",
        "\n",
        "    dataset = []\n",
        "    failed_smiles = []\n",
        "\n",
        "    with open(smiles_file, 'r') as f:\n",
        "        for i, line in enumerate(f):\n",
        "            smiles = line.strip()\n",
        "            data = extractor.process_molecule(smiles)\n",
        "            if data is not None:\n",
        "                # Store original SMILES in the data object\n",
        "                data.smiles = smiles\n",
        "                dataset.append(data)\n",
        "            else:\n",
        "                failed_smiles.append(smiles)\n",
        "\n",
        "            # Limit dataset size for testing\n",
        "            if i >= 10000:  # Adjust as needed\n",
        "                break\n",
        "\n",
        "    print(f\"1. Loaded dataset with {len(dataset)} graphs.\")\n",
        "    print(f\"2. Failed SMILES count: {len(failed_smiles)}\")\n",
        "\n",
        "    if not dataset:\n",
        "        print(\"No valid graphs generated.\")\n",
        "        return None\n",
        "\n",
        "    # Setup training\n",
        "    batch_size = 32\n",
        "    train_loader = DataLoader(dataset, batch_size=batch_size, shuffle=True)\n",
        "    print(f\"3. Created DataLoader with {len(train_loader.dataset)} graphs\")\n",
        "\n",
        "    # Set device\n",
        "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "    print(f\"4. Using device: {device}\")\n",
        "\n",
        "    # Get configuration based on dataset\n",
        "    config = get_model_config(dataset)\n",
        "\n",
        "    # Train model with direct SMILES tracking\n",
        "    print(\"5. Starting GAN-CL training with SMILES tracking...\")\n",
        "    model, metrics, final_embeddings, final_smiles = modified_train_gan_cl(\n",
        "        train_loader,\n",
        "        config,\n",
        "        device=device,\n",
        "        save_dir='./checkpoints',\n",
        "        embedding_dir='./embeddings'\n",
        "    )\n",
        "\n",
        "    print(\"6. Training completed!\")\n",
        "    print(f\"7. Final results: {len(final_smiles)} embeddings with associated SMILES\")\n",
        "\n",
        "    # At this point, all analysis should be complete\n",
        "    print(\"8. Analysis complete. The data can now be used for visualization and comparison.\")\n",
        "\n",
        "    return model, metrics, final_embeddings, final_smiles\n",
        "\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    model, metrics, embeddings, smiles = main()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "27d928ef",
      "metadata": {
        "id": "27d928ef"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.21"
    },
    "colab": {
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}