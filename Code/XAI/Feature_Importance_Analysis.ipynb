{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "612e8015",
      "metadata": {
        "id": "612e8015"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import pickle\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from sklearn.decomposition import PCA\n",
        "from sklearn.manifold import TSNE\n",
        "from sklearn.linear_model import Ridge\n",
        "from sklearn.ensemble import RandomForestRegressor, RandomForestClassifier\n",
        "from sklearn.metrics import r2_score\n",
        "from sklearn.model_selection import cross_val_score, KFold\n",
        "import umap\n",
        "\n",
        "class EmbeddingAnalyzer:\n",
        "    \"\"\"Analyze molecular embedding spaces for biases and representation quality\"\"\"\n",
        "\n",
        "    def __init__(self, metadata_path, embedding_files):\n",
        "        \"\"\"\n",
        "        Initialize analyzer with specific file paths\n",
        "\n",
        "        Args:\n",
        "            metadata_path: Path to molecule metadata pickle file\n",
        "            embedding_files: Dictionary mapping embedding types to file paths\n",
        "        \"\"\"\n",
        "        self.metadata_path = metadata_path\n",
        "        self.embedding_files = embedding_files\n",
        "\n",
        "        # Load metadata\n",
        "        with open(metadata_path, 'rb') as f:\n",
        "            self.metadata = pickle.load(f)\n",
        "\n",
        "        # Initialize dictionaries to store embeddings\n",
        "        self.embedding_data = {}\n",
        "        self.results = {}\n",
        "\n",
        "    def load_embeddings(self):\n",
        "        \"\"\"Load embedding files from provided paths\"\"\"\n",
        "        print(\"Loading embedding files...\")\n",
        "\n",
        "        for emb_type, filepath in self.embedding_files.items():\n",
        "            if not os.path.exists(filepath):\n",
        "                print(f\"Warning: File not found: {filepath}\")\n",
        "                continue\n",
        "\n",
        "            try:\n",
        "                with open(filepath, 'rb') as f:\n",
        "                    data = pickle.load(f)\n",
        "\n",
        "                # Handle different embedding file formats\n",
        "                if 'embeddings' in data:\n",
        "                    embeddings = data['embeddings']\n",
        "                elif isinstance(data, dict) and 'labels' in data:\n",
        "                    # Format from original save_embeddings function\n",
        "                    embeddings = data['embeddings']\n",
        "                else:\n",
        "                    print(f\"Warning: Couldn't extract embeddings from {filepath}\")\n",
        "                    continue\n",
        "\n",
        "                self.embedding_data[emb_type] = {\n",
        "                    'embeddings': embeddings,\n",
        "                    'metadata': data\n",
        "                }\n",
        "                print(f\"Loaded {emb_type} embeddings: {embeddings.shape}\")\n",
        "            except Exception as e:\n",
        "                print(f\"Error loading {filepath}: {e}\")\n",
        "\n",
        "        if not self.embedding_data:\n",
        "            raise ValueError(\"No embedding files loaded!\")\n",
        "\n",
        "    def prepare_property_data(self):\n",
        "        \"\"\"Extract properties from metadata into pandas DataFrame\"\"\"\n",
        "        print(\"Preparing property data from metadata...\")\n",
        "\n",
        "        # Initialize lists to collect property data\n",
        "        property_data = []\n",
        "\n",
        "        for mol_data in self.metadata:\n",
        "            # Extract properties\n",
        "            props = mol_data['properties']\n",
        "            feats = mol_data['features']\n",
        "            funcs = mol_data['functional_groups']\n",
        "            rings = mol_data['ring_info']\n",
        "\n",
        "            # Combine all properties into a single dict\n",
        "            mol_props = {\n",
        "                'graph_id': mol_data.get('graph_id', 'unknown')\n",
        "            }\n",
        "\n",
        "            # Add molecular properties\n",
        "            for k, v in props.items():\n",
        "                if isinstance(v, (int, float, bool)):\n",
        "                    mol_props[f'prop_{k}'] = v\n",
        "\n",
        "            # Add structural features\n",
        "            for k, v in feats.items():\n",
        "                if isinstance(v, (int, float, bool)):\n",
        "                    mol_props[f'feat_{k}'] = v\n",
        "\n",
        "            # Add functional groups\n",
        "            for k, v in funcs.items():\n",
        "                if isinstance(v, (int, float, bool)):\n",
        "                    mol_props[f'func_{k}'] = v\n",
        "\n",
        "            # Add ring information\n",
        "            for category, counts in rings.items():\n",
        "                if isinstance(counts, dict):\n",
        "                    for k, v in counts.items():\n",
        "                        if isinstance(v, (int, float)):\n",
        "                            mol_props[f'ring_{category}_{k}'] = v\n",
        "\n",
        "            property_data.append(mol_props)\n",
        "\n",
        "        # Convert to DataFrame\n",
        "        self.prop_df = pd.DataFrame(property_data)\n",
        "\n",
        "        # Make sure we have some numeric columns\n",
        "        numeric_cols = self.prop_df.select_dtypes(include=np.number).columns\n",
        "        if len(numeric_cols) == 0:\n",
        "            print(\"Warning: No numeric properties found in metadata!\")\n",
        "            # Add a dummy numeric column to prevent errors\n",
        "            self.prop_df['dummy'] = 0\n",
        "\n",
        "        print(f\"Prepared DataFrame with {len(self.prop_df)} molecules and {self.prop_df.shape[1]} properties\")\n",
        "\n",
        "        # Filter out columns with too many missing values or zero variance\n",
        "        self._clean_property_dataframe()\n",
        "\n",
        "    def _clean_property_dataframe(self):\n",
        "        \"\"\"Clean property DataFrame by removing low-information columns\"\"\"\n",
        "        # Exclude non-numeric columns from variance calculation\n",
        "        numeric_cols = self.prop_df.select_dtypes(include=np.number).columns\n",
        "\n",
        "        # Remove columns with too many missing values\n",
        "        missing_thresh = 0.5\n",
        "        missing_cols = [col for col in numeric_cols\n",
        "                        if self.prop_df[col].isna().mean() > missing_thresh]\n",
        "\n",
        "        # Remove columns with zero variance\n",
        "        var_thresh = 0.0\n",
        "        var_cols = [col for col in numeric_cols\n",
        "                    if col not in missing_cols and self.prop_df[col].var() <= var_thresh]\n",
        "\n",
        "        # Remove identified columns\n",
        "        drop_cols = missing_cols + var_cols\n",
        "        if drop_cols:\n",
        "            self.prop_df = self.prop_df.drop(columns=drop_cols)\n",
        "            print(f\"Removed {len(drop_cols)} low-information columns\")\n",
        "\n",
        "        # Fill remaining missing values\n",
        "        numeric_cols = self.prop_df.select_dtypes(include=np.number).columns\n",
        "        self.prop_df[numeric_cols] = self.prop_df[numeric_cols].fillna(0)\n",
        "\n",
        "    def analyze_property_prediction(self, properties=None, cv=5):\n",
        "        \"\"\"\n",
        "        Analyze how well embeddings predict molecular properties\n",
        "\n",
        "        Args:\n",
        "            properties: List of property column names to analyze (default: analyze all numeric)\n",
        "            cv: Number of cross-validation folds\n",
        "        \"\"\"\n",
        "        print(\"\\n=== Property Prediction Analysis ===\")\n",
        "\n",
        "        # Select properties to analyze\n",
        "        if properties is None:\n",
        "            # Only consider numeric columns, and exclude graph_id\n",
        "            numeric_cols = self.prop_df.select_dtypes(include=np.number).columns\n",
        "            properties = list(numeric_cols)\n",
        "\n",
        "        if len(properties) == 0:\n",
        "            print(\"No numeric properties available for prediction analysis.\")\n",
        "            return pd.DataFrame()\n",
        "\n",
        "        # Initialize results storage\n",
        "        property_results = {'property': [], 'embedding_type': [], 'r2_score': [], 'model': []}\n",
        "\n",
        "        # Iterate through available embedding types\n",
        "        for emb_type, emb_data in self.embedding_data.items():\n",
        "            print(f\"\\nAnalyzing {emb_type} embeddings...\")\n",
        "            embeddings = emb_data['embeddings']\n",
        "\n",
        "            # Analyze each property\n",
        "            for prop in properties:\n",
        "                y = self.prop_df[prop].values\n",
        "\n",
        "                # Skip if property has too little variance\n",
        "                if np.var(y) < 1e-6:\n",
        "                    continue\n",
        "\n",
        "                # Try both linear and non-linear models\n",
        "                for model_type, model in [('Linear', Ridge(alpha=1.0)),\n",
        "                                         ('RandomForest', RandomForestRegressor(n_estimators=50))]:\n",
        "                    # Perform cross-validation\n",
        "                    try:\n",
        "                        cv_scores = cross_val_score(model, embeddings, y,\n",
        "                                                  cv=KFold(n_splits=min(cv, len(y)), shuffle=True, random_state=42),\n",
        "                                                  scoring='r2')\n",
        "\n",
        "                        # Store results\n",
        "                        mean_r2 = np.mean(cv_scores)\n",
        "                        property_results['property'].append(prop)\n",
        "                        property_results['embedding_type'].append(emb_type)\n",
        "                        property_results['r2_score'].append(mean_r2)\n",
        "                        property_results['model'].append(model_type)\n",
        "\n",
        "                        print(f\"{prop} - {model_type}: R² = {mean_r2:.4f}\")\n",
        "                    except Exception as e:\n",
        "                        print(f\"Error analyzing {prop}: {e}\")\n",
        "\n",
        "        # Convert to DataFrame and store\n",
        "        results_df = pd.DataFrame(property_results)\n",
        "        self.results['property_prediction'] = results_df\n",
        "\n",
        "        return results_df\n",
        "\n",
        "    def analyze_embedding_sensitivity(self, properties=None, percentile_threshold=10):\n",
        "        \"\"\"\n",
        "        Analyze embedding sensitivity to properties\n",
        "\n",
        "        Args:\n",
        "            properties: List of property column names to analyze (default: analyze all numeric)\n",
        "            percentile_threshold: Percentile threshold for defining similar/different molecules\n",
        "        \"\"\"\n",
        "        print(\"\\n=== Embedding Sensitivity Analysis ===\")\n",
        "\n",
        "        # Select properties to analyze\n",
        "        if properties is None:\n",
        "            numeric_cols = self.prop_df.select_dtypes(include=np.number).columns\n",
        "            properties = [col for col in numeric_cols if col != 'graph_id']\n",
        "\n",
        "        # Initialize results storage\n",
        "        sensitivity_results = {\n",
        "            'property': [],\n",
        "            'embedding_type': [],\n",
        "            'avg_dist_similar': [],\n",
        "            'avg_dist_different': [],\n",
        "            'sensitivity_ratio': []\n",
        "        }\n",
        "\n",
        "        # Iterate through available embedding types\n",
        "        for emb_type, emb_data in self.embedding_data.items():\n",
        "            print(f\"\\nAnalyzing {emb_type} embeddings...\")\n",
        "            embeddings = emb_data['embeddings']\n",
        "\n",
        "            # Analyze each property\n",
        "            for prop in properties:\n",
        "                y = self.prop_df[prop].values\n",
        "\n",
        "                # Skip if property has too little variance\n",
        "                if np.var(y) < 1e-6:\n",
        "                    continue\n",
        "\n",
        "                # Compute low and high thresholds for the property\n",
        "                low_thresh = np.percentile(y, percentile_threshold)\n",
        "                high_thresh = np.percentile(y, 100 - percentile_threshold)\n",
        "\n",
        "                if low_thresh == high_thresh:\n",
        "                    print(f\"Skipping {prop} - insufficient variance\")\n",
        "                    continue\n",
        "\n",
        "                # Define similar and different molecule pairs\n",
        "                low_indices = np.where(y <= low_thresh)[0]\n",
        "                high_indices = np.where(y >= high_thresh)[0]\n",
        "\n",
        "                if len(low_indices) < 5 or len(high_indices) < 5:\n",
        "                    print(f\"Skipping {prop} - insufficient samples in partition\")\n",
        "                    continue\n",
        "\n",
        "                # Calculate distances between similar molecules\n",
        "                similar_dists = []\n",
        "\n",
        "                # Sample pairs from low group\n",
        "                np.random.seed(42)\n",
        "                low_pairs = np.random.choice(low_indices, size=(min(1000, len(low_indices) * (len(low_indices) - 1) // 2), 2), replace=True)\n",
        "                for i, j in low_pairs:\n",
        "                    if i != j:\n",
        "                        similar_dists.append(np.linalg.norm(embeddings[i] - embeddings[j]))\n",
        "\n",
        "                # Sample pairs from high group\n",
        "                high_pairs = np.random.choice(high_indices, size=(min(1000, len(high_indices) * (len(high_indices) - 1) // 2), 2), replace=True)\n",
        "                for i, j in high_pairs:\n",
        "                    if i != j:\n",
        "                        similar_dists.append(np.linalg.norm(embeddings[i] - embeddings[j]))\n",
        "\n",
        "                # Calculate distances between different molecules\n",
        "                different_dists = []\n",
        "\n",
        "                # Sample pairs between low and high groups\n",
        "                for _ in range(min(2000, len(low_indices) * len(high_indices))):\n",
        "                    i = np.random.choice(low_indices)\n",
        "                    j = np.random.choice(high_indices)\n",
        "                    different_dists.append(np.linalg.norm(embeddings[i] - embeddings[j]))\n",
        "\n",
        "                # Compute average distances and sensitivity ratio\n",
        "                avg_similar = np.mean(similar_dists)\n",
        "                avg_different = np.mean(different_dists)\n",
        "                sensitivity_ratio = avg_different / avg_similar if avg_similar > 0 else 0\n",
        "\n",
        "                # Store results\n",
        "                sensitivity_results['property'].append(prop)\n",
        "                sensitivity_results['embedding_type'].append(emb_type)\n",
        "                sensitivity_results['avg_dist_similar'].append(avg_similar)\n",
        "                sensitivity_results['avg_dist_different'].append(avg_different)\n",
        "                sensitivity_results['sensitivity_ratio'].append(sensitivity_ratio)\n",
        "\n",
        "                print(f\"{prop}: Ratio = {sensitivity_ratio:.4f} (Similar: {avg_similar:.4f}, Different: {avg_different:.4f})\")\n",
        "\n",
        "        # Convert to DataFrame and store\n",
        "        results_df = pd.DataFrame(sensitivity_results)\n",
        "        self.results['embedding_sensitivity'] = results_df\n",
        "\n",
        "        return results_df\n",
        "\n",
        "    def analyze_feature_importance(self, embedding_types=None):\n",
        "        \"\"\"\n",
        "        Analyze importance of molecular features in determining embedding structure\n",
        "\n",
        "        Args:\n",
        "            embedding_types: List of embedding types to analyze (default: analyze all)\n",
        "        \"\"\"\n",
        "        print(\"\\n=== Feature Importance Analysis ===\")\n",
        "\n",
        "        # Select embedding types to analyze\n",
        "        if embedding_types is None:\n",
        "            embedding_types = list(self.embedding_data.keys())\n",
        "\n",
        "        # Initialize results storage\n",
        "        feature_results = {'feature': [], 'embedding_type': [], 'importance': []}\n",
        "\n",
        "        # Get feature columns (exclude graph_id and target properties)\n",
        "        feature_cols = [col for col in self.prop_df.columns if col != 'graph_id']\n",
        "        X = self.prop_df[feature_cols].values\n",
        "\n",
        "        # Iterate through embedding types\n",
        "        for emb_type in embedding_types:\n",
        "            if emb_type not in self.embedding_data:\n",
        "                print(f\"Warning: {emb_type} not found in embeddings. Skipping.\")\n",
        "                continue\n",
        "\n",
        "            print(f\"\\nAnalyzing {emb_type} embeddings...\")\n",
        "            embeddings = self.embedding_data[emb_type]['embeddings']\n",
        "\n",
        "            # Train a RandomForest model for each embedding dimension\n",
        "            emb_dim = embeddings.shape[1]\n",
        "            importance_matrix = np.zeros((len(feature_cols), emb_dim))\n",
        "\n",
        "            for dim in range(emb_dim):\n",
        "                # Extract target embedding dimension\n",
        "                y = embeddings[:, dim]\n",
        "\n",
        "                # Train model\n",
        "                model = RandomForestRegressor(n_estimators=50, max_depth=6, random_state=42)\n",
        "                model.fit(X, y)\n",
        "\n",
        "                # Store feature importances\n",
        "                importance_matrix[:, dim] = model.feature_importances_\n",
        "\n",
        "            # Aggregate importance across dimensions (mean importance)\n",
        "            mean_importance = np.mean(importance_matrix, axis=1)\n",
        "\n",
        "            # Store results for each feature\n",
        "            for i, feature in enumerate(feature_cols):\n",
        "                feature_results['feature'].append(feature)\n",
        "                feature_results['embedding_type'].append(emb_type)\n",
        "                feature_results['importance'].append(mean_importance[i])\n",
        "\n",
        "        # Convert to DataFrame and store\n",
        "        results_df = pd.DataFrame(feature_results)\n",
        "        self.results['feature_importance'] = results_df\n",
        "\n",
        "        return results_df\n",
        "\n",
        "    def visualize_embeddings(self, properties=None, method='umap', save_dir='./figures'):\n",
        "        \"\"\"\n",
        "        Create visualizations of embedding spaces colored by properties\n",
        "\n",
        "        Args:\n",
        "            properties: List of properties to color points by (default: top 4 most predictable)\n",
        "            method: Dimension reduction method ('pca', 'tsne', or 'umap')\n",
        "            save_dir: Directory to save visualization figures\n",
        "        \"\"\"\n",
        "        print(\"\\n=== Embedding Visualization ===\")\n",
        "        os.makedirs(save_dir, exist_ok=True)\n",
        "\n",
        "        # If no properties specified, use top 4 from property prediction results\n",
        "        if properties is None and 'property_prediction' in self.results:\n",
        "            top_props = (self.results['property_prediction']\n",
        "                         .groupby('property')['r2_score']\n",
        "                         .max()\n",
        "                         .sort_values(ascending=False)\n",
        "                         .head(4)\n",
        "                         .index.tolist())\n",
        "            properties = top_props\n",
        "        elif properties is None:\n",
        "            # Use a few common properties if available\n",
        "            potential_props = ['prop_num_nodes', 'prop_avg_node_degree',\n",
        "                              'prop_clustering_coefficient', 'feat_is_connected']\n",
        "            properties = [p for p in potential_props if p in self.prop_df.columns][:4]\n",
        "\n",
        "        print(f\"Visualizing with properties: {properties}\")\n",
        "\n",
        "        # Iterate through embedding types\n",
        "        for emb_type, emb_data in self.embedding_data.items():\n",
        "            print(f\"\\nVisualizing {emb_type} embeddings...\")\n",
        "            embeddings = emb_data['embeddings']\n",
        "\n",
        "            # Apply dimension reduction\n",
        "            if method == 'pca':\n",
        "                reducer = PCA(n_components=2, random_state=42)\n",
        "                emb_2d = reducer.fit_transform(embeddings)\n",
        "                method_name = 'PCA'\n",
        "            elif method == 'tsne':\n",
        "                reducer = TSNE(n_components=2, random_state=42, perplexity=30)\n",
        "                emb_2d = reducer.fit_transform(embeddings)\n",
        "                method_name = 't-SNE'\n",
        "            else:  # umap\n",
        "                reducer = umap.UMAP(n_components=2, random_state=42)\n",
        "                emb_2d = reducer.fit_transform(embeddings)\n",
        "                method_name = 'UMAP'\n",
        "\n",
        "            # Create DataFrame for visualization\n",
        "            vis_df = pd.DataFrame({\n",
        "                'x': emb_2d[:, 0],\n",
        "                'y': emb_2d[:, 1]\n",
        "            })\n",
        "\n",
        "            # Create plots for each property\n",
        "            for prop in properties:\n",
        "                if prop not in self.prop_df.columns:\n",
        "                    print(f\"Warning: Property {prop} not found in data. Skipping.\")\n",
        "                    continue\n",
        "\n",
        "                # Add property values\n",
        "                vis_df['property'] = self.prop_df[prop].values\n",
        "\n",
        "                # Create figure\n",
        "                plt.figure(figsize=(10, 8))\n",
        "\n",
        "                # Create scatter plot colored by property\n",
        "                if prop.startswith('feat_') and np.all(np.isin(vis_df['property'].unique(), [0, 1])):\n",
        "                    # Categorical (boolean) property\n",
        "                    sns.scatterplot(data=vis_df, x='x', y='y', hue='property',\n",
        "                                   palette='viridis', alpha=0.7)\n",
        "                else:\n",
        "                    # Continuous property\n",
        "                    plt.scatter(vis_df['x'], vis_df['y'], c=vis_df['property'],\n",
        "                               cmap='viridis', alpha=0.7)\n",
        "                    plt.colorbar(label=prop)\n",
        "\n",
        "                # Set labels and title\n",
        "                plt.xlabel(f'{method_name} Dimension 1')\n",
        "                plt.ylabel(f'{method_name} Dimension 2')\n",
        "                plt.title(f'{emb_type} Embeddings - Colored by {prop}')\n",
        "\n",
        "                # Save figure\n",
        "                plt.tight_layout()\n",
        "                filename = f'{emb_type}_{method}_{prop}.png'\n",
        "                plt.savefig(os.path.join(save_dir, filename), dpi=300)\n",
        "                plt.close()\n",
        "\n",
        "        print(f\"Saved visualization figures to {save_dir}\")\n",
        "\n",
        "    def compare_property_prediction(self):\n",
        "        \"\"\"Compare property prediction quality between pre and post training\"\"\"\n",
        "        if 'property_prediction' not in self.results:\n",
        "            print(\"Run analyze_property_prediction first!\")\n",
        "            return\n",
        "\n",
        "        results = self.results['property_prediction']\n",
        "\n",
        "        # Filter to just pre and post training (not intermediate epochs)\n",
        "        compare_df = results[results['embedding_type'].isin(['pre_training', 'post_training'])]\n",
        "\n",
        "        # Pivot to get pre-post comparison\n",
        "        pivot_df = compare_df.pivot_table(\n",
        "            index=['property', 'model'],\n",
        "            columns='embedding_type',\n",
        "            values='r2_score'\n",
        "        ).reset_index()\n",
        "\n",
        "        # Calculate improvement\n",
        "        if 'pre_training' in pivot_df.columns and 'post_training' in pivot_df.columns:\n",
        "            pivot_df['improvement'] = pivot_df['post_training'] - pivot_df['pre_training']\n",
        "            pivot_df['relative_improvement'] = pivot_df['improvement'] / (pivot_df['pre_training'].abs() + 1e-6)\n",
        "\n",
        "            # Sort by absolute improvement\n",
        "            pivot_df = pivot_df.sort_values('improvement', ascending=False)\n",
        "\n",
        "            # Print results\n",
        "            print(\"\\n=== Property Prediction Comparison ===\")\n",
        "            print(pivot_df)\n",
        "\n",
        "            # Visualize improvements\n",
        "            plt.figure(figsize=(12, 8))\n",
        "\n",
        "            # Filter to just RandomForest results for better visualization\n",
        "            rf_df = pivot_df[pivot_df['model'] == 'RandomForest']\n",
        "\n",
        "            # Get top 15 by absolute change for plotting\n",
        "            plot_df = rf_df.head(15)\n",
        "\n",
        "            # Create bar chart\n",
        "            bars = plt.barh(plot_df['property'], plot_df['improvement'], color='skyblue')\n",
        "\n",
        "            # Highlight negative improvements in red\n",
        "            for i, imp in enumerate(plot_df['improvement']):\n",
        "                if imp < 0:\n",
        "                    bars[i].set_color('salmon')\n",
        "\n",
        "            plt.axvline(x=0, color='black', linestyle='-', linewidth=0.5)\n",
        "            plt.xlabel('Improvement in R² Score (Post - Pre)')\n",
        "            plt.title('Changes in Property Predictability After Training')\n",
        "            plt.tight_layout()\n",
        "\n",
        "            # Save figure\n",
        "            plt.savefig('property_prediction_improvement.png', dpi=300)\n",
        "            plt.close()\n",
        "\n",
        "            return pivot_df\n",
        "        else:\n",
        "            print(\"Missing either pre-training or post-training embeddings\")\n",
        "            return None\n",
        "\n",
        "    def compare_feature_importance(self):\n",
        "        \"\"\"Compare feature importance between pre and post training\"\"\"\n",
        "        if 'feature_importance' not in self.results:\n",
        "            print(\"Run analyze_feature_importance first!\")\n",
        "            return\n",
        "\n",
        "        results = self.results['feature_importance']\n",
        "\n",
        "        # Filter to just pre and post training (not intermediate epochs)\n",
        "        compare_df = results[results['embedding_type'].isin(['pre_training', 'post_training'])]\n",
        "\n",
        "        # Pivot to get pre-post comparison\n",
        "        pivot_df = compare_df.pivot_table(\n",
        "            index='feature',\n",
        "            columns='embedding_type',\n",
        "            values='importance'\n",
        "        ).reset_index()\n",
        "\n",
        "        # Calculate changes\n",
        "        if 'pre_training' in pivot_df.columns and 'post_training' in pivot_df.columns:\n",
        "            pivot_df['abs_change'] = pivot_df['post_training'] - pivot_df['pre_training']\n",
        "            pivot_df['rel_change'] = pivot_df['abs_change'] / (pivot_df['pre_training'] + 1e-6)\n",
        "\n",
        "            # Sort by absolute change\n",
        "            pivot_df = pivot_df.sort_values('abs_change', ascending=False)\n",
        "\n",
        "            # Print results\n",
        "            print(\"\\n=== Feature Importance Comparison ===\")\n",
        "            print(pivot_df)\n",
        "\n",
        "            # Visualize changes\n",
        "            plt.figure(figsize=(12, 10))\n",
        "\n",
        "            # Get top and bottom 10 by absolute change for plotting\n",
        "            top_df = pivot_df.head(10)\n",
        "            bottom_df = pivot_df.tail(10)\n",
        "            plot_df = pd.concat([top_df, bottom_df])\n",
        "\n",
        "            # Create bar chart\n",
        "            bars = plt.barh(plot_df['feature'], plot_df['abs_change'], color='skyblue')\n",
        "\n",
        "            # Highlight negative changes in red\n",
        "            for i, change in enumerate(plot_df['abs_change']):\n",
        "                if change < 0:\n",
        "                    bars[i].set_color('salmon')\n",
        "\n",
        "            plt.axvline(x=0, color='black', linestyle='-', linewidth=0.5)\n",
        "            plt.xlabel('Change in Feature Importance (Post - Pre)')\n",
        "            plt.title('Changes in Feature Importance After Training')\n",
        "            plt.tight_layout()\n",
        "\n",
        "            # Save figure\n",
        "            plt.savefig('feature_importance_change.png', dpi=300)\n",
        "            plt.close()\n",
        "\n",
        "            return pivot_df\n",
        "        else:\n",
        "            print(\"Missing either pre-training or post-training embeddings\")\n",
        "            return None\n",
        "\n",
        "    def compare_sensitivity(self):\n",
        "        \"\"\"Compare embedding sensitivity between pre and post training\"\"\"\n",
        "        if 'embedding_sensitivity' not in self.results:\n",
        "            print(\"Run analyze_embedding_sensitivity first!\")\n",
        "            return\n",
        "\n",
        "        results = self.results['embedding_sensitivity']\n",
        "\n",
        "        # Filter to just pre and post training (not intermediate epochs)\n",
        "        compare_df = results[results['embedding_type'].isin(['pre_training', 'post_training'])]\n",
        "\n",
        "        # Pivot to get pre-post comparison for sensitivity ratio\n",
        "        pivot_df = compare_df.pivot_table(\n",
        "            index='property',\n",
        "            columns='embedding_type',\n",
        "            values='sensitivity_ratio'\n",
        "        ).reset_index()\n",
        "\n",
        "        # Calculate changes\n",
        "        if 'pre_training' in pivot_df.columns and 'post_training' in pivot_df.columns:\n",
        "            pivot_df['abs_change'] = pivot_df['post_training'] - pivot_df['pre_training']\n",
        "            pivot_df['rel_change'] = pivot_df['abs_change'] / (pivot_df['pre_training'] + 1e-6)\n",
        "\n",
        "            # Sort by absolute change\n",
        "            pivot_df = pivot_df.sort_values('abs_change', ascending=False)\n",
        "\n",
        "            # Print results\n",
        "            print(\"\\n=== Sensitivity Ratio Comparison ===\")\n",
        "            print(pivot_df)\n",
        "\n",
        "            # Visualize changes\n",
        "            plt.figure(figsize=(12, 8))\n",
        "\n",
        "            # Get top 15 by absolute change for plotting\n",
        "            plot_df = pivot_df.head(15)\n",
        "\n",
        "            # Create bar chart\n",
        "            bars = plt.barh(plot_df['property'], plot_df['abs_change'], color='skyblue')\n",
        "\n",
        "            # Highlight negative changes in red\n",
        "            for i, change in enumerate(plot_df['abs_change']):\n",
        "                if change < 0:\n",
        "                    bars[i].set_color('salmon')\n",
        "\n",
        "            plt.axvline(x=0, color='black', linestyle='-', linewidth=0.5)\n",
        "            plt.xlabel('Change in Sensitivity Ratio (Post - Pre)')\n",
        "            plt.title('Changes in Embedding Sensitivity After Training')\n",
        "            plt.tight_layout()\n",
        "\n",
        "            # Save figure\n",
        "            plt.savefig('sensitivity_change.png', dpi=300)\n",
        "            plt.close()\n",
        "\n",
        "            return pivot_df\n",
        "        else:\n",
        "            print(\"Missing either pre-training or post-training embeddings\")\n",
        "            return None\n",
        "\n",
        "    def run_all_analyses(self):\n",
        "        \"\"\"Run all analysis methods and generate a comprehensive report\"\"\"\n",
        "        print(\"Starting comprehensive embedding analysis...\")\n",
        "\n",
        "        # Prepare data\n",
        "        self.prepare_property_data()\n",
        "\n",
        "        # Run all analyses\n",
        "        self.analyze_property_prediction()\n",
        "        self.analyze_embedding_sensitivity()\n",
        "        self.analyze_feature_importance()\n",
        "\n",
        "        # Generate comparison reports\n",
        "        self.compare_property_prediction()\n",
        "        self.compare_feature_importance()\n",
        "        self.compare_sensitivity()\n",
        "\n",
        "        # Create visualizations\n",
        "        self.visualize_embeddings()\n",
        "\n",
        "        print(\"\\nAnalysis complete! Results and visualizations have been saved.\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "a4f1e951",
      "metadata": {
        "id": "a4f1e951"
      },
      "outputs": [],
      "source": [
        "def create_enhanced_visualizations(analyzer, output_dir='./enhanced_figures'):\n",
        "    \"\"\"Create enhanced visualizations for better analysis communication\"\"\"\n",
        "    import os\n",
        "    import numpy as np\n",
        "    import pandas as pd\n",
        "    import matplotlib.pyplot as plt\n",
        "    import seaborn as sns\n",
        "    from sklearn.decomposition import PCA\n",
        "\n",
        "    os.makedirs(output_dir, exist_ok=True)\n",
        "\n",
        "    # ---- 1. Property Prediction Power Chart ----\n",
        "    if 'property_prediction' in analyzer.results:\n",
        "        plt.figure(figsize=(12, 10))\n",
        "\n",
        "        # Filter to important properties and organize by category\n",
        "        pred_df = analyzer.results['property_prediction']\n",
        "\n",
        "        # Filter to RandomForest results for better visualization\n",
        "        rf_df = pred_df[pred_df['model'] == 'RandomForest']\n",
        "\n",
        "        # Create pivot table for pre vs post\n",
        "        pivot_df = rf_df.pivot_table(\n",
        "            index='property',\n",
        "            columns='embedding_type',\n",
        "            values='r2_score'\n",
        "        ).reset_index()\n",
        "\n",
        "        # Add improvement column\n",
        "        if 'pre_training' in pivot_df.columns and 'post_training' in pivot_df.columns:\n",
        "            pivot_df['improvement'] = pivot_df['post_training'] - pivot_df['pre_training']\n",
        "\n",
        "            # Sort by improvement\n",
        "            pivot_df = pivot_df.sort_values('improvement', ascending=False)\n",
        "\n",
        "            # Create a categorical column for property type\n",
        "            def categorize_property(prop):\n",
        "                if 'mw' in prop.lower() or 'weight' in prop.lower():\n",
        "                    return 'Molecular Weight'\n",
        "                elif 'logp' in prop.lower():\n",
        "                    return 'Lipophilicity'\n",
        "                elif 'ring' in prop.lower():\n",
        "                    return 'Ring Structure'\n",
        "                elif 'aromatic' in prop.lower():\n",
        "                    return 'Aromaticity'\n",
        "                elif 'func_' in prop.lower():\n",
        "                    return 'Functional Group'\n",
        "                elif 'contrib' in prop.lower():\n",
        "                    return 'Atom Contribution'\n",
        "                else:\n",
        "                    return 'Other Properties'\n",
        "\n",
        "            pivot_df['category'] = pivot_df['property'].apply(categorize_property)\n",
        "\n",
        "            # Filter to top 20 properties by absolute improvement\n",
        "            plot_df = pivot_df.copy()\n",
        "            plot_df['abs_improvement'] = plot_df['improvement'].abs()\n",
        "            plot_df = plot_df.sort_values('abs_improvement', ascending=False).head(20)\n",
        "\n",
        "            # Create plot\n",
        "            fig, ax = plt.subplots(figsize=(12, 10))\n",
        "\n",
        "            # Plot bars\n",
        "            bar_width = 0.35\n",
        "            x = np.arange(len(plot_df))\n",
        "\n",
        "            # Pre-training bars\n",
        "            rects1 = ax.barh(x - bar_width/2, plot_df['pre_training'], bar_width,\n",
        "                             label='Pre-training', color='skyblue', alpha=0.7)\n",
        "\n",
        "            # Post-training bars\n",
        "            rects2 = ax.barh(x + bar_width/2, plot_df['post_training'], bar_width,\n",
        "                             label='Post-training', color='orange', alpha=0.7)\n",
        "\n",
        "            # Add property names\n",
        "            property_labels = [p.replace('prop_', '').replace('func_', '').replace('ring_', '')\n",
        "                              for p in plot_df['property']]\n",
        "            ax.set_yticks(x)\n",
        "            ax.set_yticklabels(property_labels)\n",
        "\n",
        "            # Add a line at R²=0\n",
        "            ax.axvline(x=0, color='black', linestyle='-', linewidth=0.5)\n",
        "\n",
        "            # Add labels and title\n",
        "            ax.set_xlabel('R² Score (Higher is Better)')\n",
        "            ax.set_title('Property Prediction Power: Pre vs. Post Training', fontsize=14)\n",
        "            ax.legend()\n",
        "\n",
        "            # Add color-coded categories (FIXED: the indexing error was here)\n",
        "            categories = plot_df['category'].unique()\n",
        "            for i, cat in enumerate(categories):\n",
        "                # Get indices in the x array where the category matches\n",
        "                idx_in_plot_df = plot_df[plot_df['category'] == cat].index\n",
        "                # Map these indices to positions in the x array\n",
        "                positions_in_x = [j for j, idx in enumerate(plot_df.index) if idx in idx_in_plot_df]\n",
        "\n",
        "                if positions_in_x:  # If we have positions for this category\n",
        "                    min_idx = min(positions_in_x)\n",
        "                    max_idx = max(positions_in_x)\n",
        "                    ax.axhspan(min_idx - 0.5, max_idx + 0.5, alpha=0.1, color=plt.cm.tab10(i))\n",
        "                    ax.text(ax.get_xlim()[0], (min_idx + max_idx) / 2, cat,\n",
        "                            ha='left', va='center', fontsize=9, fontweight='bold')\n",
        "\n",
        "            plt.tight_layout()\n",
        "            plt.savefig(os.path.join(output_dir, 'property_prediction_power.png'), dpi=300)\n",
        "            plt.close()\n",
        "\n",
        "    # ---- 2. Property Sensitivity Heatmap ----\n",
        "    if 'embedding_sensitivity' in analyzer.results:\n",
        "        sens_df = analyzer.results['embedding_sensitivity']\n",
        "\n",
        "        # Pivot to get property x embedding type matrix of sensitivity ratios\n",
        "        pivot_sens = sens_df.pivot_table(\n",
        "            index='property',\n",
        "            columns='embedding_type',\n",
        "            values='sensitivity_ratio'\n",
        "        )\n",
        "\n",
        "        # Add a delta column (percentage change)\n",
        "        if 'pre_training' in pivot_sens.columns and 'post_training' in pivot_sens.columns:\n",
        "            delta = ((pivot_sens['post_training'] - pivot_sens['pre_training']) /\n",
        "                    pivot_sens['pre_training'] * 100)\n",
        "            pivot_sens['delta_pct'] = delta\n",
        "\n",
        "            # Sort by delta\n",
        "            pivot_sens = pivot_sens.sort_values('delta_pct', ascending=False)\n",
        "\n",
        "            # Select top properties by absolute change percentage\n",
        "            plot_sens = pivot_sens.head(20)\n",
        "\n",
        "            # Create heatmap\n",
        "            plt.figure(figsize=(12, 10))\n",
        "\n",
        "            # Format for better display\n",
        "            heatmap_df = plot_sens[['pre_training', 'post_training']].copy()\n",
        "            # Clean property names for display\n",
        "            heatmap_df.index = [idx.replace('prop_', '').replace('func_', '').replace('ring_', '')\n",
        "                              for idx in heatmap_df.index]\n",
        "\n",
        "            # Create diverging colormap centered at 1.0\n",
        "            cmap = sns.diverging_palette(220, 20, as_cmap=True)\n",
        "\n",
        "            # Plot heatmap\n",
        "            ax = sns.heatmap(heatmap_df, annot=True, fmt=\".2f\", cmap=cmap, center=1.0,\n",
        "                           linewidths=0.5, cbar_kws={\"label\": \"Sensitivity Ratio\"})\n",
        "\n",
        "            # Add delta values\n",
        "            for i, idx in enumerate(plot_sens.index):\n",
        "                delta_val = plot_sens.loc[idx, 'delta_pct']\n",
        "                color = 'green' if delta_val > 0 else 'red'\n",
        "                plt.text(2.5, i + 0.5, f\"{delta_val:.1f}%\",\n",
        "                        ha='center', va='center', fontweight='bold', color=color)\n",
        "\n",
        "            plt.title('Property Sensitivity: Pre vs. Post Training', fontsize=14)\n",
        "            plt.tight_layout()\n",
        "            plt.savefig(os.path.join(output_dir, 'property_sensitivity_heatmap.png'), dpi=300)\n",
        "            plt.close()\n",
        "\n",
        "    # ---- 3. Feature Importance Radar Chart ----\n",
        "    if 'feature_importance' in analyzer.results:\n",
        "        feat_df = analyzer.results['feature_importance']\n",
        "\n",
        "        # Filter to pre and post training\n",
        "        radar_df = feat_df[feat_df['embedding_type'].isin(['pre_training', 'post_training'])]\n",
        "\n",
        "        # Group features by category\n",
        "        def group_feature(feat):\n",
        "            if 'ring' in feat:\n",
        "                return 'Ring Structure'\n",
        "            elif 'aromatic' in feat:\n",
        "                return 'Aromaticity'\n",
        "            elif 'func_' in feat:\n",
        "                return 'Functional Group'\n",
        "            elif any(term in feat for term in ['hybridization', 'valence', 'charge']):\n",
        "                return 'Electronic Properties'\n",
        "            elif any(term in feat for term in ['path', 'diameter', 'degree']):\n",
        "                return 'Topological Properties'\n",
        "            else:\n",
        "                return 'Other'\n",
        "\n",
        "        radar_df['category'] = radar_df['feature'].apply(group_feature)\n",
        "\n",
        "        # Calculate average importance by category\n",
        "        radar_pivot = radar_df.pivot_table(\n",
        "            index='category',\n",
        "            columns='embedding_type',\n",
        "            values='importance',\n",
        "            aggfunc='mean'\n",
        "        ).reset_index()\n",
        "\n",
        "        if not radar_pivot.empty and 'pre_training' in radar_pivot.columns and 'post_training' in radar_pivot.columns:\n",
        "            # Create radar chart\n",
        "            categories = radar_pivot['category']\n",
        "            pre_values = radar_pivot['pre_training']\n",
        "            post_values = radar_pivot['post_training']\n",
        "\n",
        "            # Compute angles for the radar chart\n",
        "            N = len(categories)\n",
        "            if N > 0:  # Only proceed if we have categories\n",
        "                angles = np.linspace(0, 2*np.pi, N, endpoint=False).tolist()\n",
        "                angles += angles[:1]  # Close the loop\n",
        "\n",
        "                # Add the values to complete the loop\n",
        "                pre_values = pre_values.tolist()\n",
        "                pre_values += pre_values[:1]\n",
        "                post_values = post_values.tolist()\n",
        "                post_values += post_values[:1]\n",
        "\n",
        "                # Create figure\n",
        "                fig, ax = plt.subplots(figsize=(10, 8), subplot_kw=dict(polar=True))\n",
        "\n",
        "                # Plot pre-training values\n",
        "                ax.plot(angles, pre_values, 'o-', linewidth=2, label='Pre-training', color='blue', alpha=0.7)\n",
        "                ax.fill(angles, pre_values, alpha=0.1, color='blue')\n",
        "\n",
        "                # Plot post-training values\n",
        "                ax.plot(angles, post_values, 'o-', linewidth=2, label='Post-training', color='orange', alpha=0.7)\n",
        "                ax.fill(angles, post_values, alpha=0.1, color='orange')\n",
        "\n",
        "                # Set category labels\n",
        "                categories = categories.tolist()\n",
        "                categories += categories[:1]  # Complete the loop\n",
        "                ax.set_xticks(angles)\n",
        "                ax.set_xticklabels(categories, fontsize=10, fontweight='bold')\n",
        "\n",
        "                # Add legend and title\n",
        "                ax.legend(loc='upper right')\n",
        "                plt.title('Feature Importance by Category: Pre vs. Post Training', fontsize=14)\n",
        "\n",
        "                plt.tight_layout()\n",
        "                plt.savefig(os.path.join(output_dir, 'feature_importance_radar.png'), dpi=300)\n",
        "                plt.close()\n",
        "\n",
        "    # ---- 4. Ring Structure Analysis ----\n",
        "    # Extract ring-related properties\n",
        "    ring_props = [prop for prop in analyzer.prop_df.columns if 'ring' in prop.lower()]\n",
        "\n",
        "    if ring_props and 'embedding_sensitivity' in analyzer.results:\n",
        "        # Filter sensitivity results to ring properties\n",
        "        ring_sens = analyzer.results['embedding_sensitivity']\n",
        "        ring_sens = ring_sens[ring_sens['property'].isin(ring_props)]\n",
        "\n",
        "        # Create a comparison of ring sensitivity\n",
        "        ring_pivot = ring_sens.pivot_table(\n",
        "            index='property',\n",
        "            columns='embedding_type',\n",
        "            values='sensitivity_ratio'\n",
        "        ).reset_index()\n",
        "\n",
        "        # Add improvement column\n",
        "        if 'pre_training' in ring_pivot.columns and 'post_training' in ring_pivot.columns:\n",
        "            ring_pivot['pct_change'] = ((ring_pivot['post_training'] - ring_pivot['pre_training']) /\n",
        "                                      ring_pivot['pre_training'] * 100)\n",
        "\n",
        "            # Categorize by ring type\n",
        "            def ring_category(prop):\n",
        "                if 'single' in prop:\n",
        "                    return 'Single Rings'\n",
        "                elif 'fused' in prop:\n",
        "                    return 'Fused Rings'\n",
        "                elif 'spiro' in prop:\n",
        "                    return 'Spiro Rings'\n",
        "                elif 'bridged' in prop:\n",
        "                    return 'Bridged Rings'\n",
        "                elif 'sizes' in prop:\n",
        "                    # Extract ring size\n",
        "                    size = prop.split('_')[-1]\n",
        "                    return f'Ring Size {size}'\n",
        "                else:\n",
        "                    return 'General Ring Properties'\n",
        "\n",
        "            ring_pivot['category'] = ring_pivot['property'].apply(ring_category)\n",
        "\n",
        "            if not ring_pivot.empty:  # Only proceed if we have data\n",
        "                # Create grouped bar chart\n",
        "                plt.figure(figsize=(12, 8))\n",
        "\n",
        "                # Group by category and sort\n",
        "                ring_pivot = ring_pivot.sort_values(['category', 'pct_change'])\n",
        "\n",
        "                # Set positions and width\n",
        "                pos = np.arange(len(ring_pivot))\n",
        "                width = 0.35\n",
        "\n",
        "                # Create bars\n",
        "                fig, ax = plt.subplots(figsize=(12, 8))\n",
        "\n",
        "                # Pre-training bars\n",
        "                pre_bars = ax.barh(pos - width/2, ring_pivot['pre_training'], width,\n",
        "                                  label='Pre-training', color='skyblue')\n",
        "\n",
        "                # Post-training bars\n",
        "                post_bars = ax.barh(pos + width/2, ring_pivot['post_training'], width,\n",
        "                                   label='Post-training', color='orange')\n",
        "\n",
        "                # Add property labels\n",
        "                ax.set_yticks(pos)\n",
        "                # Clean up property names for display\n",
        "                ax.set_yticklabels([p.replace('ring_ring_', '') for p in ring_pivot['property']])\n",
        "\n",
        "                # Add a reference line at 1.0\n",
        "                ax.axvline(x=1.0, color='gray', linestyle='--', alpha=0.7)\n",
        "\n",
        "                # Add labels and title\n",
        "                ax.set_xlabel('Sensitivity Ratio (Higher = Better Separation)')\n",
        "                ax.set_title('Ring Structure Sensitivity: Pre vs. Post Training', fontsize=14)\n",
        "                ax.legend()\n",
        "\n",
        "                # Add percentage change annotations\n",
        "                for i, (_, row) in enumerate(ring_pivot.iterrows()):\n",
        "                    pct = row['pct_change']\n",
        "                    color = 'green' if pct > 0 else 'red'\n",
        "                    ax.text(max(row['pre_training'], row['post_training']) + 0.05, i,\n",
        "                           f\"{pct:.1f}%\", va='center', color=color, fontweight='bold')\n",
        "\n",
        "                # Highlight categories with background colors\n",
        "                categories = ring_pivot['category'].unique()\n",
        "                for i, cat in enumerate(categories):\n",
        "                    # Get indices in the current plot\n",
        "                    positions = [j for j, (_, row) in enumerate(ring_pivot.iterrows()) if row['category'] == cat]\n",
        "\n",
        "                    if positions:  # If we have positions for this category\n",
        "                        min_pos = min(positions)\n",
        "                        max_pos = max(positions)\n",
        "                        ax.axhspan(min_pos - 0.5, max_pos + 0.5, color=plt.cm.Pastel1(i), alpha=0.3)\n",
        "                        # Add category label\n",
        "                        ax.text(ax.get_xlim()[0] - 0.1, (min_pos + max_pos) / 2, cat,\n",
        "                               ha='right', va='center', fontweight='bold', fontsize=10)\n",
        "\n",
        "                plt.tight_layout()\n",
        "                plt.savefig(os.path.join(output_dir, 'ring_structure_sensitivity.png'), dpi=300)\n",
        "                plt.close()\n",
        "\n",
        "    # ---- 5. Functional Group Impact ----\n",
        "    # Extract functional group related properties\n",
        "    func_props = [prop for prop in analyzer.prop_df.columns if 'func_' in prop]\n",
        "\n",
        "    if func_props and 'embedding_sensitivity' in analyzer.results and 'feature_importance' in analyzer.results:\n",
        "        # Get sensitivities for functional groups\n",
        "        func_sens = analyzer.results['embedding_sensitivity']\n",
        "        func_sens = func_sens[func_sens['property'].isin(func_props)]\n",
        "\n",
        "        # Get importance for functional groups\n",
        "        func_imp = analyzer.results['feature_importance']\n",
        "        func_imp = func_imp[func_imp['feature'].isin(func_props)]\n",
        "\n",
        "        # Create a combined dataframe\n",
        "        func_data = []\n",
        "\n",
        "        for prop in func_props:\n",
        "            # Get sensitivity data\n",
        "            sens_pre = func_sens[(func_sens['property'] == prop) &\n",
        "                                (func_sens['embedding_type'] == 'pre_training')]['sensitivity_ratio'].values\n",
        "            sens_post = func_sens[(func_sens['property'] == prop) &\n",
        "                                 (func_sens['embedding_type'] == 'post_training')]['sensitivity_ratio'].values\n",
        "\n",
        "            # Get importance data\n",
        "            imp_pre = func_imp[(func_imp['feature'] == prop) &\n",
        "                              (func_imp['embedding_type'] == 'pre_training')]['importance'].values\n",
        "            imp_post = func_imp[(func_imp['feature'] == prop) &\n",
        "                               (func_imp['embedding_type'] == 'post_training')]['importance'].values\n",
        "\n",
        "            # Only add if we have all data\n",
        "            if len(sens_pre) > 0 and len(sens_post) > 0 and len(imp_pre) > 0 and len(imp_post) > 0:\n",
        "                func_data.append({\n",
        "                    'property': prop,\n",
        "                    'sensitivity_pre': sens_pre[0],\n",
        "                    'sensitivity_post': sens_post[0],\n",
        "                    'importance_pre': imp_pre[0],\n",
        "                    'importance_post': imp_post[0],\n",
        "                    'sensitivity_change': (sens_post[0] - sens_pre[0]) / sens_pre[0] * 100,\n",
        "                    'importance_change': (imp_post[0] - imp_pre[0]) / imp_pre[0] * 100\n",
        "                })\n",
        "\n",
        "        # Create dataframe\n",
        "        if func_data:\n",
        "            func_df = pd.DataFrame(func_data)\n",
        "\n",
        "            if not func_df.empty:  # Only proceed if we have data\n",
        "                # Create a quad chart (importance vs sensitivity)\n",
        "                fig, ax = plt.subplots(figsize=(10, 8))\n",
        "\n",
        "                # Clean property names\n",
        "                func_df['display_name'] = func_df['property'].str.replace('func_', '')\n",
        "\n",
        "                # Create arrows from pre to post\n",
        "                for _, row in func_df.iterrows():\n",
        "                    ax.arrow(row['importance_pre'], row['sensitivity_pre'],\n",
        "                            row['importance_post'] - row['importance_pre'],\n",
        "                            row['sensitivity_post'] - row['sensitivity_pre'],\n",
        "                            head_width=0.01, head_length=0.02, fc='black', ec='black', length_includes_head=True)\n",
        "\n",
        "                    # Add property label at midpoint\n",
        "                    mid_x = (row['importance_pre'] + row['importance_post']) / 2\n",
        "                    mid_y = (row['sensitivity_pre'] + row['sensitivity_post']) / 2\n",
        "                    ax.text(mid_x, mid_y, row['display_name'], fontsize=9,\n",
        "                           ha='center', va='center', bbox=dict(facecolor='white', alpha=0.7, boxstyle='round'))\n",
        "\n",
        "                # Add quadrant lines\n",
        "                imp_min = min(func_df[['importance_pre', 'importance_post']].values.flatten())\n",
        "                imp_max = max(func_df[['importance_pre', 'importance_post']].values.flatten())\n",
        "                sens_min = min(func_df[['sensitivity_pre', 'sensitivity_post']].values.flatten())\n",
        "                sens_max = max(func_df[['sensitivity_pre', 'sensitivity_post']].values.flatten())\n",
        "\n",
        "                imp_mid = (imp_min + imp_max) / 2\n",
        "                sens_mid = (sens_min + sens_max) / 2\n",
        "\n",
        "                ax.axhline(y=sens_mid, color='gray', linestyle='--', alpha=0.5)\n",
        "                ax.axvline(x=imp_mid, color='gray', linestyle='--', alpha=0.5)\n",
        "\n",
        "                # Set axis limits with some padding\n",
        "                ax.set_xlim(imp_min - 0.01, imp_max + 0.01)\n",
        "                ax.set_ylim(sens_min - 0.05, sens_max + 0.05)\n",
        "\n",
        "                # Add quadrant labels\n",
        "                ax.text(imp_max * 0.9, sens_max * 0.9,\n",
        "                       \"High Importance\\nHigh Sensitivity\", ha='center', va='center',\n",
        "                       bbox=dict(facecolor='lightyellow', alpha=0.7))\n",
        "\n",
        "                ax.text(imp_min * 1.1, sens_max * 0.9,\n",
        "                       \"Low Importance\\nHigh Sensitivity\", ha='center', va='center',\n",
        "                       bbox=dict(facecolor='lightblue', alpha=0.7))\n",
        "\n",
        "                ax.text(imp_max * 0.9, sens_min * 1.1,\n",
        "                       \"High Importance\\nLow Sensitivity\", ha='center', va='center',\n",
        "                       bbox=dict(facecolor='lightgreen', alpha=0.7))\n",
        "\n",
        "                ax.text(imp_min * 1.1, sens_min * 1.1,\n",
        "                       \"Low Importance\\nLow Sensitivity\", ha='center', va='center',\n",
        "                       bbox=dict(facecolor='lightgray', alpha=0.7))\n",
        "\n",
        "                # Add labels and title\n",
        "                ax.set_xlabel('Feature Importance')\n",
        "                ax.set_ylabel('Sensitivity Ratio')\n",
        "                ax.set_title('Functional Group Representation: Pre vs. Post Training', fontsize=14)\n",
        "\n",
        "                # Add legend\n",
        "                import matplotlib.lines as mlines\n",
        "                arrow = mlines.Line2D([], [], color='black', marker='>', linestyle='-',\n",
        "                                     markersize=10, label='Pre → Post')\n",
        "                ax.legend(handles=[arrow], loc='upper left')\n",
        "\n",
        "                plt.tight_layout()\n",
        "                plt.savefig(os.path.join(output_dir, 'functional_group_impact.png'), dpi=300)\n",
        "                plt.close()\n",
        "\n",
        "    print(f\"Enhanced visualizations saved to {output_dir}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "d8a8ab16",
      "metadata": {
        "id": "d8a8ab16"
      },
      "outputs": [],
      "source": [
        "def create_additional_visualizations(analyzer, output_dir='./enhanced_figures'):\n",
        "    \"\"\"Create additional visualizations focusing on property distribution,\n",
        "    feature correlations, functional group clustering, and ring structure mapping\"\"\"\n",
        "    import os\n",
        "    import numpy as np\n",
        "    import pandas as pd\n",
        "    import matplotlib.pyplot as plt\n",
        "    import seaborn as sns\n",
        "    from sklearn.decomposition import PCA\n",
        "    from matplotlib.colors import Normalize\n",
        "    import networkx as nx\n",
        "    from sklearn.metrics import silhouette_score\n",
        "    from scipy.spatial import ConvexHull\n",
        "\n",
        "    os.makedirs(output_dir, exist_ok=True)\n",
        "\n",
        "    # ---- Visualization 3: Property Distribution in Embedding Space ----\n",
        "    if any(emb_type in analyzer.embedding_data for emb_type in ['pre_training', 'post_training']):\n",
        "        # Get pre and post training embeddings\n",
        "        pre_emb = analyzer.embedding_data.get('pre_training', {}).get('embeddings')\n",
        "        post_emb = analyzer.embedding_data.get('post_training', {}).get('embeddings')\n",
        "\n",
        "        if pre_emb is not None and post_emb is not None:\n",
        "            # Select a few key properties\n",
        "            key_properties = [\n",
        "                # Try to get molecular weight and LogP related properties\n",
        "                next((c for c in analyzer.prop_df.columns if 'mw' in c.lower()), None),\n",
        "                next((c for c in analyzer.prop_df.columns if 'logp' in c.lower()), None),\n",
        "                # Try other common properties\n",
        "                next((c for c in analyzer.prop_df.columns if 'aromatic' in c.lower()), None),\n",
        "                next((c for c in analyzer.prop_df.columns if 'ring' in c.lower()), None)\n",
        "            ]\n",
        "\n",
        "            # Filter out None values\n",
        "            key_properties = [p for p in key_properties if p is not None]\n",
        "\n",
        "            # If we don't have any of the target properties, select the first few numeric ones\n",
        "            if not key_properties:\n",
        "                numeric_cols = analyzer.prop_df.select_dtypes(include=np.number).columns\n",
        "                key_properties = list(numeric_cols)[:4]  # Take up to 4 properties\n",
        "\n",
        "            if key_properties:  # Only proceed if we have properties\n",
        "                # Calculate PCA for embeddings\n",
        "                pca_pre = PCA(n_components=2)\n",
        "                pca_post = PCA(n_components=2)\n",
        "\n",
        "                pre_2d = pca_pre.fit_transform(pre_emb)\n",
        "                post_2d = pca_post.fit_transform(post_emb)\n",
        "\n",
        "                # Create grid of plots\n",
        "                n_props = len(key_properties)\n",
        "                fig, axes = plt.subplots(n_props, 2, figsize=(14, 4 * n_props))\n",
        "\n",
        "                # If only one property, wrap axes in list\n",
        "                if n_props == 1:\n",
        "                    axes = [axes]\n",
        "\n",
        "                for i, prop in enumerate(key_properties):\n",
        "                    # Get property values\n",
        "                    prop_values = analyzer.prop_df[prop].values\n",
        "\n",
        "                    # Create color map\n",
        "                    norm = Normalize(vmin=np.min(prop_values), vmax=np.max(prop_values))\n",
        "\n",
        "                    # Plot pre-training\n",
        "                    ax_pre = axes[i][0]\n",
        "                    sc_pre = ax_pre.scatter(pre_2d[:, 0], pre_2d[:, 1],\n",
        "                                         c=prop_values, cmap='viridis',\n",
        "                                         alpha=0.8, norm=norm)\n",
        "                    ax_pre.set_title(f'Pre-training: {prop}')\n",
        "                    ax_pre.set_xlabel('PC1')\n",
        "                    ax_pre.set_ylabel('PC2')\n",
        "\n",
        "                    # Add contour lines if we have enough points\n",
        "                    if len(pre_2d) > 10:\n",
        "                        try:\n",
        "                            x = pre_2d[:, 0]\n",
        "                            y = pre_2d[:, 1]\n",
        "\n",
        "                            # Try to create a 2D histogram and then contour\n",
        "                            hist, x_edges, y_edges = np.histogram2d(x, y, bins=10)\n",
        "                            x_centers = (x_edges[:-1] + x_edges[1:]) / 2\n",
        "                            y_centers = (y_edges[:-1] + y_edges[1:]) / 2\n",
        "\n",
        "                            X, Y = np.meshgrid(x_centers, y_centers)\n",
        "                            ax_pre.contour(X, Y, hist.T, colors='black', alpha=0.3, levels=3)\n",
        "                        except:\n",
        "                            pass  # Skip contours if they fail\n",
        "\n",
        "                    # Plot post-training\n",
        "                    ax_post = axes[i][1]\n",
        "                    sc_post = ax_post.scatter(post_2d[:, 0], post_2d[:, 1],\n",
        "                                           c=prop_values, cmap='viridis',\n",
        "                                           alpha=0.8, norm=norm)\n",
        "                    ax_post.set_title(f'Post-training: {prop}')\n",
        "                    ax_post.set_xlabel('PC1')\n",
        "                    ax_post.set_ylabel('PC2')\n",
        "\n",
        "                    # Add contour lines if we have enough points\n",
        "                    if len(post_2d) > 10:\n",
        "                        try:\n",
        "                            x = post_2d[:, 0]\n",
        "                            y = post_2d[:, 1]\n",
        "\n",
        "                            # Try to create a 2D histogram and then contour\n",
        "                            hist, x_edges, y_edges = np.histogram2d(x, y, bins=10)\n",
        "                            x_centers = (x_edges[:-1] + x_edges[1:]) / 2\n",
        "                            y_centers = (y_edges[:-1] + y_edges[1:]) / 2\n",
        "\n",
        "                            X, Y = np.meshgrid(x_centers, y_centers)\n",
        "                            ax_post.contour(X, Y, hist.T, colors='black', alpha=0.3, levels=3)\n",
        "                        except:\n",
        "                            pass  # Skip contours if they fail\n",
        "\n",
        "                    # Add colorbar\n",
        "                    cbar = fig.colorbar(sc_post, ax=[ax_pre, ax_post], orientation='horizontal')\n",
        "                    cbar.set_label(prop)\n",
        "\n",
        "                plt.tight_layout()\n",
        "                plt.savefig(os.path.join(output_dir, 'property_distribution.png'), dpi=300)\n",
        "                plt.close()\n",
        "\n",
        "    # ---- Visualization 5: Feature Correlation Network ----\n",
        "    if 'feature_importance' in analyzer.results:\n",
        "        # Get feature importance data\n",
        "        feat_df = analyzer.results['feature_importance']\n",
        "\n",
        "        # Filter to relevant features and embedding types\n",
        "        pre_feats = feat_df[feat_df['embedding_type'] == 'pre_training']\n",
        "        post_feats = feat_df[feat_df['embedding_type'] == 'post_training']\n",
        "\n",
        "        if not pre_feats.empty and not post_feats.empty:\n",
        "            # Get top features by importance\n",
        "            top_n = 10  # Number of top features to include\n",
        "\n",
        "            pre_top = pre_feats.sort_values('importance', ascending=False).head(top_n)\n",
        "            post_top = post_feats.sort_values('importance', ascending=False).head(top_n)\n",
        "\n",
        "            # Combine unique features from both sets\n",
        "            all_features = set(pre_top['feature']).union(set(post_top['feature']))\n",
        "\n",
        "            if analyzer.prop_df is not None and len(all_features) > 1:\n",
        "                # Calculate correlations between these features\n",
        "                feature_corr = analyzer.prop_df[[f for f in all_features if f in analyzer.prop_df.columns]].corr()\n",
        "\n",
        "                # Create two network plots\n",
        "                fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(18, 8))\n",
        "\n",
        "                # Function to create network\n",
        "                def create_correlation_network(ax, features, importances_df, title):\n",
        "                    # Create graph\n",
        "                    G = nx.Graph()\n",
        "\n",
        "                    # Map feature to importance\n",
        "                    feat_imp = {f: i for f, i in zip(importances_df['feature'], importances_df['importance'])}\n",
        "\n",
        "                    # Add nodes\n",
        "                    for feature in feature_corr.columns:\n",
        "                        if feature in feat_imp:\n",
        "                            # Scale node size by importance\n",
        "                            size = feat_imp[feature] * 3000  # Scale factor\n",
        "                            G.add_node(feature, size=size)\n",
        "\n",
        "                    # Add edges for correlations\n",
        "                    for i, feat1 in enumerate(feature_corr.columns):\n",
        "                        for j, feat2 in enumerate(feature_corr.columns):\n",
        "                            if i < j and feat1 in feat_imp and feat2 in feat_imp:\n",
        "                                corr = abs(feature_corr.loc[feat1, feat2])\n",
        "                                if corr > 0.3:  # Only show stronger correlations\n",
        "                                    G.add_edge(feat1, feat2, weight=corr)\n",
        "\n",
        "                    # Draw network\n",
        "                    if len(G.nodes) > 1:  # Only draw if we have at least 2 nodes\n",
        "                        pos = nx.spring_layout(G, seed=42)\n",
        "\n",
        "                        # Draw nodes\n",
        "                        node_sizes = [G.nodes[n]['size'] for n in G.nodes]\n",
        "                        nx.draw_networkx_nodes(G, pos, ax=ax, node_size=node_sizes,\n",
        "                                               node_color='skyblue', alpha=0.8)\n",
        "\n",
        "                        # Draw edges with weights\n",
        "                        edge_weights = [G[u][v]['weight'] * 3 for u, v in G.edges]\n",
        "                        nx.draw_networkx_edges(G, pos, ax=ax, width=edge_weights,\n",
        "                                               alpha=0.5, edge_color='gray')\n",
        "\n",
        "                        # Draw labels\n",
        "                        labels = {n: n.replace('prop_', '').replace('func_', '').replace('ring_', '')\n",
        "                                 for n in G.nodes}\n",
        "                        nx.draw_networkx_labels(G, pos, ax=ax, labels=labels, font_size=8)\n",
        "\n",
        "                        # Set title\n",
        "                        ax.set_title(title)\n",
        "                        ax.axis('off')\n",
        "\n",
        "                # Create networks\n",
        "                create_correlation_network(ax1, pre_top, pre_top, \"Pre-training Feature Correlation Network\")\n",
        "                create_correlation_network(ax2, post_top, post_top, \"Post-training Feature Correlation Network\")\n",
        "\n",
        "                plt.tight_layout()\n",
        "                plt.savefig(os.path.join(output_dir, 'feature_correlation_network.png'), dpi=300)\n",
        "                plt.close()\n",
        "\n",
        "    # ---- Visualization 7: Functional Group Clustering ----\n",
        "    func_props = [prop for prop in analyzer.prop_df.columns if 'func_' in prop]\n",
        "\n",
        "    if func_props and 'pre_training' in analyzer.embedding_data and 'post_training' in analyzer.embedding_data:\n",
        "        pre_emb = analyzer.embedding_data['pre_training']['embeddings']\n",
        "        post_emb = analyzer.embedding_data['post_training']['embeddings']\n",
        "\n",
        "        # Get top 4 functional groups\n",
        "        top_func_groups = []\n",
        "        for prop in func_props:\n",
        "            # Check if the property has any True values\n",
        "            if analyzer.prop_df[prop].sum() > 0:\n",
        "                top_func_groups.append(prop)\n",
        "                if len(top_func_groups) >= 4:\n",
        "                    break\n",
        "\n",
        "        if top_func_groups:  # Only proceed if we have functional groups\n",
        "            # Create PCA projections\n",
        "            pca_pre = PCA(n_components=2).fit_transform(pre_emb)\n",
        "            pca_post = PCA(n_components=2).fit_transform(post_emb)\n",
        "\n",
        "            # Create grid of plots\n",
        "            fig, axes = plt.subplots(len(top_func_groups), 2, figsize=(12, 4 * len(top_func_groups)))\n",
        "\n",
        "            # If only one functional group, wrap axes in list\n",
        "            if len(top_func_groups) == 1:\n",
        "                axes = [axes]\n",
        "\n",
        "            for i, func in enumerate(top_func_groups):\n",
        "                # Get molecules with this functional group\n",
        "                has_func = analyzer.prop_df[func] > 0\n",
        "\n",
        "                # Plot pre-training\n",
        "                ax_pre = axes[i][0]\n",
        "\n",
        "                # Plot all points\n",
        "                ax_pre.scatter(pca_pre[:, 0], pca_pre[:, 1], color='gray', alpha=0.3)\n",
        "\n",
        "                # Highlight molecules with the functional group\n",
        "                if has_func.any():\n",
        "                    ax_pre.scatter(pca_pre[has_func, 0], pca_pre[has_func, 1],\n",
        "                                  color='red', label=f'Has {func}')\n",
        "\n",
        "                    # Try to add convex hull\n",
        "                    try:\n",
        "                        points = pca_pre[has_func]\n",
        "                        if len(points) >= 3:  # Need at least 3 points for convex hull\n",
        "                            hull = ConvexHull(points)\n",
        "                            for simplex in hull.simplices:\n",
        "                                ax_pre.plot(points[simplex, 0], points[simplex, 1], 'r-', alpha=0.5)\n",
        "                    except:\n",
        "                        pass  # Skip hull if it fails\n",
        "\n",
        "                ax_pre.set_title(f'Pre-training: {func}')\n",
        "                ax_pre.legend()\n",
        "\n",
        "                # Plot post-training\n",
        "                ax_post = axes[i][1]\n",
        "\n",
        "                # Plot all points\n",
        "                ax_post.scatter(pca_post[:, 0], pca_post[:, 1], color='gray', alpha=0.3)\n",
        "\n",
        "                # Highlight molecules with the functional group\n",
        "                if has_func.any():\n",
        "                    ax_post.scatter(pca_post[has_func, 0], pca_post[has_func, 1],\n",
        "                                   color='red', label=f'Has {func}')\n",
        "\n",
        "                    # Try to add convex hull\n",
        "                    try:\n",
        "                        points = pca_post[has_func]\n",
        "                        if len(points) >= 3:  # Need at least 3 points for convex hull\n",
        "                            hull = ConvexHull(points)\n",
        "                            for simplex in hull.simplices:\n",
        "                                ax_post.plot(points[simplex, 0], points[simplex, 1], 'r-', alpha=0.5)\n",
        "                    except:\n",
        "                        pass  # Skip hull if it fails\n",
        "\n",
        "                    # Calculate silhouette score if possible\n",
        "                    try:\n",
        "                        if sum(has_func) >= 2 and sum(~has_func) >= 2:  # Need at least 2 points in each class\n",
        "                            pre_score = silhouette_score(pca_pre, has_func)\n",
        "                            post_score = silhouette_score(pca_post, has_func)\n",
        "\n",
        "                            # Add silhouette scores to plot\n",
        "                            ax_pre.text(0.05, 0.95, f'Silhouette: {pre_score:.3f}',\n",
        "                                       transform=ax_pre.transAxes, fontsize=9,\n",
        "                                       verticalalignment='top', bbox=dict(boxstyle='round', facecolor='white', alpha=0.7))\n",
        "\n",
        "                            ax_post.text(0.05, 0.95, f'Silhouette: {post_score:.3f}',\n",
        "                                        transform=ax_post.transAxes, fontsize=9,\n",
        "                                        verticalalignment='top', bbox=dict(boxstyle='round', facecolor='white', alpha=0.7))\n",
        "                    except:\n",
        "                        pass  # Skip silhouette score if it fails\n",
        "\n",
        "                ax_post.set_title(f'Post-training: {func}')\n",
        "                ax_post.legend()\n",
        "\n",
        "            plt.tight_layout()\n",
        "            plt.savefig(os.path.join(output_dir, 'functional_group_clustering.png'), dpi=300)\n",
        "            plt.close()\n",
        "\n",
        "    # ---- Visualization 9: Ring Structure Embedding Map ----\n",
        "    ring_props = [prop for prop in analyzer.prop_df.columns if 'ring' in prop.lower()]\n",
        "\n",
        "    if ring_props and 'pre_training' in analyzer.embedding_data and 'post_training' in analyzer.embedding_data:\n",
        "        pre_emb = analyzer.embedding_data['pre_training']['embeddings']\n",
        "        post_emb = analyzer.embedding_data['post_training']['embeddings']\n",
        "\n",
        "        # Calculate PCA projections\n",
        "        pca = PCA(n_components=2)\n",
        "        # Use the combined embeddings to get consistent components\n",
        "        combined_emb = np.vstack([pre_emb, post_emb])\n",
        "        pca_combined = pca.fit_transform(combined_emb)\n",
        "\n",
        "        # Split back into pre and post\n",
        "        pre_2d = pca_combined[:len(pre_emb)]\n",
        "        post_2d = pca_combined[len(pre_emb):]\n",
        "\n",
        "        # Categorize ring types\n",
        "        ring_categories = {\n",
        "            'single': next((p for p in ring_props if 'single' in p), None),\n",
        "            'fused': next((p for p in ring_props if 'fused' in p), None),\n",
        "            'bridged': next((p for p in ring_props if 'bridged' in p), None),\n",
        "            'spiro': next((p for p in ring_props if 'spiro' in p), None),\n",
        "            'size_5': next((p for p in ring_props if 'size' in p and '5' in p), None),\n",
        "            'size_6': next((p for p in ring_props if 'size' in p and '6' in p), None)\n",
        "        }\n",
        "\n",
        "        # Filter out None values\n",
        "        ring_categories = {k: v for k, v in ring_categories.items() if v is not None}\n",
        "\n",
        "        if ring_categories:  # Only proceed if we have ring categories\n",
        "            # Get total ring count if available\n",
        "            total_rings = next((p for p in ring_props if 'total' in p), None)\n",
        "\n",
        "            # Create figure\n",
        "            fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(16, 8))\n",
        "\n",
        "            # Function to plot rings\n",
        "            def plot_ring_structures(ax, embeddings_2d, title):\n",
        "                # Plot all molecules as background\n",
        "                ax.scatter(embeddings_2d[:, 0], embeddings_2d[:, 1], color='lightgray', alpha=0.3)\n",
        "\n",
        "                # Plot each ring category with different marker\n",
        "                markers = ['o', 's', '^', 'D', 'p', '*']\n",
        "                colors = ['red', 'blue', 'green', 'purple', 'orange', 'cyan']\n",
        "\n",
        "                for i, (category, prop) in enumerate(ring_categories.items()):\n",
        "                    # Get molecules with this ring type\n",
        "                    has_ring = analyzer.prop_df[prop] > 0\n",
        "\n",
        "                    if has_ring.any():\n",
        "                        # Size by ring count if available\n",
        "                        if total_rings is not None:\n",
        "                            # Use total ring count as size, but ensure minimum size\n",
        "                            sizes = analyzer.prop_df[total_rings].values * 20 + 30\n",
        "                        else:\n",
        "                            sizes = 50  # Default size\n",
        "\n",
        "                        # Plot points with this ring type\n",
        "                        ax.scatter(embeddings_2d[has_ring, 0], embeddings_2d[has_ring, 1],\n",
        "                                  marker=markers[i % len(markers)],\n",
        "                                  s=sizes if isinstance(sizes, int) else sizes[has_ring],\n",
        "                                  color=colors[i % len(colors)],\n",
        "                                  alpha=0.7, label=category)\n",
        "\n",
        "                        # Try to add convex hull\n",
        "                        try:\n",
        "                            points = embeddings_2d[has_ring]\n",
        "                            if len(points) >= 3:  # Need at least 3 points for convex hull\n",
        "                                hull = ConvexHull(points)\n",
        "                                for simplex in hull.simplices:\n",
        "                                    ax.plot(points[simplex, 0], points[simplex, 1],\n",
        "                                           color=colors[i % len(colors)], alpha=0.3)\n",
        "                        except:\n",
        "                            pass  # Skip hull if it fails\n",
        "\n",
        "                ax.set_title(title)\n",
        "                ax.legend(loc='best')\n",
        "\n",
        "            # Plot pre and post training embeddings\n",
        "            plot_ring_structures(ax1, pre_2d, \"Pre-training Ring Structures\")\n",
        "            plot_ring_structures(ax2, post_2d, \"Post-training Ring Structures\")\n",
        "\n",
        "            plt.tight_layout()\n",
        "            plt.savefig(os.path.join(output_dir, 'ring_structure_embedding_map.png'), dpi=300)\n",
        "            plt.close()\n",
        "\n",
        "    print(f\"Additional visualizations saved to {output_dir}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "334856c6",
      "metadata": {
        "id": "334856c6"
      },
      "outputs": [],
      "source": [
        "if __name__ == \"__main__\":\n",
        "    # Set paths to your specific files\n",
        "    metadata_path = \"./embeddings/metadata/molecule_metadata_20250303_112449.pkl\"\n",
        "\n",
        "    # Define embedding files with their types\n",
        "    embedding_files = {\n",
        "        'pre_training': \"./embeddings/pre_training_embeddings_20250303_112449.pkl\",\n",
        "        'post_training': \"./embeddings/post_training_embeddings_20250303_112449.pkl\",\n",
        "        'epoch_50': \"./embeddings/epoch_50_embeddings_20250303_112449.pkl\",\n",
        "        'final': \"./embeddings/final_embeddings_20250303_112446.pkl\"\n",
        "    }\n",
        "\n",
        "    # Initialize analyzer\n",
        "    analyzer = EmbeddingAnalyzer(metadata_path, embedding_files)\n",
        "\n",
        "    # Load embeddings\n",
        "    analyzer.load_embeddings()\n",
        "\n",
        "    # After running the full analysis\n",
        "    analyzer.run_all_analyses()\n",
        "\n",
        "    # Create all visualizations\n",
        "    create_enhanced_visualizations(analyzer)\n",
        "    create_additional_visualizations(analyzer)"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python [conda env:base] *",
      "language": "python",
      "name": "conda-base-py"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.13.5"
    },
    "colab": {
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}