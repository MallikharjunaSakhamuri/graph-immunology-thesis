{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "db86c83f-f469-48d5-ae66-79ad08d0f448",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üöÄ OPTIMIZED EDGE-TYPE AWARE CAPSULE NETWORK WITH CURVATURE REGULARIZATION\n",
      "===========================================================================\n",
      "‚ö° OPTIMIZATION FEATURES:\n",
      "‚úÖ Reduced routing iterations (3‚Üí2)\n",
      "‚úÖ Cached edge type statistics\n",
      "‚úÖ Simplified normalization\n",
      "‚úÖ Precomputed merged graphs\n",
      "‚úÖ Gradient accumulation (batch_size=4, accumulate=2)\n",
      "‚úÖ Reduced logging frequency\n",
      "üÜï Curvature Regularization Loss\n",
      "Using device: cuda\n",
      "Curvature regularization: Œ≥=0.1, samples=100, similarity=normalized_adjacency\n",
      "\n",
      "==================== OPTIMIZED P WITH CURVATURE ====================\n",
      "Preparing FULL combined training dataset for P...\n",
      "  P: 9312 real graphs loaded, 350 failed\n",
      "  P: 871 real graphs loaded, 32 failed\n",
      "  P: 9287 synthetic graphs loaded, 25 failed\n",
      "  P: 871 synthetic graphs loaded, 0 failed\n",
      "  Train: 9312 real + 9287 synthetic = 18599\n",
      "  Val: 871 real + 871 synthetic = 1742\n",
      "  P: 249 real graphs loaded, 8 failed\n",
      "  P: 3232 real graphs loaded, 161 failed\n",
      "  Input dimension: 17\n",
      "Training Optimized Edge-Type Aware Capsule Network with Curvature Regularization...\n",
      "Model parameters: 23,937\n",
      "    Epoch 0: Train Loss=3.5326, Val Loss=2.9274 (MSE=2.9260, Curve=0.0132)\n",
      "    Epoch 5: Train Loss=2.8998, Val Loss=2.9012 (MSE=2.8981, Curve=0.0310)\n",
      "    Epoch 10: Train Loss=2.7428, Val Loss=2.9776 (MSE=2.9742, Curve=0.0342)\n",
      "    Epoch 15: Train Loss=2.7535, Val Loss=3.0088 (MSE=3.0042, Curve=0.0457)\n",
      "    Early stopping at epoch 19\n",
      "    Model with curvature regularization saved to optimized_edge_aware_capsule_curvature_P_model.pth\n",
      "Testing on 2016 Core Set...\n",
      "  Core Set - Rp: 0.563, RMSE: 1.788\n",
      "  Core Routing - Intra-P: 0.949, Intra-L: 0.026, Inter: 0.024\n",
      "Testing on 2019 Holdout Set...\n",
      "  Holdout Set - Rp: 0.487, RMSE: 1.549\n",
      "\n",
      "==================== OPTIMIZED L WITH CURVATURE ====================\n",
      "Preparing FULL combined training dataset for L...\n",
      "  L: 9312 real graphs loaded, 350 failed\n",
      "  L: 871 real graphs loaded, 32 failed\n",
      "  L: 9312 synthetic graphs loaded, 0 failed\n",
      "  L: 871 synthetic graphs loaded, 0 failed\n",
      "  Train: 9312 real + 9312 synthetic = 18624\n",
      "  Val: 871 real + 871 synthetic = 1742\n",
      "  L: 249 real graphs loaded, 8 failed\n",
      "  L: 3232 real graphs loaded, 161 failed\n",
      "  Input dimension: 17\n",
      "Training Optimized Edge-Type Aware Capsule Network with Curvature Regularization...\n",
      "Model parameters: 23,937\n",
      "    Epoch 0: Train Loss=3.5457, Val Loss=2.7036 (MSE=2.6918, Curve=0.1177)\n",
      "    Epoch 5: Train Loss=2.5892, Val Loss=2.5497 (MSE=2.5431, Curve=0.0656)\n",
      "    Epoch 10: Train Loss=2.3531, Val Loss=2.6038 (MSE=2.5994, Curve=0.0439)\n",
      "    Early stopping at epoch 12\n",
      "    Model with curvature regularization saved to optimized_edge_aware_capsule_curvature_L_model.pth\n",
      "Testing on 2016 Core Set...\n",
      "  Core Set - Rp: 0.582, RMSE: 1.670\n",
      "  Core Routing - Intra-P: 0.010, Intra-L: 0.966, Inter: 0.024\n",
      "Testing on 2019 Holdout Set...\n",
      "  Holdout Set - Rp: 0.543, RMSE: 1.472\n",
      "\n",
      "==================== OPTIMIZED I WITH CURVATURE ====================\n",
      "Preparing FULL combined training dataset for I...\n",
      "  I: 9312 real graphs loaded, 350 failed\n",
      "  I: 871 real graphs loaded, 32 failed\n",
      "  I: 9312 synthetic graphs loaded, 0 failed\n",
      "  I: 871 synthetic graphs loaded, 0 failed\n",
      "  Train: 9312 real + 9312 synthetic = 18624\n",
      "  Val: 871 real + 871 synthetic = 1742\n",
      "  I: 249 real graphs loaded, 8 failed\n",
      "  I: 3232 real graphs loaded, 161 failed\n",
      "  Input dimension: 17\n",
      "Training Optimized Edge-Type Aware Capsule Network with Curvature Regularization...\n",
      "Model parameters: 23,937\n",
      "    Epoch 0: Train Loss=3.5491, Val Loss=3.8353 (MSE=3.8271, Curve=0.0828)\n",
      "    Epoch 5: Train Loss=2.5820, Val Loss=2.6589 (MSE=2.6510, Curve=0.0794)\n",
      "    Epoch 10: Train Loss=2.3842, Val Loss=2.5698 (MSE=2.5650, Curve=0.0473)\n",
      "    Epoch 15: Train Loss=2.4042, Val Loss=2.7780 (MSE=2.7728, Curve=0.0524)\n",
      "    Epoch 20: Train Loss=2.3043, Val Loss=2.4409 (MSE=2.4358, Curve=0.0516)\n",
      "    Epoch 25: Train Loss=2.1473, Val Loss=2.5622 (MSE=2.5576, Curve=0.0460)\n",
      "    Early stopping at epoch 30\n",
      "    Model with curvature regularization saved to optimized_edge_aware_capsule_curvature_I_model.pth\n",
      "Testing on 2016 Core Set...\n",
      "  Core Set - Rp: 0.617, RMSE: 1.644\n",
      "  Core Routing - Intra-P: 0.015, Intra-L: 0.024, Inter: 0.961\n",
      "Testing on 2019 Holdout Set...\n",
      "  Holdout Set - Rp: 0.551, RMSE: 1.452\n",
      "\n",
      "==================== OPTIMIZED PL WITH CURVATURE ====================\n",
      "Preparing FULL combined training dataset for PL...\n",
      "  PL: 9312 real graphs loaded, 350 failed\n",
      "  PL: 871 real graphs loaded, 32 failed\n",
      "  PL: 9312 synthetic graphs loaded, 0 failed\n",
      "  PL: 871 synthetic graphs loaded, 0 failed\n",
      "  Train: 9312 real + 9312 synthetic = 18624\n",
      "  Val: 871 real + 871 synthetic = 1742\n",
      "  PL: 249 real graphs loaded, 8 failed\n",
      "  PL: 3232 real graphs loaded, 161 failed\n",
      "  Input dimension: 17\n",
      "Training Optimized Edge-Type Aware Capsule Network with Curvature Regularization...\n",
      "Model parameters: 23,937\n",
      "    Epoch 0: Train Loss=3.4988, Val Loss=4.2337 (MSE=4.2204, Curve=0.1323)\n",
      "    Epoch 5: Train Loss=2.5581, Val Loss=2.5750 (MSE=2.5652, Curve=0.0988)\n",
      "    Epoch 10: Train Loss=2.3049, Val Loss=2.4167 (MSE=2.4099, Curve=0.0686)\n",
      "    Epoch 15: Train Loss=2.3410, Val Loss=2.5893 (MSE=2.5813, Curve=0.0797)\n",
      "    Early stopping at epoch 20\n",
      "    Model with curvature regularization saved to optimized_edge_aware_capsule_curvature_PL_model.pth\n",
      "Testing on 2016 Core Set...\n",
      "  Core Set - Rp: 0.625, RMSE: 1.678\n",
      "  Core Routing - Intra-P: 0.489, Intra-L: 0.497, Inter: 0.014\n",
      "Testing on 2019 Holdout Set...\n",
      "  Holdout Set - Rp: 0.596, RMSE: 1.418\n",
      "\n",
      "==================== OPTIMIZED PI WITH CURVATURE ====================\n",
      "Preparing FULL combined training dataset for PI...\n",
      "  PI: 9312 real graphs loaded, 350 failed\n",
      "  PI: 871 real graphs loaded, 32 failed\n",
      "  PI: 9312 synthetic graphs loaded, 0 failed\n",
      "  PI: 871 synthetic graphs loaded, 0 failed\n",
      "  Train: 9312 real + 9312 synthetic = 18624\n",
      "  Val: 871 real + 871 synthetic = 1742\n",
      "  PI: 249 real graphs loaded, 8 failed\n",
      "  PI: 3232 real graphs loaded, 161 failed\n",
      "  Input dimension: 17\n",
      "Training Optimized Edge-Type Aware Capsule Network with Curvature Regularization...\n",
      "Model parameters: 23,937\n",
      "    Epoch 0: Train Loss=3.2962, Val Loss=2.8202 (MSE=2.8091, Curve=0.1115)\n",
      "    Epoch 5: Train Loss=2.5081, Val Loss=2.4923 (MSE=2.4839, Curve=0.0844)\n",
      "    Epoch 10: Train Loss=2.3269, Val Loss=2.3119 (MSE=2.3057, Curve=0.0627)\n",
      "    Epoch 15: Train Loss=2.3734, Val Loss=2.5982 (MSE=2.5911, Curve=0.0709)\n",
      "    Epoch 20: Train Loss=2.2433, Val Loss=2.3233 (MSE=2.3148, Curve=0.0855)\n",
      "    Early stopping at epoch 21\n",
      "    Model with curvature regularization saved to optimized_edge_aware_capsule_curvature_PI_model.pth\n",
      "Testing on 2016 Core Set...\n",
      "  Core Set - Rp: 0.627, RMSE: 1.605\n",
      "  Core Routing - Intra-P: 0.016, Intra-L: 0.015, Inter: 0.969\n",
      "Testing on 2019 Holdout Set...\n",
      "  Holdout Set - Rp: 0.577, RMSE: 1.437\n",
      "\n",
      "==================== OPTIMIZED LI WITH CURVATURE ====================\n",
      "Preparing FULL combined training dataset for LI...\n",
      "  LI: 9312 real graphs loaded, 350 failed\n",
      "  LI: 871 real graphs loaded, 32 failed\n",
      "  LI: 9312 synthetic graphs loaded, 0 failed\n",
      "  LI: 871 synthetic graphs loaded, 0 failed\n",
      "  Train: 9312 real + 9312 synthetic = 18624\n",
      "  Val: 871 real + 871 synthetic = 1742\n",
      "  LI: 249 real graphs loaded, 8 failed\n",
      "  LI: 3232 real graphs loaded, 161 failed\n",
      "  Input dimension: 17\n",
      "Training Optimized Edge-Type Aware Capsule Network with Curvature Regularization...\n",
      "Model parameters: 23,937\n",
      "    Epoch 0: Train Loss=3.3593, Val Loss=2.5211 (MSE=2.5123, Curve=0.0875)\n",
      "    Epoch 5: Train Loss=2.4373, Val Loss=2.4278 (MSE=2.4192, Curve=0.0859)\n",
      "    Epoch 10: Train Loss=2.2228, Val Loss=2.2844 (MSE=2.2796, Curve=0.0478)\n",
      "    Epoch 15: Train Loss=2.2566, Val Loss=2.2919 (MSE=2.2830, Curve=0.0890)\n",
      "    Early stopping at epoch 20\n",
      "    Model with curvature regularization saved to optimized_edge_aware_capsule_curvature_LI_model.pth\n",
      "Testing on 2016 Core Set...\n",
      "  Core Set - Rp: 0.656, RMSE: 1.593\n",
      "  Core Routing - Intra-P: 0.012, Intra-L: 0.020, Inter: 0.968\n",
      "Testing on 2019 Holdout Set...\n",
      "  Holdout Set - Rp: 0.583, RMSE: 1.416\n",
      "\n",
      "==================== OPTIMIZED PLI WITH CURVATURE ====================\n",
      "Preparing FULL combined training dataset for PLI...\n",
      "  PLI: 9312 real graphs loaded, 350 failed\n",
      "  PLI: 871 real graphs loaded, 32 failed\n",
      "  PLI: 9312 synthetic graphs loaded, 0 failed\n",
      "  PLI: 871 synthetic graphs loaded, 0 failed\n",
      "  Train: 9312 real + 9312 synthetic = 18624\n",
      "  Val: 871 real + 871 synthetic = 1742\n",
      "  PLI: 249 real graphs loaded, 8 failed\n",
      "  PLI: 3232 real graphs loaded, 161 failed\n",
      "  Input dimension: 17\n",
      "Training Optimized Edge-Type Aware Capsule Network with Curvature Regularization...\n",
      "Model parameters: 23,937\n",
      "    Epoch 0: Train Loss=3.2947, Val Loss=2.8302 (MSE=2.8188, Curve=0.1138)\n",
      "    Epoch 5: Train Loss=2.4403, Val Loss=2.4192 (MSE=2.4121, Curve=0.0708)\n",
      "    Epoch 10: Train Loss=2.2207, Val Loss=2.2025 (MSE=2.1969, Curve=0.0554)\n",
      "    Epoch 15: Train Loss=2.2903, Val Loss=2.4776 (MSE=2.4705, Curve=0.0711)\n",
      "    Epoch 20: Train Loss=2.1664, Val Loss=2.2446 (MSE=2.2375, Curve=0.0715)\n",
      "    Early stopping at epoch 22\n",
      "    Model with curvature regularization saved to optimized_edge_aware_capsule_curvature_PLI_model.pth\n",
      "Testing on 2016 Core Set...\n",
      "  Core Set - Rp: 0.661, RMSE: 1.607\n",
      "  Core Routing - Intra-P: 0.015, Intra-L: 0.016, Inter: 0.969\n",
      "Testing on 2019 Holdout Set...\n",
      "  Holdout Set - Rp: 0.592, RMSE: 1.414\n",
      "\n",
      "üíæ Saving results with curvature regularization...\n",
      "‚úÖ Results saved to 'optimized_edge_aware_capsule_curvature_results.csv'\n",
      "‚úÖ Models saved with naming: 'optimized_edge_aware_capsule_curvature_{combination}_model.pth'\n",
      "\n",
      "üéâ OPTIMIZED EXECUTION WITH CURVATURE REGULARIZATION COMPLETED!\n",
      "üìä 7 models tested with curvature regularization\n",
      "üîç Curvature parameters: Œ≥=0.1, samples=100, similarity=normalized_adjacency\n",
      "\n",
      "üìñ LOADING CURVATURE-REGULARIZED MODEL:\n",
      "```python\n",
      "model, checkpoint = load_optimized_model('optimized_edge_aware_capsule_curvature_PLI_model.pth')\n",
      "# Access curvature parameters:\n",
      "gamma = checkpoint['gamma']\n",
      "similarity_type = checkpoint['similarity_type']\n",
      "loss_history = checkpoint['loss_history']\n",
      "```\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import json\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch_geometric.data import Data\n",
    "from torch_geometric.loader import DataLoader\n",
    "from torch_geometric.nn import GCNConv\n",
    "from torch_geometric.utils import add_self_loops, to_undirected\n",
    "import os\n",
    "import numpy as np\n",
    "from scipy.stats import pearsonr\n",
    "import pickle\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Atom property dictionary\n",
    "atom_property_dict = {\n",
    "    'H': {'atomic_num': 1, 'mass': 1.008, 'electronegativity': 2.20, 'vdw_radius': 1.20},\n",
    "    'C': {'atomic_num': 6, 'mass': 12.011, 'electronegativity': 2.55, 'vdw_radius': 1.70},\n",
    "    'N': {'atomic_num': 7, 'mass': 14.007, 'electronegativity': 3.04, 'vdw_radius': 1.55},\n",
    "    'O': {'atomic_num': 8, 'mass': 15.999, 'electronegativity': 3.44, 'vdw_radius': 1.52},\n",
    "    'P': {'atomic_num': 15, 'mass': 30.974, 'electronegativity': 2.19, 'vdw_radius': 1.80},\n",
    "    'S': {'atomic_num': 16, 'mass': 32.065, 'electronegativity': 2.58, 'vdw_radius': 1.80},\n",
    "    'F': {'atomic_num': 9, 'mass': 18.998, 'electronegativity': 3.98, 'vdw_radius': 1.47},\n",
    "    'Cl': {'atomic_num': 17, 'mass': 35.453, 'electronegativity': 3.16, 'vdw_radius': 1.75},\n",
    "    'Br': {'atomic_num': 35, 'mass': 79.904, 'electronegativity': 2.96, 'vdw_radius': 1.85},\n",
    "    'I': {'atomic_num': 53, 'mass': 126.904, 'electronegativity': 2.66, 'vdw_radius': 1.98},\n",
    "    'CA': {'atomic_num': 6, 'mass': 12.011, 'electronegativity': 2.55, 'vdw_radius': 1.70},\n",
    "    'CZ': {'atomic_num': 6, 'mass': 12.011, 'electronegativity': 2.55, 'vdw_radius': 1.70},\n",
    "    'OG': {'atomic_num': 8, 'mass': 15.999, 'electronegativity': 3.44, 'vdw_radius': 1.52},\n",
    "    'ZN': {'atomic_num': 30, 'mass': 65.38, 'electronegativity': 1.65, 'vdw_radius': 1.39},\n",
    "    'MG': {'atomic_num': 12, 'mass': 24.305, 'electronegativity': 1.31, 'vdw_radius': 1.73},\n",
    "    'FE': {'atomic_num': 26, 'mass': 55.845, 'electronegativity': 1.83, 'vdw_radius': 1.72},\n",
    "    'MN': {'atomic_num': 25, 'mass': 54.938, 'electronegativity': 1.55, 'vdw_radius': 1.73},\n",
    "    'CU': {'atomic_num': 29, 'mass': 63.546, 'electronegativity': 1.90, 'vdw_radius': 1.40},\n",
    "}\n",
    "\n",
    "def load_csv(csv_path, max_samples=None, use_half=False):\n",
    "    df = pd.read_csv(csv_path)\n",
    "    df = df[df['Affinity_pK'] != 0]\n",
    "    \n",
    "    if len(df) == 0:\n",
    "        print(f\"Warning: No valid data found in {csv_path}\")\n",
    "        return pd.DataFrame()\n",
    "    \n",
    "    if use_half:\n",
    "        half_size = len(df) // 2\n",
    "        if half_size == 0:\n",
    "            half_size = 1\n",
    "        df = df.head(half_size)\n",
    "        print(f\"Using half dataset: {half_size} samples from {csv_path}\")\n",
    "    elif max_samples:\n",
    "        df = df.head(max_samples)\n",
    "    \n",
    "    return df \n",
    "\n",
    "# OPTIMIZED: Simplified normalization\n",
    "def fast_normalize(features):\n",
    "    if features.size(0) <= 1:\n",
    "        return torch.zeros_like(features)\n",
    "    \n",
    "    mean = features.mean(dim=0, keepdim=True)\n",
    "    std = features.std(dim=0, keepdim=True, unbiased=False)\n",
    "    std = torch.clamp(std, min=1e-6)\n",
    "    \n",
    "    normalized = (features - mean) / std\n",
    "    return torch.clamp(normalized, min=-3, max=3)\n",
    "\n",
    "def create_enhanced_features(node, atom_property_dict, graph_type='P'):\n",
    "    atom_type = node['attype']\n",
    "    prop = atom_property_dict.get(atom_type, \n",
    "                                 {'atomic_num': 6, 'mass': 12.011, 'electronegativity': 2.55, 'vdw_radius': 1.70})\n",
    "    \n",
    "    if 'pl' in node:\n",
    "        is_protein = node['pl'] == 'P'\n",
    "        is_ligand = node['pl'] == 'L'\n",
    "        is_interaction = graph_type == 'I'\n",
    "    else:\n",
    "        is_protein = graph_type == 'P'\n",
    "        is_ligand = graph_type == 'L'\n",
    "        is_interaction = graph_type == 'I'\n",
    "    \n",
    "    features = [\n",
    "        prop['atomic_num'] / 30.0, prop['mass'] / 100.0, prop['electronegativity'] / 4.0, prop['vdw_radius'] / 2.0,\n",
    "        prop['atomic_num'] ** 0.5 / 5.5, prop['mass'] / prop['atomic_num'], 1.0 / prop['electronegativity'], prop['vdw_radius'] ** 2,\n",
    "        1.0 if prop['atomic_num'] in [6] else 0.0, 1.0 if prop['atomic_num'] in [7] else 0.0,\n",
    "        1.0 if prop['atomic_num'] in [8] else 0.0, 1.0 if prop['atomic_num'] in [16] else 0.0,\n",
    "        1.0 if prop['atomic_num'] > 10 else 0.0, 1.0 if prop['electronegativity'] > 3.0 else 0.0,\n",
    "        1.0 if is_protein else 0.0, 1.0 if is_ligand else 0.0, 1.0 if is_interaction else 0.0,\n",
    "    ]\n",
    "    return features\n",
    "\n",
    "def load_single_graph(pdb_id, base_path, graph_type):\n",
    "    if graph_type == 'P':\n",
    "        json_path = os.path.join(base_path, pdb_id, f'{pdb_id}_protein_graph.json')\n",
    "    elif graph_type == 'L':\n",
    "        json_path = os.path.join(base_path, pdb_id, f'{pdb_id}_ligand_graph.json')\n",
    "    elif graph_type == 'I':\n",
    "        json_path = os.path.join(base_path, pdb_id, f'{pdb_id}_interaction_graph.json')\n",
    "    else:\n",
    "        return None\n",
    "    \n",
    "    try:\n",
    "        with open(json_path, 'r') as file:\n",
    "            graph = json.load(file)\n",
    "    except FileNotFoundError:\n",
    "        return None\n",
    "\n",
    "    if not graph['nodes']:\n",
    "        return None\n",
    "\n",
    "    node_features = []\n",
    "    node_types = []\n",
    "    \n",
    "    for node in graph['nodes']:\n",
    "        features = create_enhanced_features(node, atom_property_dict, graph_type)\n",
    "        node_features.append(features)\n",
    "        \n",
    "        if 'pl' in node:\n",
    "            node_types.append(node['pl'])\n",
    "        else:\n",
    "            node_types.append(graph_type)\n",
    "\n",
    "    node_features = torch.tensor(node_features, dtype=torch.float)\n",
    "    \n",
    "    if torch.isnan(node_features).any() or torch.isinf(node_features).any():\n",
    "        return None\n",
    "    \n",
    "    edge_index = []\n",
    "    edge_features = []\n",
    "    edge_types = []\n",
    "    \n",
    "    for edge in graph['edges']:\n",
    "        if edge['id1'] is None or edge['id2'] is None:\n",
    "            continue\n",
    "            \n",
    "        length = max(edge['length'], 0.1)\n",
    "        edge_index.append([edge['id1'], edge['id2']])\n",
    "        edge_features.append([length / 10.0, 1.0 / length, np.exp(-length/2.0)])\n",
    "        \n",
    "        node1_type = node_types[edge['id1']] if edge['id1'] < len(node_types) else graph_type\n",
    "        node2_type = node_types[edge['id2']] if edge['id2'] < len(node_types) else graph_type\n",
    "        \n",
    "        if node1_type == 'P' and node2_type == 'P':\n",
    "            edge_types.append(0)\n",
    "        elif node1_type == 'L' and node2_type == 'L':\n",
    "            edge_types.append(1)\n",
    "        else:\n",
    "            edge_types.append(2)\n",
    "\n",
    "    if not edge_index:\n",
    "        num_nodes = len(node_features)\n",
    "        edge_index = torch.arange(num_nodes).unsqueeze(0).repeat(2, 1)\n",
    "        edge_features = torch.ones(num_nodes, 3) * 0.5\n",
    "        edge_types = [0] * num_nodes\n",
    "    else:\n",
    "        edge_index = torch.tensor(edge_index, dtype=torch.long).t().contiguous()\n",
    "        edge_features = torch.tensor(edge_features, dtype=torch.float)\n",
    "        \n",
    "        edge_index = to_undirected(edge_index)\n",
    "        if edge_features.size(0) * 2 == edge_index.size(1):\n",
    "            edge_features = edge_features.repeat(2, 1)\n",
    "            edge_types = edge_types + edge_types\n",
    "\n",
    "    return {\n",
    "        'node_features': node_features,\n",
    "        'edge_index': edge_index,\n",
    "        'edge_features': edge_features,\n",
    "        'edge_types': torch.tensor(edge_types, dtype=torch.long),\n",
    "        'num_nodes': len(node_features),\n",
    "        'graph_type': graph_type,\n",
    "        'node_types': node_types\n",
    "    }\n",
    "\n",
    "def merge_graphs(graphs):\n",
    "    all_node_features = []\n",
    "    all_edge_indices = []\n",
    "    all_edge_features = []\n",
    "    all_edge_types = []\n",
    "    graph_type_markers = []\n",
    "    \n",
    "    node_offset = 0\n",
    "    \n",
    "    for graph in graphs:\n",
    "        if graph is None:\n",
    "            continue\n",
    "            \n",
    "        all_node_features.append(graph['node_features'])\n",
    "        adjusted_edge_index = graph['edge_index'] + node_offset\n",
    "        all_edge_indices.append(adjusted_edge_index)\n",
    "        all_edge_features.append(graph['edge_features'])\n",
    "        all_edge_types.append(graph['edge_types'])\n",
    "        \n",
    "        graph_type_markers.extend([graph['graph_type']] * graph['num_nodes'])\n",
    "        node_offset += graph['num_nodes']\n",
    "    \n",
    "    if not all_node_features:\n",
    "        return None\n",
    "    \n",
    "    merged_node_features = torch.cat(all_node_features, dim=0)\n",
    "    merged_edge_index = torch.cat(all_edge_indices, dim=1) if all_edge_indices else torch.empty((2, 0), dtype=torch.long)\n",
    "    merged_edge_features = torch.cat(all_edge_features, dim=0) if all_edge_features else torch.empty((0, 3))\n",
    "    merged_edge_types = torch.cat(all_edge_types, dim=0) if all_edge_types else torch.empty((0,), dtype=torch.long)\n",
    "    \n",
    "    return merged_node_features, merged_edge_index, merged_edge_features, merged_edge_types, graph_type_markers\n",
    "\n",
    "# OPTIMIZED: Precompute merged graphs during data loading\n",
    "def precompute_combined_graph(pdb_id, base_path, combination):\n",
    "    graphs_to_load = []\n",
    "    \n",
    "    if 'P' in combination:\n",
    "        graphs_to_load.append('P')\n",
    "    if 'L' in combination:\n",
    "        graphs_to_load.append('L')\n",
    "    if 'I' in combination:\n",
    "        graphs_to_load.append('I')\n",
    "    \n",
    "    loaded_graphs = []\n",
    "    for graph_type in graphs_to_load:\n",
    "        graph = load_single_graph(pdb_id, base_path, graph_type)\n",
    "        loaded_graphs.append(graph)\n",
    "    \n",
    "    merged_result = merge_graphs(loaded_graphs)\n",
    "    if merged_result is None:\n",
    "        return None\n",
    "    \n",
    "    node_features, edge_index, edge_features, edge_types, graph_type_markers = merged_result\n",
    "    node_features = fast_normalize(node_features)\n",
    "    edge_index, edge_attr = add_self_loops(edge_index, edge_features, num_nodes=node_features.size(0))\n",
    "    \n",
    "    # num_self_loops = node_features.size(0)\n",
    "    # self_loop_types = torch.zeros(num_self_loops, dtype=torch.long)\n",
    "    # edge_types = torch.cat([edge_types, self_loop_types], dim=0)\n",
    "\n",
    "    num_self_loops = node_features.size(0)\n",
    "    if combination == 'P':\n",
    "        self_loop_types = torch.zeros(num_self_loops, dtype=torch.long)  # Type 0 (intra-protein)\n",
    "    elif combination == 'L': \n",
    "        self_loop_types = torch.ones(num_self_loops, dtype=torch.long)   # Type 1 (intra-ligand)\n",
    "    else:\n",
    "        # For mixed combinations, assign based on actual node types\n",
    "        self_loop_types = []\n",
    "        for node_type in graph_type_markers:\n",
    "            if node_type == 'P':\n",
    "                self_loop_types.append(0)  # Intra-protein\n",
    "            elif node_type == 'L':\n",
    "                self_loop_types.append(1)  # Intra-ligand\n",
    "            else:\n",
    "                self_loop_types.append(2)  # Default to inter\n",
    "        self_loop_types = torch.tensor(self_loop_types, dtype=torch.long)\n",
    "    edge_types = torch.cat([edge_types, self_loop_types], dim=0)\n",
    "    \n",
    "    \n",
    "    if edge_attr.size(0) > 0:\n",
    "        edge_attr = fast_normalize(edge_attr)\n",
    "    \n",
    "    # OPTIMIZATION: Cache edge type statistics\n",
    "    edge_type_counts = torch.bincount(edge_types, minlength=3)\n",
    "    \n",
    "    data = Data(x=node_features, edge_index=edge_index, edge_attr=edge_attr)\n",
    "    data.edge_types = edge_types\n",
    "    data.edge_type_counts = edge_type_counts  # Cached for routing\n",
    "    data.graph_type_markers = graph_type_markers\n",
    "    return data\n",
    "\n",
    "def prepare_real_dataset_combined(df, base_path, combination):\n",
    "    data_list = []\n",
    "    failed_count = 0\n",
    "    \n",
    "    if df is None or len(df) == 0:\n",
    "        print(f\"  {combination}: No real data available\")\n",
    "        return []\n",
    "    \n",
    "    for _, row in df.iterrows():\n",
    "        pdb_id, affinity = row['PDB_ID'], row['Affinity_pK']\n",
    "        \n",
    "        if np.isnan(affinity) or np.isinf(affinity):\n",
    "            failed_count += 1\n",
    "            continue\n",
    "            \n",
    "        data = precompute_combined_graph(pdb_id, base_path, combination)\n",
    "        if data is not None:\n",
    "            data.y = torch.tensor([affinity], dtype=torch.float)\n",
    "            data.is_synthetic = False\n",
    "            data_list.append(data)\n",
    "        else:\n",
    "            failed_count += 1\n",
    "    \n",
    "    print(f\"  {combination}: {len(data_list)} real graphs loaded, {failed_count} failed\")\n",
    "    return data_list\n",
    "\n",
    "def prepare_synthetic_dataset(synthetic_dir, combination):\n",
    "    data_list = []\n",
    "    failed_count = 0\n",
    "    \n",
    "    if not os.path.exists(synthetic_dir):\n",
    "        print(f\"Synthetic directory not found: {synthetic_dir}\")\n",
    "        return []\n",
    "    \n",
    "    pdb_dirs = [d for d in os.listdir(synthetic_dir) if os.path.isdir(os.path.join(synthetic_dir, d))]\n",
    "    \n",
    "    for pdb_dir in pdb_dirs:\n",
    "        graph_file = os.path.join(synthetic_dir, pdb_dir, f'{pdb_dir}_{combination}.pkl')\n",
    "        affinity_file = os.path.join(synthetic_dir, pdb_dir, f'{pdb_dir}_affinity.pkl')\n",
    "        \n",
    "        if not os.path.exists(graph_file) or not os.path.exists(affinity_file):\n",
    "            failed_count += 1\n",
    "            continue\n",
    "        \n",
    "        try:\n",
    "            with open(graph_file, 'rb') as f:\n",
    "                graph_data = pickle.load(f)\n",
    "            with open(affinity_file, 'rb') as f:\n",
    "                affinity_data = pickle.load(f)\n",
    "            \n",
    "            affinity = affinity_data.get('affinity', None)\n",
    "            if affinity is None or np.isnan(affinity) or np.isinf(affinity):\n",
    "                failed_count += 1\n",
    "                continue\n",
    "            \n",
    "            # Check if node_types exists\n",
    "            if 'node_types' not in graph_data:\n",
    "                print(f\"‚ùå Missing node_types in {graph_file}\")\n",
    "                failed_count += 1\n",
    "                continue\n",
    "                \n",
    "            node_types = graph_data['node_types']\n",
    "            if not node_types:\n",
    "                print(f\"‚ùå Empty node_types in {graph_file}\")\n",
    "                failed_count += 1\n",
    "                continue\n",
    "            \n",
    "            node_features = torch.tensor(graph_data['node_features'], dtype=torch.float)\n",
    "            edge_index = torch.tensor(graph_data['edge_index'], dtype=torch.long)\n",
    "            edge_attr = torch.tensor(graph_data['edge_features'], dtype=torch.float)\n",
    "            \n",
    "            # Validate node_types length matches node_features\n",
    "            if len(node_types) != node_features.size(0):\n",
    "                print(f\"‚ùå Node types mismatch in {graph_file}: {len(node_types)} types, {node_features.size(0)} nodes\")\n",
    "                failed_count += 1\n",
    "                continue\n",
    "            \n",
    "            if edge_index.size(0) != 2:\n",
    "                edge_index = edge_index.t()\n",
    "            \n",
    "            # Create edge types based on actual node types\n",
    "            edge_types = []\n",
    "            for i in range(edge_index.size(1)):\n",
    "                src_type = node_types[edge_index[0, i]]\n",
    "                dst_type = node_types[edge_index[1, i]]\n",
    "                \n",
    "                if src_type == 'P' and dst_type == 'P':\n",
    "                    edge_types.append(0)  # Intra-protein\n",
    "                elif src_type == 'L' and dst_type == 'L':\n",
    "                    edge_types.append(1)  # Intra-ligand\n",
    "                else:\n",
    "                    edge_types.append(2)  # Inter-protein-ligand\n",
    "            \n",
    "            edge_types = torch.tensor(edge_types, dtype=torch.long)\n",
    "            \n",
    "            node_features = fast_normalize(node_features)\n",
    "            edge_index, edge_attr = add_self_loops(edge_index, edge_attr, num_nodes=node_features.size(0))\n",
    "            \n",
    "            # Create self-loop types based on actual node types\n",
    "            self_loop_types = []\n",
    "            for node_type in node_types:\n",
    "                if node_type == 'P':\n",
    "                    self_loop_types.append(0)  # Intra-protein\n",
    "                elif node_type == 'L':\n",
    "                    self_loop_types.append(1)  # Intra-ligand\n",
    "                else:\n",
    "                    self_loop_types.append(2)  # Default\n",
    "            \n",
    "            self_loop_types = torch.tensor(self_loop_types, dtype=torch.long)\n",
    "            edge_types = torch.cat([edge_types, self_loop_types], dim=0)\n",
    "            \n",
    "            if edge_attr.size(0) > 0:\n",
    "                edge_attr = fast_normalize(edge_attr)\n",
    "            \n",
    "            edge_type_counts = torch.bincount(edge_types, minlength=3)\n",
    "            \n",
    "            data = Data(x=node_features, edge_index=edge_index, edge_attr=edge_attr)\n",
    "            data.edge_types = edge_types\n",
    "            data.edge_type_counts = edge_type_counts\n",
    "            data.graph_type_markers = node_types\n",
    "            data.y = torch.tensor([affinity], dtype=torch.float)\n",
    "            data.is_synthetic = True\n",
    "            data_list.append(data)\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"‚ùå Error loading {graph_file}: {e}\")\n",
    "            failed_count += 1\n",
    "            continue\n",
    "    \n",
    "    print(f\"  {combination}: {len(data_list)} synthetic graphs loaded, {failed_count} failed\")\n",
    "    return data_list\n",
    "\n",
    "def prepare_combined_training_dataset(real_train_csv, real_val_csv, real_data_path, \n",
    "                                    synthetic_train_dir, synthetic_val_dir, combination, use_half=False):\n",
    "    print(f\"Preparing {'HALF' if use_half else 'FULL'} combined training dataset for {combination}...\")\n",
    "    \n",
    "    real_train_df = load_csv(real_train_csv, use_half=use_half)\n",
    "    real_val_df = load_csv(real_val_csv, use_half=use_half)\n",
    "    \n",
    "    real_train_data = prepare_real_dataset_combined(real_train_df, real_data_path, combination)\n",
    "    real_val_data = prepare_real_dataset_combined(real_val_df, real_data_path, combination)\n",
    "\n",
    "    # Load synthetic data (halved)\n",
    "    synthetic_train_data = prepare_synthetic_dataset(synthetic_train_dir, combination)\n",
    "    synthetic_val_data = prepare_synthetic_dataset(synthetic_val_dir, combination)\n",
    "    \n",
    "    # Combine\n",
    "    combined_train_data = real_train_data + synthetic_train_data\n",
    "    combined_val_data = real_val_data + synthetic_val_data\n",
    "    \n",
    "    print(f\"  Train: {len(real_train_data)} real + {len(synthetic_train_data)} synthetic = {len(combined_train_data)}\")\n",
    "    print(f\"  Val: {len(real_val_data)} real + {len(synthetic_val_data)} synthetic = {len(combined_val_data)}\")\n",
    "    \n",
    "    \n",
    "    return combined_train_data, combined_val_data   \n",
    "\n",
    "\n",
    "def precompute_important_pairs(edge_index, num_nodes, num_samples=100):\n",
    "    \"\"\"Precompute important node pairs based on graph structure\"\"\"\n",
    "    if num_nodes < 2:\n",
    "        return torch.empty((0, 2), dtype=torch.long)\n",
    "    \n",
    "    # Compute node degrees\n",
    "    degrees = torch.zeros(num_nodes)\n",
    "    if edge_index.size(1) > 0:\n",
    "        degrees.scatter_add_(0, edge_index[0], torch.ones(edge_index.size(1)))\n",
    "        degrees.scatter_add_(0, edge_index[1], torch.ones(edge_index.size(1)))\n",
    "    \n",
    "    # Focus on high-degree nodes (hubs) and their neighbors\n",
    "    if degrees.sum() > 0:\n",
    "        # Get top 20% highest degree nodes\n",
    "        num_hubs = max(1, num_nodes // 5)\n",
    "        hub_indices = degrees.argsort(descending=True)[:num_hubs]\n",
    "        \n",
    "        pairs = []\n",
    "        # Connect hubs to their neighbors\n",
    "        for hub in hub_indices:\n",
    "            neighbors = edge_index[1, edge_index[0] == hub]\n",
    "            if len(neighbors) > 0:\n",
    "                # Sample up to 5 neighbors per hub\n",
    "                sample_size = min(5, len(neighbors))\n",
    "                sampled = neighbors[torch.randperm(len(neighbors))[:sample_size]]\n",
    "                for neighbor in sampled:\n",
    "                    pairs.append([hub.item(), neighbor.item()])\n",
    "        \n",
    "        # Add some inter-hub connections\n",
    "        for i in range(len(hub_indices)):\n",
    "            for j in range(i+1, min(i+3, len(hub_indices))):\n",
    "                pairs.append([hub_indices[i].item(), hub_indices[j].item()])\n",
    "        \n",
    "        # Fill remaining with stratified random sampling\n",
    "        remaining = num_samples - len(pairs)\n",
    "        if remaining > 0:\n",
    "            # Sample across different degree ranges\n",
    "            low_degree = (degrees < degrees.median()).nonzero().squeeze()\n",
    "            high_degree = (degrees >= degrees.median()).nonzero().squeeze()\n",
    "            \n",
    "            if len(low_degree) > 0 and len(high_degree) > 0:\n",
    "                for _ in range(remaining // 2):\n",
    "                    i = low_degree[torch.randint(len(low_degree), (1,))].item()\n",
    "                    j = high_degree[torch.randint(len(high_degree), (1,))].item()\n",
    "                    pairs.append([i, j])\n",
    "            \n",
    "            # Random pairs for remaining\n",
    "            for _ in range(remaining - remaining // 2):\n",
    "                i, j = torch.randint(0, num_nodes, (2,)).tolist()\n",
    "                if i != j:\n",
    "                    pairs.append([i, j])\n",
    "        \n",
    "        return torch.tensor(pairs[:num_samples], dtype=torch.long)\n",
    "    else:\n",
    "        # Random sampling for disconnected graphs\n",
    "        pairs = []\n",
    "        for _ in range(num_samples):\n",
    "            i, j = torch.randint(0, num_nodes, (2,)).tolist()\n",
    "            if i != j:\n",
    "                pairs.append([i, j])\n",
    "        return torch.tensor(pairs, dtype=torch.long)\n",
    "\n",
    "def compute_batch_pairwise_similarity(edge_index, sampled_pairs, batch, similarity_type='adjacency'):\n",
    "    \"\"\"\n",
    "    Compute similarity for sampled pairs across entire batch at once\n",
    "    \"\"\"\n",
    "    device = edge_index.device\n",
    "    \n",
    "    if similarity_type == 'adjacency':\n",
    "        # Create adjacency matrix representation for fast lookup\n",
    "        num_nodes = batch.size(0)\n",
    "        adj = torch.zeros((num_nodes, num_nodes), device=device, dtype=torch.bool)\n",
    "        adj[edge_index[0], edge_index[1]] = 1\n",
    "        adj[edge_index[1], edge_index[0]] = 1  # Symmetric\n",
    "        \n",
    "        # Batch similarity computation\n",
    "        similarities = adj[sampled_pairs[:, 0], sampled_pairs[:, 1]].float()\n",
    "        \n",
    "    elif similarity_type == 'normalized_adjacency':\n",
    "        # Compute degrees\n",
    "        degrees = torch.zeros(batch.size(0), device=device)\n",
    "        degrees.scatter_add_(0, edge_index[0], torch.ones(edge_index.size(1), device=device))\n",
    "        degrees.scatter_add_(0, edge_index[1], torch.ones(edge_index.size(1), device=device))\n",
    "        degrees[degrees == 0] = 1.0\n",
    "        \n",
    "        # Adjacency check\n",
    "        num_nodes = batch.size(0)\n",
    "        adj = torch.zeros((num_nodes, num_nodes), device=device, dtype=torch.bool)\n",
    "        adj[edge_index[0], edge_index[1]] = 1\n",
    "        adj[edge_index[1], edge_index[0]] = 1\n",
    "        \n",
    "        # Normalized similarities\n",
    "        is_connected = adj[sampled_pairs[:, 0], sampled_pairs[:, 1]].float()\n",
    "        norm_factor = 1.0 / torch.sqrt(degrees[sampled_pairs[:, 0]] * degrees[sampled_pairs[:, 1]])\n",
    "        similarities = is_connected * norm_factor\n",
    "    \n",
    "    return similarities\n",
    "\n",
    "# NEW OPTIMIZED: Fast batch curvature loss\n",
    "def curvature_regularization_loss_batch_fast(embeddings, edge_index, batch, num_samples_per_graph=50):\n",
    "    \"\"\"\n",
    "    Compute curvature loss for entire batch at once\n",
    "    \"\"\"\n",
    "    device = embeddings.device\n",
    "    batch_size = batch.max().item() + 1\n",
    "    \n",
    "    # Pre-allocate for all samples\n",
    "    all_pairs = []\n",
    "    all_similarities = []\n",
    "    all_distances = []\n",
    "    \n",
    "    # Get node counts per graph\n",
    "    node_counts = torch.bincount(batch)\n",
    "    \n",
    "    # Sample pairs for each graph efficiently\n",
    "    for graph_idx in range(batch_size):\n",
    "        mask = (batch == graph_idx)\n",
    "        graph_nodes = torch.where(mask)[0]\n",
    "        num_graph_nodes = len(graph_nodes)\n",
    "        \n",
    "        if num_graph_nodes < 2:\n",
    "            continue\n",
    "        \n",
    "        # Efficient sampling\n",
    "        n_samples = min(num_samples_per_graph, num_graph_nodes * (num_graph_nodes - 1) // 2)\n",
    "        \n",
    "        # Sample pairs within this graph\n",
    "        if n_samples < num_graph_nodes * 2:\n",
    "            # Random sampling for small graphs\n",
    "            idx1 = torch.randint(0, num_graph_nodes, (n_samples,), device=device)\n",
    "            idx2 = torch.randint(0, num_graph_nodes, (n_samples,), device=device)\n",
    "            # Remove self-loops\n",
    "            valid = idx1 != idx2\n",
    "            idx1, idx2 = idx1[valid], idx2[valid]\n",
    "        else:\n",
    "            # Sample from edges for larger graphs\n",
    "            graph_edge_mask = (batch[edge_index[0]] == graph_idx) & (batch[edge_index[1]] == graph_idx)\n",
    "            graph_edges = edge_index[:, graph_edge_mask]\n",
    "            \n",
    "            if graph_edges.size(1) > 0:\n",
    "                edge_samples = min(n_samples // 2, graph_edges.size(1))\n",
    "                sampled_idx = torch.randperm(graph_edges.size(1), device=device)[:edge_samples]\n",
    "                sampled_edges = graph_edges[:, sampled_idx]\n",
    "                \n",
    "                # Add random pairs\n",
    "                random_samples = n_samples - edge_samples\n",
    "                rand_idx1 = torch.randint(0, num_graph_nodes, (random_samples,), device=device)\n",
    "                rand_idx2 = torch.randint(0, num_graph_nodes, (random_samples,), device=device)\n",
    "                valid = rand_idx1 != rand_idx2\n",
    "                rand_idx1, rand_idx2 = rand_idx1[valid], rand_idx2[valid]\n",
    "                \n",
    "                # Combine\n",
    "                idx1 = torch.cat([sampled_edges[0] - graph_nodes[0], rand_idx1])\n",
    "                idx2 = torch.cat([sampled_edges[1] - graph_nodes[0], rand_idx2])\n",
    "            else:\n",
    "                continue\n",
    "        \n",
    "        # Map to global indices\n",
    "        global_idx1 = graph_nodes[idx1]\n",
    "        global_idx2 = graph_nodes[idx2]\n",
    "        \n",
    "        pairs = torch.stack([global_idx1, global_idx2], dim=1)\n",
    "        all_pairs.append(pairs)\n",
    "    \n",
    "    if not all_pairs:\n",
    "        return torch.tensor(0.0, device=device)\n",
    "    \n",
    "    # Concatenate all pairs\n",
    "    all_pairs = torch.cat(all_pairs, dim=0)\n",
    "    \n",
    "    # Batch compute similarities\n",
    "    similarities = compute_batch_pairwise_similarity(edge_index, all_pairs, batch, 'normalized_adjacency')\n",
    "    \n",
    "    # Batch compute embedding distances\n",
    "    distances = torch.sum((embeddings[all_pairs[:, 0]] - embeddings[all_pairs[:, 1]]) ** 2, dim=1)\n",
    "    \n",
    "    # Curvature loss\n",
    "    loss = torch.mean((distances - similarities) ** 2)\n",
    "    \n",
    "    return loss    \n",
    "\n",
    "# OPTIMIZED: Reduced routing iterations and cached statistics\n",
    "class OptimizedEdgeTypeAwareCapsuleLayer(nn.Module):\n",
    "    def __init__(self, input_dim, capsule_dim=32, num_iterations=2):  # Reduced from 3 to 2\n",
    "        super(OptimizedEdgeTypeAwareCapsuleLayer, self).__init__()\n",
    "        self.input_dim = input_dim\n",
    "        self.capsule_dim = capsule_dim\n",
    "        self.num_iterations = num_iterations\n",
    "        \n",
    "        self.W_intra_protein = nn.Linear(input_dim, capsule_dim, bias=False)\n",
    "        self.W_intra_ligand = nn.Linear(input_dim, capsule_dim, bias=False)\n",
    "        self.W_inter_connection = nn.Linear(input_dim, capsule_dim, bias=False)\n",
    "        \n",
    "        self.routing_coefficients = None\n",
    "        \n",
    "    def squash(self, s):\n",
    "        s_norm = torch.norm(s, dim=-1, keepdim=True)\n",
    "        scale = (s_norm**2 / (1 + s_norm**2))\n",
    "        return scale * s / (s_norm + 1e-8)\n",
    "    \n",
    "    def forward(self, x, edge_index, edge_types, batch, edge_type_counts=None):\n",
    "        batch_size = batch.max().item() + 1\n",
    "        device = x.device\n",
    "        \n",
    "        u_intra_protein = self.W_intra_protein(x)\n",
    "        u_intra_ligand = self.W_intra_ligand(x)\n",
    "        u_inter = self.W_inter_connection(x)\n",
    "        \n",
    "        u = torch.stack([u_intra_protein, u_intra_ligand, u_inter], dim=1)\n",
    "        \n",
    "        # OPTIMIZED: Use cached edge type counts if available\n",
    "        b = torch.zeros(x.size(0), 3, device=device)\n",
    "        \n",
    "        if edge_type_counts is not None:\n",
    "            total_edges = edge_type_counts.sum()\n",
    "            if total_edges > 0:\n",
    "                if edge_type_counts[2] > 0:\n",
    "                    b[:, 2] += 4.0\n",
    "                    if edge_type_counts[0] > 0:\n",
    "                        b[:, 0] += 2.0\n",
    "                    if edge_type_counts[1] > 0:\n",
    "                        b[:, 1] += 2.0\n",
    "                else:\n",
    "                    if edge_type_counts[0] > 5:\n",
    "                        b[:, 0] += 2.5\n",
    "                    if edge_type_counts[1] > 5:\n",
    "                        b[:, 1] += 2.5\n",
    "        \n",
    "        routing_history = []\n",
    "        \n",
    "        for iteration in range(self.num_iterations):\n",
    "            c = F.softmax(b, dim=-1)\n",
    "            routing_history.append(c.detach().cpu())\n",
    "            \n",
    "            s = torch.zeros(batch_size, 3, self.capsule_dim, device=device)\n",
    "            \n",
    "            for batch_idx in range(batch_size):\n",
    "                batch_mask = (batch == batch_idx)\n",
    "                if batch_mask.sum() == 0:\n",
    "                    continue\n",
    "                \n",
    "                batch_u = u[batch_mask]\n",
    "                batch_c = c[batch_mask]\n",
    "                \n",
    "                for cap_idx in range(3):\n",
    "                    s[batch_idx, cap_idx] = torch.sum(\n",
    "                        batch_c[:, cap_idx:cap_idx+1] * batch_u[:, cap_idx], dim=0\n",
    "                    )\n",
    "                \n",
    "                s[batch_idx] = self.squash(s[batch_idx].clone())\n",
    "            \n",
    "            if iteration < self.num_iterations - 1:\n",
    "                for batch_idx in range(batch_size):\n",
    "                    batch_mask = (batch == batch_idx)\n",
    "                    if batch_mask.sum() == 0:\n",
    "                        continue\n",
    "                    \n",
    "                    batch_u = u[batch_mask]\n",
    "                    batch_s = s[batch_idx]\n",
    "                    \n",
    "                    agreement = torch.sum(batch_u * batch_s.unsqueeze(0), dim=-1)\n",
    "                    \n",
    "                    # OPTIMIZED: Simplified routing updates\n",
    "                    if edge_type_counts is not None:\n",
    "                        edge_type_bonus = torch.zeros_like(agreement)\n",
    "                        \n",
    "                        inter_count = edge_type_counts[2].item()\n",
    "                        if inter_count > 0:\n",
    "                            edge_type_bonus[:, 2] += 2.5\n",
    "                            if edge_type_counts[0] > 0:\n",
    "                                edge_type_bonus[:, 0] += 0.8\n",
    "                            if edge_type_counts[1] > 0:\n",
    "                                edge_type_bonus[:, 1] += 0.8\n",
    "                        else:\n",
    "                            for edge_type in range(2):\n",
    "                                if edge_type_counts[edge_type] > 3:\n",
    "                                    edge_type_bonus[:, edge_type] += 1.2\n",
    "                        \n",
    "                        agreement += edge_type_bonus\n",
    "                    \n",
    "                    b[batch_mask] += agreement\n",
    "        \n",
    "        self.routing_coefficients = routing_history[-1]\n",
    "        return s, self.routing_coefficients\n",
    "\n",
    "class OptimizedEdgeAwareCapsuleGNN(nn.Module):\n",
    "    def __init__(self, input_dim=17, hidden_dim=64, num_layers=2):\n",
    "        super(OptimizedEdgeAwareCapsuleGNN, self).__init__()\n",
    "        \n",
    "        self.hidden_dim = hidden_dim\n",
    "        \n",
    "        self.convs = nn.ModuleList()\n",
    "        self.input_proj = nn.Linear(input_dim, hidden_dim)\n",
    "        \n",
    "        for i in range(num_layers):\n",
    "            self.convs.append(GCNConv(hidden_dim, hidden_dim))\n",
    "        \n",
    "        self.capsule_layer = OptimizedEdgeTypeAwareCapsuleLayer(hidden_dim, capsule_dim=32)\n",
    "        \n",
    "        self.predictor = nn.Sequential(\n",
    "            nn.Linear(3 * 32, hidden_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.2),\n",
    "            nn.Linear(hidden_dim, hidden_dim // 2),\n",
    "            nn.ReLU(), \n",
    "            nn.Dropout(0.1),\n",
    "            nn.Linear(hidden_dim // 2, 1)\n",
    "        )\n",
    "        \n",
    "        self.dropout = nn.Dropout(0.1)\n",
    "        self.apply(self._init_weights)\n",
    "        \n",
    "        self.last_routing_coefficients = None\n",
    "        # MODIFIED: Store additional info for batch curvature computation\n",
    "        self.last_embeddings = None\n",
    "        self.last_edge_index = None\n",
    "        self.last_batch = None\n",
    "    \n",
    "    def _init_weights(self, module):\n",
    "        if isinstance(module, nn.Linear):\n",
    "            nn.init.kaiming_normal_(module.weight, mode='fan_out', nonlinearity='relu')\n",
    "            if module.bias is not None:\n",
    "                nn.init.constant_(module.bias, 0)\n",
    "    \n",
    "    def forward(self, x, edge_index, edge_attr, batch, edge_types=None, edge_type_counts=None):\n",
    "        x = self.input_proj(x)\n",
    "        x = F.relu(x)\n",
    "        \n",
    "        for i, conv in enumerate(self.convs):\n",
    "            residual = x\n",
    "            x = conv(x, edge_index)\n",
    "            x = F.relu(x)\n",
    "            x = self.dropout(x)\n",
    "            \n",
    "            if i > 0:\n",
    "                x = x + residual\n",
    "        \n",
    "        # MODIFIED: Store for batch curvature computation\n",
    "        self.last_embeddings = x\n",
    "        self.last_edge_index = edge_index\n",
    "        self.last_batch = batch\n",
    "        \n",
    "        if edge_types is None:\n",
    "            edge_types = torch.zeros(edge_index.size(1), dtype=torch.long, device=edge_index.device)\n",
    "        \n",
    "        capsule_outputs, routing_coeffs = self.capsule_layer(x, edge_index, edge_types, batch, edge_type_counts)\n",
    "        self.last_routing_coefficients = routing_coeffs\n",
    "        \n",
    "        batch_size = capsule_outputs.size(0)\n",
    "        flattened = capsule_outputs.view(batch_size, -1)\n",
    "        output = self.predictor(flattened)\n",
    "        \n",
    "        return output\n",
    "    \n",
    "    def get_routing_analysis(self, combination):\n",
    "        if self.last_routing_coefficients is None:\n",
    "            return None\n",
    "        \n",
    "        routing = self.last_routing_coefficients\n",
    "        \n",
    "        analysis = {\n",
    "            'intra_protein_attention': routing[:, 0].mean().item(),\n",
    "            'intra_ligand_attention': routing[:, 1].mean().item(), \n",
    "            'inter_connection_attention': routing[:, 2].mean().item(),\n",
    "            'combination': combination,\n",
    "            'has_interaction': 'I' in combination\n",
    "        }\n",
    "        \n",
    "        return analysis\n",
    "\n",
    "# OPTIMIZED: Training with gradient accumulation and smaller batch size\n",
    "def train_optimized_model_with_curvature(model, train_loader, val_loader, combination, \n",
    "                                         epochs=150, device='cuda', gamma=0.1, \n",
    "                                         num_curve_samples=50, similarity_type='normalized_adjacency'):\n",
    "    \"\"\"\n",
    "    Train model with optimized batch curvature regularization\n",
    "    \"\"\"\n",
    "    model = model.to(device)\n",
    "    optimizer = torch.optim.AdamW(model.parameters(), lr=0.001, weight_decay=1e-4)\n",
    "    criterion = nn.MSELoss()\n",
    "    scheduler = torch.optim.lr_scheduler.CosineAnnealingWarmRestarts(optimizer, T_0=5, T_mult=2, eta_min=1e-6)\n",
    "    \n",
    "    best_val_loss = float('inf')\n",
    "    best_model_state = None\n",
    "    patience = 8\n",
    "    patience_counter = 0\n",
    "    \n",
    "    routing_stats = []\n",
    "    accumulation_steps = 2\n",
    "    \n",
    "    # Track losses\n",
    "    loss_history = {'mse': [], 'curvature': [], 'total': []}\n",
    "    \n",
    "    for epoch in range(epochs):\n",
    "        model.train()\n",
    "        total_loss = 0\n",
    "        total_mse_loss = 0\n",
    "        total_curve_loss = 0\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        for batch_idx, batch in enumerate(train_loader):\n",
    "            batch = batch.to(device)\n",
    "            \n",
    "            edge_types = getattr(batch, 'edge_types', None)\n",
    "            edge_type_counts = getattr(batch, 'edge_type_counts', None)\n",
    "            \n",
    "            # Forward pass\n",
    "            pred = model(batch.x, batch.edge_index, batch.edge_attr, batch.batch, \n",
    "                        edge_types, edge_type_counts).squeeze()\n",
    "            \n",
    "            # MSE loss\n",
    "            mse_loss = criterion(pred, batch.y)\n",
    "            \n",
    "            # NEW: Optimized batch curvature loss\n",
    "            if model.last_embeddings is not None and gamma > 0:\n",
    "                curve_loss = curvature_regularization_loss_batch_fast(\n",
    "                    model.last_embeddings, \n",
    "                    model.last_edge_index, \n",
    "                    model.last_batch,\n",
    "                    num_samples_per_graph=num_curve_samples\n",
    "                )\n",
    "            else:\n",
    "                curve_loss = torch.tensor(0.0, device=device)\n",
    "            \n",
    "            # Total loss\n",
    "            loss = mse_loss + gamma * curve_loss\n",
    "            \n",
    "            loss = loss / accumulation_steps\n",
    "            loss.backward()\n",
    "            \n",
    "            total_mse_loss += mse_loss.item()\n",
    "            total_curve_loss += curve_loss.item()\n",
    "            \n",
    "            if (batch_idx + 1) % accumulation_steps == 0:\n",
    "                torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n",
    "                optimizer.step()\n",
    "                optimizer.zero_grad()\n",
    "            \n",
    "            total_loss += loss.item() * accumulation_steps\n",
    "        \n",
    "        # Handle remaining gradients\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n",
    "        optimizer.step()\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        # Validation\n",
    "        model.eval()\n",
    "        val_loss = 0\n",
    "        val_mse_loss = 0\n",
    "        val_curve_loss = 0\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            for batch in val_loader:\n",
    "                batch = batch.to(device)\n",
    "                edge_types = getattr(batch, 'edge_types', None)\n",
    "                edge_type_counts = getattr(batch, 'edge_type_counts', None)\n",
    "                pred = model(batch.x, batch.edge_index, batch.edge_attr, batch.batch,\n",
    "                           edge_types, edge_type_counts).squeeze()\n",
    "                \n",
    "                mse_loss = criterion(pred, batch.y)\n",
    "                val_mse_loss += mse_loss.item()\n",
    "                \n",
    "                # NEW: Batch curvature loss for validation\n",
    "                if model.last_embeddings is not None and gamma > 0:\n",
    "                    curve_loss = curvature_regularization_loss_batch_fast(\n",
    "                        model.last_embeddings,\n",
    "                        model.last_edge_index,\n",
    "                        model.last_batch,\n",
    "                        num_samples_per_graph=num_curve_samples\n",
    "                    )\n",
    "                    val_curve_loss += curve_loss.item()\n",
    "                else:\n",
    "                    curve_loss = torch.tensor(0.0, device=device)\n",
    "                \n",
    "                val_loss += mse_loss.item() + gamma * curve_loss.item()\n",
    "        \n",
    "        avg_val_loss = val_loss / len(val_loader)\n",
    "        avg_val_mse = val_mse_loss / len(val_loader)\n",
    "        avg_val_curve = val_curve_loss / len(val_loader)\n",
    "        \n",
    "        scheduler.step()\n",
    "        \n",
    "        # Track losses\n",
    "        loss_history['mse'].append(avg_val_mse)\n",
    "        loss_history['curvature'].append(avg_val_curve)\n",
    "        loss_history['total'].append(avg_val_loss)\n",
    "        \n",
    "        if epoch % 5 == 0:\n",
    "            routing_analysis = model.get_routing_analysis(combination)\n",
    "            if routing_analysis:\n",
    "                routing_analysis['epoch'] = epoch\n",
    "                routing_stats.append(routing_analysis)\n",
    "        \n",
    "        if avg_val_loss < best_val_loss:\n",
    "            best_val_loss = avg_val_loss\n",
    "            best_model_state = model.state_dict().copy()\n",
    "            patience_counter = 0\n",
    "        else:\n",
    "            patience_counter += 1\n",
    "        \n",
    "        if epoch >= 8 and patience_counter >= patience:\n",
    "            print(f\"    Early stopping at epoch {epoch}\")\n",
    "            break\n",
    "        \n",
    "        if epoch % 5 == 0:\n",
    "            print(f\"    Epoch {epoch}: Train Loss={total_loss/len(train_loader):.4f}, \"\n",
    "                  f\"Val Loss={avg_val_loss:.4f} (MSE={avg_val_mse:.4f}, Curve={avg_val_curve:.4f})\")\n",
    "    \n",
    "    if best_model_state is not None:\n",
    "        model.load_state_dict(best_model_state)\n",
    "    \n",
    "    model_save_path = f\"optimized_edge_aware_capsule_curvature_{combination}_model.pth\"\n",
    "    torch.save({\n",
    "        'model_state_dict': model.state_dict(),\n",
    "        'combination': combination,\n",
    "        'input_dim': model.input_proj.in_features,\n",
    "        'hidden_dim': model.hidden_dim,\n",
    "        'best_val_loss': best_val_loss,\n",
    "        'routing_stats': routing_stats,\n",
    "        'loss_history': loss_history,\n",
    "        'gamma': gamma,\n",
    "        'similarity_type': similarity_type\n",
    "    }, model_save_path)\n",
    "    print(f\"    Model with curvature regularization saved to {model_save_path}\")\n",
    "    \n",
    "    return model, routing_stats, loss_history\n",
    "\n",
    "def test_optimized_model(model, test_loader, combination, device='cuda'):\n",
    "    model.eval()\n",
    "    predictions = []\n",
    "    targets = []\n",
    "    routing_analyses = []\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for batch in test_loader:\n",
    "            batch = batch.to(device)\n",
    "            edge_types = getattr(batch, 'edge_types', None)\n",
    "            edge_type_counts = getattr(batch, 'edge_type_counts', None)\n",
    "            pred = model(batch.x, batch.edge_index, batch.edge_attr, batch.batch,\n",
    "                        edge_types, edge_type_counts).squeeze()\n",
    "            \n",
    "            if pred.dim() == 0:\n",
    "                pred = pred.unsqueeze(0)\n",
    "            if batch.y.dim() == 0:\n",
    "                batch.y = batch.y.unsqueeze(0)\n",
    "            \n",
    "            predictions.extend(pred.cpu().numpy())\n",
    "            targets.extend(batch.y.cpu().numpy())\n",
    "            \n",
    "            routing_analysis = model.get_routing_analysis(combination)\n",
    "            if routing_analysis:\n",
    "                routing_analyses.append(routing_analysis)\n",
    "    \n",
    "    predictions = np.array(predictions)\n",
    "    targets = np.array(targets)\n",
    "    \n",
    "    if len(predictions) > 1 and predictions.std() > 0.01:\n",
    "        rp, _ = pearsonr(predictions, targets)\n",
    "    else:\n",
    "        rp = 0.0\n",
    "    \n",
    "    rmse = np.sqrt(np.mean((predictions - targets) ** 2))\n",
    "    \n",
    "    return predictions, targets, rp, rmse, routing_analyses\n",
    "\n",
    "def load_optimized_model(model_path, device='cuda'):\n",
    "    \"\"\"Load a saved optimized edge-aware capsule model\"\"\"\n",
    "    checkpoint = torch.load(model_path, map_location=device)\n",
    "    \n",
    "    input_dim = checkpoint['input_dim']\n",
    "    hidden_dim = checkpoint['hidden_dim']\n",
    "    model = OptimizedEdgeAwareCapsuleGNN(input_dim=input_dim, hidden_dim=hidden_dim, num_layers=2)\n",
    "    \n",
    "    model.load_state_dict(checkpoint['model_state_dict'])\n",
    "    model = model.to(device)\n",
    "    \n",
    "    print(f\"Loaded optimized model for combination: {checkpoint['combination']}\")\n",
    "    print(f\"Best validation loss: {checkpoint['best_val_loss']:.4f}\")\n",
    "    \n",
    "    return model, checkpoint\n",
    "\n",
    "def analyze_optimized_edge_importance(model, test_loader, combination, device='cuda'):\n",
    "    \"\"\"Analyze which edge types the optimized model focuses on\"\"\"\n",
    "    model.eval()\n",
    "    \n",
    "    edge_type_attention = {'intra_protein': [], 'intra_ligand': [], 'inter_connection': []}\n",
    "    edge_type_counts = {'intra_protein': 0, 'intra_ligand': 0, 'inter_connection': 0}\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for batch in test_loader:\n",
    "            batch = batch.to(device)\n",
    "            edge_types = getattr(batch, 'edge_types', None)\n",
    "            edge_type_counts_batch = getattr(batch, 'edge_type_counts', None)\n",
    "            \n",
    "            pred = model(batch.x, batch.edge_index, batch.edge_attr, batch.batch, \n",
    "                        edge_types, edge_type_counts_batch)\n",
    "            \n",
    "            if hasattr(model, 'last_routing_coefficients') and model.last_routing_coefficients is not None:\n",
    "                routing = model.last_routing_coefficients\n",
    "                \n",
    "                edge_type_attention['intra_protein'].extend(routing[:, 0].tolist())\n",
    "                edge_type_attention['intra_ligand'].extend(routing[:, 1].tolist())\n",
    "                edge_type_attention['inter_connection'].extend(routing[:, 2].tolist())\n",
    "            \n",
    "            if edge_type_counts_batch is not None:\n",
    "                edge_type_counts['intra_protein'] += edge_type_counts_batch[0].item()\n",
    "                edge_type_counts['intra_ligand'] += edge_type_counts_batch[1].item()\n",
    "                edge_type_counts['inter_connection'] += edge_type_counts_batch[2].item()\n",
    "    \n",
    "    analysis = {\n",
    "        'combination': combination,\n",
    "        'avg_attention': {\n",
    "            'intra_protein': np.mean(edge_type_attention['intra_protein']),\n",
    "            'intra_ligand': np.mean(edge_type_attention['intra_ligand']),\n",
    "            'inter_connection': np.mean(edge_type_attention['inter_connection'])\n",
    "        },\n",
    "        'edge_counts': edge_type_counts,\n",
    "        'attention_vs_count_ratio': {\n",
    "            'intra_protein': np.mean(edge_type_attention['intra_protein']) / max(edge_type_counts['intra_protein'], 1),\n",
    "            'intra_ligand': np.mean(edge_type_attention['intra_ligand']) / max(edge_type_counts['intra_ligand'], 1),\n",
    "            'inter_connection': np.mean(edge_type_attention['inter_connection']) / max(edge_type_counts['inter_connection'], 1)\n",
    "        }\n",
    "    }\n",
    "    \n",
    "    return analysis\n",
    "\n",
    "def main():\n",
    "    print(\"üöÄ OPTIMIZED EDGE-TYPE AWARE CAPSULE NETWORK WITH CURVATURE REGULARIZATION\")\n",
    "    print(\"=\"*75)\n",
    "    \n",
    "    # File paths - UPDATE THESE TO YOUR ACTUAL PATHS\n",
    "    real_train_csv = 'D:\\\\PhD\\\\Chapter_4\\\\Code2\\\\pdbbind\\\\pdb_ids_Affinity\\\\training_set_with_affinity.csv'\n",
    "    real_val_csv = 'D:\\\\PhD\\\\Chapter_4\\\\Code2\\\\pdbbind\\\\pdb_ids_Affinity\\\\validation_set_with_affinity.csv'\n",
    "    real_data_path = 'D:\\\\PhD\\\\Chapter_4\\\\Code2\\\\pdbbind\\\\dataset'\n",
    "    \n",
    "    synthetic_train_dir = 'D:\\\\PhD\\\\Chapter_4\\\\Code2\\\\pdbbind\\\\complete_graphs_20250709_163209\\\\training_synthetic'\n",
    "    synthetic_val_dir = 'D:\\\\PhD\\\\Chapter_4\\\\Code2\\\\pdbbind\\\\complete_graphs_20250709_163209\\\\validation_synthetic'\n",
    "    \n",
    "    core_csv = 'D:\\\\PhD\\\\Chapter_4\\\\Code2\\\\pdbbind\\\\pdb_ids_Affinity\\\\core_set_with_affinity.csv'\n",
    "    holdout_csv = 'D:\\\\PhD\\\\Chapter_4\\\\Code2\\\\pdbbind\\\\pdb_ids_Affinity\\\\hold_out_set_with_affinity.csv'\n",
    "    \n",
    "    print(\"‚ö° OPTIMIZATION FEATURES:\")\n",
    "    print(\"‚úÖ Reduced routing iterations (3‚Üí2)\")\n",
    "    print(\"‚úÖ Cached edge type statistics\")\n",
    "    print(\"‚úÖ Simplified normalization\")\n",
    "    print(\"‚úÖ Precomputed merged graphs\")\n",
    "    print(\"‚úÖ Gradient accumulation (batch_size=4, accumulate=2)\")\n",
    "    print(\"‚úÖ Reduced logging frequency\")\n",
    "    print(\"üÜï Curvature Regularization Loss\")\n",
    "    \n",
    "    combinations = ['P', 'L', 'I', 'PL', 'PI', 'LI', 'PLI']\n",
    "    \n",
    "    # HYPERPARAMETERS for curvature regularization\n",
    "    gamma = 0.1  # Weight for curvature loss\n",
    "    num_curve_samples = 100  # Number of node pairs to sample\n",
    "    similarity_type = 'normalized_adjacency'  # or 'adjacency' or 'diffusion'\n",
    "    \n",
    "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "    print(f\"Using device: {device}\")\n",
    "    print(f\"Curvature regularization: Œ≥={gamma}, samples={num_curve_samples}, similarity={similarity_type}\")\n",
    "    \n",
    "    results = {}\n",
    "    routing_analyses = {}\n",
    "    saved_models = {}\n",
    "    \n",
    "    for combination in combinations:\n",
    "        print(f\"\\n{'='*20} OPTIMIZED {combination} WITH CURVATURE {'='*20}\")\n",
    "        \n",
    "        try:\n",
    "            combined_train_data, combined_val_data = prepare_combined_training_dataset(\n",
    "                real_train_csv, real_val_csv, real_data_path,\n",
    "                synthetic_train_dir, synthetic_val_dir, combination,\n",
    "                use_half=False\n",
    "            )\n",
    "            \n",
    "            if len(combined_train_data) == 0 or len(combined_val_data) == 0:\n",
    "                print(f\"  Insufficient combined data for {combination}, skipping...\")\n",
    "                continue\n",
    "            \n",
    "            core_df = load_csv(core_csv, use_half=False)\n",
    "            holdout_df = load_csv(holdout_csv, use_half=False)\n",
    "            core_data = prepare_real_dataset_combined(core_df, real_data_path, combination)\n",
    "            holdout_data = prepare_real_dataset_combined(holdout_df, real_data_path, combination)\n",
    "            \n",
    "            train_loader = DataLoader(combined_train_data, batch_size=4, shuffle=True)\n",
    "            val_loader = DataLoader(combined_val_data, batch_size=4)\n",
    "            core_loader = DataLoader(core_data, batch_size=4) if core_data else None\n",
    "            holdout_loader = DataLoader(holdout_data, batch_size=4) if holdout_data else None\n",
    "            \n",
    "            input_dim = combined_train_data[0].x.size(1)\n",
    "            print(f\"  Input dimension: {input_dim}\")\n",
    "            \n",
    "            print(\"Training Optimized Edge-Type Aware Capsule Network with Curvature Regularization...\")\n",
    "            model = OptimizedEdgeAwareCapsuleGNN(input_dim=input_dim, hidden_dim=64, num_layers=2)\n",
    "            print(f\"Model parameters: {sum(p.numel() for p in model.parameters()):,}\")\n",
    "            \n",
    "            # MODIFIED: Use new training function with curvature regularization\n",
    "            trained_model, routing_stats, loss_history = train_optimized_model_with_curvature(\n",
    "                model, train_loader, val_loader, combination, \n",
    "                epochs=150, device=device, gamma=gamma,\n",
    "                num_curve_samples=num_curve_samples, similarity_type=similarity_type\n",
    "            )\n",
    "            \n",
    "            # MODIFIED: Save model path with new name\n",
    "            saved_models[combination] = f\"optimized_edge_aware_capsule_curvature_{combination}_model.pth\"\n",
    "            \n",
    "            # Test the model (same as before)\n",
    "            core_rp = core_rmse = holdout_rp = holdout_rmse = 0\n",
    "            \n",
    "            if core_loader:\n",
    "                print(\"Testing on 2016 Core Set...\")\n",
    "                core_preds, core_targets, core_rp, core_rmse, core_routing = test_optimized_model(\n",
    "                    trained_model, core_loader, combination, device)\n",
    "                print(f\"  Core Set - Rp: {core_rp:.3f}, RMSE: {core_rmse:.3f}\")\n",
    "                \n",
    "                if core_routing:\n",
    "                    avg_routing = {\n",
    "                        'intra_protein': np.mean([r['intra_protein_attention'] for r in core_routing]),\n",
    "                        'intra_ligand': np.mean([r['intra_ligand_attention'] for r in core_routing]), \n",
    "                        'inter_connection': np.mean([r['inter_connection_attention'] for r in core_routing])\n",
    "                    }\n",
    "                    print(f\"  Core Routing - Intra-P: {avg_routing['intra_protein']:.3f}, \"\n",
    "                          f\"Intra-L: {avg_routing['intra_ligand']:.3f}, Inter: {avg_routing['inter_connection']:.3f}\")\n",
    "                    routing_analyses[combination] = avg_routing\n",
    "            \n",
    "            if holdout_loader:\n",
    "                print(\"Testing on 2019 Holdout Set...\")\n",
    "                holdout_preds, holdout_targets, holdout_rp, holdout_rmse, holdout_routing = test_optimized_model(\n",
    "                    trained_model, holdout_loader, combination, device)\n",
    "                print(f\"  Holdout Set - Rp: {holdout_rp:.3f}, RMSE: {holdout_rmse:.3f}\")\n",
    "            \n",
    "            results[combination] = {\n",
    "                'core_rp': core_rp,\n",
    "                'core_rmse': core_rmse,\n",
    "                'holdout_rp': holdout_rp,\n",
    "                'holdout_rmse': holdout_rmse,\n",
    "                'routing_stats': routing_stats,\n",
    "                'loss_history': loss_history,\n",
    "                'train_samples': len(combined_train_data),\n",
    "                'val_samples': len(combined_val_data),\n",
    "                'gamma': gamma,\n",
    "                'similarity_type': similarity_type\n",
    "            }\n",
    "            \n",
    "            # Clean up GPU memory\n",
    "            del trained_model, model\n",
    "            torch.cuda.empty_cache()\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"Error processing {combination}: {e}\")\n",
    "            import traceback\n",
    "            traceback.print_exc()\n",
    "            continue\n",
    "    \n",
    "    # MODIFIED: Save results with new filename\n",
    "    print(\"\\nüíæ Saving results with curvature regularization...\")\n",
    "    \n",
    "    test_results_data = []\n",
    "    for combination in combinations:\n",
    "        if combination in results:\n",
    "            r = results[combination]\n",
    "            routing = routing_analyses.get(combination, {'intra_protein': 0, 'intra_ligand': 0, 'inter_connection': 0})\n",
    "            \n",
    "            test_results_data.append({\n",
    "                'combination': combination,\n",
    "                'core_rp': r['core_rp'],\n",
    "                'core_rmse': r['core_rmse'],\n",
    "                'holdout_rp': r['holdout_rp'],\n",
    "                'holdout_rmse': r['holdout_rmse'],\n",
    "                'train_samples': r['train_samples'],\n",
    "                'val_samples': r['val_samples'],\n",
    "                'intra_protein_attention': routing['intra_protein'],\n",
    "                'intra_ligand_attention': routing['intra_ligand'],\n",
    "                'inter_connection_attention': routing['inter_connection'],\n",
    "                'gamma': r['gamma'],\n",
    "                'similarity_type': r['similarity_type'],\n",
    "                'model_path': saved_models.get(combination, 'N/A')\n",
    "            })\n",
    "    \n",
    "    results_df = pd.DataFrame(test_results_data)\n",
    "    results_df.to_csv('optimized_edge_aware_capsule_curvature_results.csv', index=False)\n",
    "    \n",
    "    print(\"‚úÖ Results saved to 'optimized_edge_aware_capsule_curvature_results.csv'\")\n",
    "    print(\"‚úÖ Models saved with naming: 'optimized_edge_aware_capsule_curvature_{combination}_model.pth'\")\n",
    "    \n",
    "    print(\"\\nüéâ OPTIMIZED EXECUTION WITH CURVATURE REGULARIZATION COMPLETED!\")\n",
    "    print(f\"üìä {len(results)} models tested with curvature regularization\")\n",
    "    print(f\"üîç Curvature parameters: Œ≥={gamma}, samples={num_curve_samples}, similarity={similarity_type}\")\n",
    "    \n",
    "    print(\"\\nüìñ LOADING CURVATURE-REGULARIZED MODEL:\")\n",
    "    print(\"```python\")\n",
    "    print(\"model, checkpoint = load_optimized_model('optimized_edge_aware_capsule_curvature_PLI_model.pth')\")\n",
    "    print(\"# Access curvature parameters:\")\n",
    "    print(\"gamma = checkpoint['gamma']\")\n",
    "    print(\"similarity_type = checkpoint['similarity_type']\")\n",
    "    print(\"loss_history = checkpoint['loss_history']\")\n",
    "    print(\"```\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    try:\n",
    "        main()\n",
    "    except Exception as e:\n",
    "        print(f\"\\n‚ùå CRITICAL ERROR: {e}\")\n",
    "        import traceback\n",
    "        traceback.print_exc()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e5d98ad-acfd-4369-b1f7-bfbdab1faebf",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [torchfix]",
   "language": "python",
   "name": "torchfix"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
