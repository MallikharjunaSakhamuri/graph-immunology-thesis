{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/MallikharjunaSakhamuri/EdgeRepresentationDHT/blob/main/EdgeRepresentationDHT.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "J8t2CE-XWf5r"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import numpy as np"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yPiflVrPWosg",
        "scrolled": true
      },
      "outputs": [],
      "source": [
        "!pip install stellargraph"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TvXHPnkbwR4o"
      },
      "outputs": [],
      "source": [
        "!pip install chardet"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OAInvHTmFhal"
      },
      "outputs": [],
      "source": [
        "import stellargraph as sg\n",
        "from stellargraph.mapper import PaddedGraphGenerator\n",
        "from stellargraph.layer import GCNSupervisedGraphClassification\n",
        "from stellargraph import StellarGraph"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9rkZgJjAFiqY"
      },
      "outputs": [],
      "source": [
        "from sklearn import model_selection\n",
        "from sklearn.metrics import confusion_matrix,auc,precision_recall_curve,roc_curve\n",
        "from tensorflow.keras import Model\n",
        "from tensorflow.keras.layers import Dense\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "from tensorflow.keras.losses import binary_crossentropy,mean_squared_error\n",
        "from tensorflow.keras.callbacks import EarlyStopping\n",
        "import tensorflow as tf\n",
        "from stellargraph import IndexedArray"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5KrVFrSEFlXS"
      },
      "outputs": [],
      "source": [
        "import itertools\n",
        "import matplotlib.pyplot as plt"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "roaJbWysFm8A"
      },
      "outputs": [],
      "source": [
        "def read_para(path):\n",
        "    df = pd.read_csv(path,sep='\\t',header=None)\n",
        "    dic = {}\n",
        "    for i in range(df.shape[0]):\n",
        "        hla = df[0].iloc[i]\n",
        "        paratope = df[1].iloc[i]\n",
        "        try:\n",
        "            dic[hla] = paratope\n",
        "        except KeyError:\n",
        "            dic[hla] = []\n",
        "            dic[hla].append(paratope)\n",
        "    return dic"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "iJPLF2XGFoWk"
      },
      "outputs": [],
      "source": [
        "def dict_inventory(inventory):\n",
        "    dicA, dicB, dicC = {}, {}, {}\n",
        "    dic = {'A': dicA, 'B': dicB, 'C': dicC}\n",
        "\n",
        "    for hla in inventory:\n",
        "        type_ = hla[4]  # A,B,C\n",
        "        first2 = hla[6:8]  # 01\n",
        "        last2 = hla[8:]  # 01\n",
        "        try:\n",
        "            dic[type_][first2].append(last2)\n",
        "        except KeyError:\n",
        "            dic[type_][first2] = []\n",
        "            dic[type_][first2].append(last2)\n",
        "\n",
        "    return dic"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LNlXP2GHFrMK"
      },
      "outputs": [],
      "source": [
        "def rescue_unknown_hla(hla, dic_inventory):\n",
        "    type_ = hla[4]\n",
        "    first2 = hla[6:8]\n",
        "    last2 = hla[8:]\n",
        "    big_category = dic_inventory[type_]\n",
        "    #print(hla)\n",
        "    if not big_category.get(first2) == None:\n",
        "        small_category = big_category.get(first2)\n",
        "        distance = [abs(int(last2) - int(i)) for i in small_category]\n",
        "        optimal = min(zip(small_category, distance), key=lambda x: x[1])[0]\n",
        "        return 'HLA-' + str(type_) + '*' + str(first2) + str(optimal)\n",
        "    else:\n",
        "        small_category = list(big_category.keys())\n",
        "        distance = [abs(int(first2) - int(i)) for i in small_category]\n",
        "        optimal = min(zip(small_category, distance), key=lambda x: x[1])[0]\n",
        "        return 'HLA-' + str(type_) + '*' + str(optimal) + str(big_category[optimal][0])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lTDefDkrFtZc"
      },
      "outputs": [],
      "source": [
        "class Graph_Constructor():\n",
        "\n",
        "    @staticmethod\n",
        "    def combinator(pep,hla):\n",
        "        source = ['p' + str(i+1) for i in range(len(pep))]\n",
        "        target = ['h' + str(i+1) for i in range(len(hla))]\n",
        "        return source,target\n",
        "\n",
        "    @staticmethod\n",
        "    def numerical(pep,hla,after_pca,embed=12):   # after_pca [21,12]\n",
        "        pep = pep.replace('X','-').upper()\n",
        "        hla = hla.replace('X','-').upper()\n",
        "        feature_array_pep = np.empty([len(pep),embed])\n",
        "        feature_array_hla = np.empty([len(hla),embed])\n",
        "        amino = 'ARNDCQEGHILKMFPSTWYV-'\n",
        "        for i in range(len(pep)):\n",
        "            feature_array_pep[i,:] = after_pca[amino.index(pep[i]),:]\n",
        "        for i in range(len(hla)):\n",
        "            feature_array_hla[i,:] = after_pca[amino.index(hla[i]),:]\n",
        "        feature_array = np.concatenate([feature_array_pep,feature_array_hla],axis=0)\n",
        "        #print(feature_array_pep.shape,feature_array_hla.shape,feature_array.shape)\n",
        "        return feature_array\n",
        "\n",
        "    @staticmethod\n",
        "    def unweight_edge(pep,hla,after_pca):\n",
        "        source,target = Graph_Constructor.combinator(pep,hla)\n",
        "        combine = list(itertools.product(source,target))\n",
        "        weight = itertools.repeat(1,len(source)*len(target))\n",
        "        edges = pd.DataFrame({'source':[item[0] for item in combine],'target':[item[1] for item in combine],'weight':weight})\n",
        "        feature_array = Graph_Constructor.numerical(pep,hla,after_pca)\n",
        "        try:nodes = IndexedArray(feature_array,index=source+target)\n",
        "        except: print(pep,hla,feature_array.shape)\n",
        "        graph = StellarGraph(nodes,edges,node_type_default='corner',edge_type_default='line')\n",
        "        return graph\n",
        "\n",
        "    @staticmethod\n",
        "    def update_edges_new(pep,hla,edges):\n",
        "\n",
        "#       print ( \"pep\", pep)\n",
        "#       print ( \"hla\", hla)\n",
        "#       print ( \"edges\", edges)\n",
        "\n",
        "      pep = pep.replace('X','-').upper()\n",
        "      hla = hla.replace('X','-').upper()\n",
        "\n",
        "      amino = 'ARNDCQEGHILKMFPSTWYV-'\n",
        "\n",
        "      edges_weights = pd.read_csv(r'D:\\PhD\\New_Graph_Model\\data\\Edge_weights_pca_array_new.txt',sep=' ',header = None)\n",
        "\n",
        "      # create a dictionary to map amino acid characters to their position in the amino acid string\n",
        "      amino_dict = {}\n",
        "      for i, c in enumerate(amino):\n",
        "        amino_dict[c] = i\n",
        "\n",
        "      # loop through the rows of the edges dataframe\n",
        "      for index, row in edges.iterrows():\n",
        "            # get the source and target nodes from the row\n",
        "            source = row['source']\n",
        "            target = row['target']\n",
        "\n",
        "\n",
        "            # check if source is a peptide or HLA allele\n",
        "            if source[0] == 'p':\n",
        "            # get the position of the amino acid in the peptide sequence\n",
        "                pos = int(source[1:]) - 1\n",
        "\n",
        "                # get the amino acid character at that position\n",
        "                amino_char = pep[pos]\n",
        "\n",
        "                # get the index of the amino acid character in the amino acid string\n",
        "                amino_pos = amino_dict[amino_char]\n",
        "\n",
        "            elif source[0] == 'h':\n",
        "                # get the position of the amino acid in the HLA allele\n",
        "                pos = int(source[1:]) - 1\n",
        "\n",
        "                # get the amino acid character at that position\n",
        "                amino_char = hla[pos]\n",
        "\n",
        "                # get the index of the amino acid character in the amino acid string\n",
        "                amino_pos = amino_dict[amino_char]\n",
        "\n",
        "            # check if target is a peptide or HLA allele\n",
        "            if target[0] == 'p':\n",
        "                # get the position of the amino acid in the peptide sequence\n",
        "                pos = int(target[1:]) - 1\n",
        "\n",
        "                # get the amino acid character at that position\n",
        "                amino_char = pep[pos]\n",
        "\n",
        "                # get the index of the amino acid character in the amino acid string\n",
        "                amino_pos2 = amino_dict[amino_char]\n",
        "\n",
        "            elif target[0] == 'h':\n",
        "                # get the position of the amino acid in the HLA allele\n",
        "                pos = int(target[1:]) - 1\n",
        "\n",
        "                # get the amino acid character at that position\n",
        "                amino_char = hla[pos]\n",
        "\n",
        "                # get the index of the amino acid character in the amino acid string\n",
        "                amino_pos2 = amino_dict[amino_char]\n",
        "\n",
        "                # get the weight value from the weights dataframe\n",
        "                weight = edges_weights.iloc[amino_pos, amino_pos2]\n",
        "\n",
        "            # update the weight column in the edges dataframe\n",
        "            #edges.at[index, 'weight'] = weight\n",
        "            edges['weight'] = edges['weight'].astype(float)\n",
        "            edges.at[index, 'weight'] = (weight)\n",
        "\n",
        "      # Check if source[0] and target[0] are same, if not update weight accordingly\n",
        "      for index, row in edges.iterrows():\n",
        "        source, target = row['source'], row['target']\n",
        "        weight = row['weight']\n",
        "        mean = np.mean(edges['weight'])\n",
        "        std = np.std(edges['weight'])\n",
        "\n",
        "        if source[0] != target[0] and (weight > (mean + std) or weight < (mean - std)):\n",
        "            edges.at[index, 'weight'] = -1\n",
        "\n",
        "      return edges\n",
        "\n",
        "    @staticmethod\n",
        "    def weight_anchor_edge(pep,hla,after_pca):\n",
        "        source, target = Graph_Constructor.combinator(pep, hla)\n",
        "        combine = list(itertools.product(source, target))\n",
        "        weight = itertools.repeat(1, len(source) * len(target))\n",
        "        edges = pd.DataFrame({'source': [item[0] for item in combine], 'target': [item[1] for item in combine], 'weight': weight})\n",
        "        for i in range(edges.shape[0]):\n",
        "            col1 = edges.iloc[i]['source']\n",
        "            col2 = edges.iloc[i]['target']\n",
        "            col3 = edges.iloc[i]['weight']\n",
        "            if col1 == 'a2' or col1 == 'a9' or col1 ==  'a10':\n",
        "                edges.iloc[i]['weight'] = 1.5\n",
        "        feature_array = Graph_Constructor.numerical(pep, hla, after_pca)\n",
        "        nodes = IndexedArray(feature_array, index=source + target)\n",
        "        graph = StellarGraph(nodes, edges, node_type_default='corner', edge_type_default='line')\n",
        "        return graph\n",
        "\n",
        "    @staticmethod\n",
        "    def intra_and_inter(pep,hla,after_pca):\n",
        "        source, target = Graph_Constructor.combinator(pep, hla)\n",
        "        combine = list(itertools.product(source, target))\n",
        "        weight = itertools.repeat(2, len(source) * len(target))\n",
        "        edges_inter = pd.DataFrame({'source': [item[0] for item in combine], 'target': [item[1] for item in combine], 'weight': weight})\n",
        "        intra_pep = list(itertools.combinations(source,2))\n",
        "        intra_hla = list(itertools.combinations(target,2))\n",
        "        intra = intra_pep + intra_hla\n",
        "        weight = itertools.repeat(1,len(intra))\n",
        "        edges_intra = pd.DataFrame({'source':[item[0] for item in intra],'target':[item[1] for item in intra],'weight':weight})\n",
        "        edges = pd.concat([edges_inter,edges_intra])\n",
        "        edges = edges.set_index(pd.Index(np.arange(edges.shape[0])))\n",
        "\n",
        "        # Updating Edge weights\n",
        "        edges = Graph_Constructor.update_edges_new(pep,hla,edges)\n",
        "\n",
        "        # Dual Hyper Transfer\n",
        "        #New_nodes,new_Edges = Graph_Constructor.DHT(source,target,edges,feature_array)\n",
        "\n",
        "        feature_array = Graph_Constructor.numerical(pep, hla, after_pca)\n",
        "        nodes = IndexedArray(feature_array, index=source + target)\n",
        "#         print('Nodes Type',type(nodes))\n",
        "#         print('Edges Type',type(edges))\n",
        "#         print (\"nodes, edges:\",nodes, edges)\n",
        "        graph = StellarGraph(nodes, edges, node_type_default='corner', edge_type_default='line')\n",
        "        return graph\n",
        "\n",
        "    @staticmethod\n",
        "    def entrance_v1(df,after_pca,hla_dic,dic_inventory):\n",
        "        graphs = []\n",
        "        graph_labels = []\n",
        "        for i in range(df.shape[0]):\n",
        "            print(i)\n",
        "            pep = df['peptide'].iloc[i]\n",
        "            try:\n",
        "                hla = hla_dic[df['HLA'].iloc[i]]\n",
        "            except KeyError:\n",
        "                hla = hla_dic[rescue_unknown_hla(df['HLA'].iloc[i],dic_inventory)]\n",
        "            label = df['immunogenicity'].iloc[i]\n",
        "            #if label != 'Negative': label = 0\n",
        "            #else: label = 1\n",
        "            #graph = Graph_Constructor.unweight_edge(pep,hla,after_pca)\n",
        "            #graph = Graph_Constructor.unweight_edge(pep,hla,after_pca)\n",
        "            graph = Graph_Constructor.intra_and_inter(pep,hla,after_pca)\n",
        "            graphs.append(graph)\n",
        "            graph_labels.append(label)\n",
        "        graph_labels = pd.Series(graph_labels)\n",
        "        return graphs,graph_labels\n",
        "\n",
        "    @staticmethod\n",
        "    def entrance(df, after_pca, hla_dic, dic_inventory):\n",
        "        graphs = []\n",
        "        graph_labels = []\n",
        "        for i in range(df.shape[0]):\n",
        "            print(i)\n",
        "            pep = df['peptide'].iloc[i]\n",
        "            try:\n",
        "                hla = hla_dic[df['HLA'].iloc[i]]\n",
        "            except KeyError:\n",
        "                hla = hla_dic[rescue_unknown_hla(df['HLA'].iloc[i], dic_inventory)]\n",
        "            label = df['immunogenicity'].iloc[i]\n",
        "\n",
        "            # Construct the hypergraph\n",
        "            graph = HyperGraph_Constructor.hypergraph(pep, hla, after_pca)\n",
        "\n",
        "            graphs.append(graph)\n",
        "            graph_labels.append(label)\n",
        "        graph_labels = pd.Series(graph_labels)\n",
        "        return graphs, graph_labels"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ydQv8DaMwR4q"
      },
      "outputs": [],
      "source": [
        "class HyperGraph_Constructor_v1(Graph_Constructor):\n",
        "\n",
        "    @staticmethod\n",
        "    def combinator_hyperedge(pep, hla):\n",
        "        # Create source-target pairs for hyperedges\n",
        "        source = ['p' + str(i+1) for i in range(len(pep))]\n",
        "        target = ['h' + str(i+1) for i in range(len(hla))]\n",
        "        # In hypergraphs, one edge can link multiple nodes, so return both sets together\n",
        "        return source, target\n",
        "\n",
        "    @staticmethod\n",
        "    def hypergraph(pep, hla, after_pca):\n",
        "        source, target = HyperGraph_Constructor.combinator_hyperedge(pep, hla)\n",
        "        combine = list(itertools.product(source, target))  # Hyperedges can be more complex\n",
        "        weight = itertools.repeat(1, len(source) * len(target))\n",
        "\n",
        "        edges = pd.DataFrame({'source': [item[0] for item in combine],\n",
        "                              'target': [item[1] for item in combine],\n",
        "                              'weight': weight})\n",
        "\n",
        "        feature_array = HyperGraph_Constructor.numerical(pep, hla, after_pca)\n",
        "        nodes = IndexedArray(feature_array, index=source + target)\n",
        "        hypergraph = StellarGraph(nodes, edges, node_type_default='corner', edge_type_default='line')\n",
        "\n",
        "        return hypergraph"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7E1_e7jCwR4q"
      },
      "outputs": [],
      "source": [
        "class HyperGraph_Constructor(Graph_Constructor):\n",
        "\n",
        "    @staticmethod\n",
        "    def combinator_hyperedge(pep, hla):\n",
        "        \"\"\"\n",
        "        Create source-target pairs for hyperedges where each node (from peptide and HLA)\n",
        "        is connected to each other in a bipartite-like graph.\n",
        "        \"\"\"\n",
        "        source = ['p' + str(i+1) for i in range(len(pep))]\n",
        "        target = ['h' + str(i+1) for i in range(len(hla))]\n",
        "        return source, target\n",
        "\n",
        "    @staticmethod\n",
        "    def hypergraph(pep, hla, after_pca):\n",
        "        \"\"\"\n",
        "        Construct a hypergraph from peptide and HLA sequences with associated numerical features.\n",
        "        \"\"\"\n",
        "        # Get source and target nodes\n",
        "        source, target = HyperGraph_Constructor.combinator_hyperedge(pep, hla)\n",
        "\n",
        "        # Create product of all possible source-target pairs to form hyperedges\n",
        "        combine = list(itertools.product(source, target))\n",
        "\n",
        "        # Assign a weight of 1 to all hyperedges (this can be adapted for variable weighting)\n",
        "        weights = [1] * len(combine)  # Replace itertools.repeat for clarity\n",
        "\n",
        "        # Create edges DataFrame with source-target pairs and their weights\n",
        "        edges = pd.DataFrame({\n",
        "            'source': [item[0] for item in combine],\n",
        "            'target': [item[1] for item in combine],\n",
        "            'weight': weights\n",
        "        })\n",
        "\n",
        "        # Generate node features from peptide and HLA sequences\n",
        "        feature_array = HyperGraph_Constructor.numerical(pep, hla, after_pca)\n",
        "\n",
        "        # Combine source and target nodes to form the node set\n",
        "        node_index = source + target\n",
        "\n",
        "        # Create a dictionary where node index maps to their feature vectors\n",
        "        nodes = pd.DataFrame(feature_array, index=node_index)\n",
        "\n",
        "        # Create a StellarGraph object with nodes and edges\n",
        "        hypergraph = StellarGraph(nodes=nodes, edges=edges)\n",
        "\n",
        "        return hypergraph\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yNcUYnyMwR4q"
      },
      "outputs": [],
      "source": [
        "class HyperGraph_Constructor_DHT(Graph_Constructor):\n",
        "\n",
        "    @staticmethod\n",
        "    def combinator_hyperedge(pep, hla):\n",
        "        \"\"\"\n",
        "        Create source-target pairs for hyperedges where each node (from peptide and HLA)\n",
        "        is connected to each other in a bipartite-like graph.\n",
        "        \"\"\"\n",
        "        source = ['p' + str(i+1) for i in range(len(pep))]\n",
        "        target = ['h' + str(i+1) for i in range(len(hla))]\n",
        "        return source, target\n",
        "\n",
        "    @staticmethod\n",
        "    def dht_incidence_matrix(edges_df, node_index):\n",
        "        \"\"\"\n",
        "        Create the incidence matrix for the dual hypergraph transformation.\n",
        "        Swaps the role of nodes and edges.\n",
        "        \"\"\"\n",
        "        incidence_matrix = np.zeros((len(node_index), len(edges_df)))\n",
        "\n",
        "        # For each edge, set the incidence of its source and target nodes\n",
        "        for i, (source, target) in enumerate(zip(edges_df['source'], edges_df['target'])):\n",
        "            incidence_matrix[node_index.index(source), i] = 1\n",
        "            incidence_matrix[node_index.index(target), i] = 1\n",
        "\n",
        "        return incidence_matrix.T  # Return the transpose for the dual form\n",
        "\n",
        "    @staticmethod\n",
        "    def dual_hypergraph(pep, hla, after_pca):\n",
        "        \"\"\"\n",
        "        Construct the dual hypergraph where edges are treated as nodes, and nodes as hyperedges.\n",
        "        \"\"\"\n",
        "        source, target = HyperGraph_Constructor_DHT.combinator_hyperedge(pep, hla)\n",
        "        combine = list(itertools.product(source, target))\n",
        "        weights = [1] * len(combine)  # Assign weight of 1 to all edges\n",
        "\n",
        "        # Create edges DataFrame with source-target pairs and weights\n",
        "        edges = pd.DataFrame({\n",
        "            'source': [item[0] for item in combine],\n",
        "            'target': [item[1] for item in combine],\n",
        "            'weight': weights\n",
        "        })\n",
        "\n",
        "        # Generate node features from peptide and HLA sequences\n",
        "        feature_array = HyperGraph_Constructor_DHT.numerical(pep, hla, after_pca)\n",
        "        node_index = source + target\n",
        "\n",
        "        # Apply Dual Hypergraph Transformation (DHT) - swap the role of nodes and edges\n",
        "        incidence_matrix = HyperGraph_Constructor_DHT.dht_incidence_matrix(edges, node_index)\n",
        "\n",
        "        # Create a StellarGraph object with nodes and incidence matrix (now hyperedges)\n",
        "        nodes_df = pd.DataFrame(feature_array, index=node_index)\n",
        "        dual_hypergraph = StellarGraph(nodes=nodes_df, edges=edges)\n",
        "\n",
        "        return dual_hypergraph\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KWV98YTRwR4q"
      },
      "outputs": [],
      "source": [
        "class HyperGraph_Constructor_DHT_Cluster(HyperGraph_Constructor_DHT):\n",
        "\n",
        "    @staticmethod\n",
        "    def entrance(df, after_pca, hla_dic, dic_inventory, n_clusters=5):\n",
        "        \"\"\"\n",
        "        Custom entrance method for HyperGraph_Constructor_DHT_Cluster, which constructs dual hypergraphs\n",
        "        with edge clustering applied for graph-level classification.\n",
        "\n",
        "        Args:\n",
        "        - df: DataFrame containing HLA and peptide data.\n",
        "        - after_pca: PCA-transformed feature data for amino acids.\n",
        "        - hla_dic: Dictionary mapping HLA alleles to their pseudo sequences.\n",
        "        - dic_inventory: Dictionary containing inventory of HLA alleles.\n",
        "        - n_clusters: Number of clusters for edge clustering (default: 5).\n",
        "\n",
        "        Returns:\n",
        "        - graphs: List of dual hypergraphs with clustered edges.\n",
        "        - graph_labels: Corresponding labels for graph-level classification.\n",
        "        \"\"\"\n",
        "        graphs = []\n",
        "        graph_labels = []\n",
        "\n",
        "        # Loop over each sample in the DataFrame\n",
        "        for i in range(df.shape[0]):\n",
        "            print(f\"Processing sample {i + 1}/{df.shape[0]}\")  # Debug print to track progress\n",
        "\n",
        "            # Get the peptide and HLA information\n",
        "            pep = df['peptide'].iloc[i]\n",
        "            try:\n",
        "                hla = hla_dic[df['HLA'].iloc[i]]\n",
        "            except KeyError:\n",
        "                hla = hla_dic[rescue_unknown_hla(df['HLA'].iloc[i], dic_inventory)]\n",
        "\n",
        "            # Get the label (immunogenicity) for graph classification\n",
        "            label = df['immunogenicity'].iloc[i]\n",
        "\n",
        "            # Construct the dual hypergraph with clustering applied\n",
        "            graph = HyperGraph_Constructor_DHT_Cluster.dual_hypergraph_cluster(pep, hla, after_pca, n_clusters=n_clusters)\n",
        "\n",
        "            # Append the constructed graph and its label to the lists\n",
        "            graphs.append(graph)\n",
        "            graph_labels.append(label)\n",
        "\n",
        "        # Convert graph_labels to a pandas Series for consistency\n",
        "        graph_labels = pd.Series(graph_labels)\n",
        "\n",
        "        return graphs, graph_labels\n",
        "\n",
        "    @staticmethod\n",
        "    def generate_edge_features(edges_df, node_features):\n",
        "        \"\"\"\n",
        "        Generate edge features by combining the source and target node features.\n",
        "        This can be done by concatenating or averaging the node features of the source and target nodes.\n",
        "\n",
        "        Args:\n",
        "        - edges_df: DataFrame with source and target node identifiers.\n",
        "        - node_features: DataFrame or numpy array containing the node features.\n",
        "\n",
        "        Returns:\n",
        "        - edge_features: Features generated for each edge by combining the source and target node features.\n",
        "        \"\"\"\n",
        "        edge_features = []\n",
        "\n",
        "        # Loop over each edge to combine the source and target node features\n",
        "        for idx, row in edges_df.iterrows():\n",
        "            source_node = row['source']\n",
        "            target_node = row['target']\n",
        "\n",
        "            # Get the node features for the source and target\n",
        "            source_feature = node_features.loc[source_node].values\n",
        "            target_feature = node_features.loc[target_node].values\n",
        "\n",
        "            # Combine source and target features (e.g., concatenation, mean, etc.)\n",
        "            combined_feature = np.concatenate([source_feature, target_feature])\n",
        "\n",
        "            # Append the combined feature to the list\n",
        "            edge_features.append(combined_feature)\n",
        "\n",
        "        # Convert the list to a numpy array for clustering\n",
        "        edge_features = np.array(edge_features)\n",
        "\n",
        "        return edge_features\n",
        "\n",
        "    @staticmethod\n",
        "    def hypercluster(edges_df, edge_features, n_clusters=5):\n",
        "        \"\"\"\n",
        "        Perform clustering of edges to aggregate similar edges into clusters.\n",
        "        This reduces the graph complexity by grouping edges based on their similarity.\n",
        "        \"\"\"\n",
        "        from sklearn.cluster import KMeans\n",
        "\n",
        "        # Perform KMeans clustering on the edge features\n",
        "        kmeans = KMeans(n_clusters=n_clusters)\n",
        "        clusters = kmeans.fit_predict(edge_features)\n",
        "\n",
        "        # Add cluster labels to the edges DataFrame\n",
        "        edges_df['cluster'] = clusters\n",
        "\n",
        "        # Check if the 'weight' column exists, if not, assign a default weight\n",
        "        if 'weight' not in edges_df.columns:\n",
        "            edges_df['weight'] = 1.0  # Assign default weight\n",
        "\n",
        "        # Aggregate edges within each cluster (e.g., mean weight, or other aggregation methods)\n",
        "        clustered_edges = edges_df.groupby('cluster').agg({\n",
        "            'source': 'first',\n",
        "            'target': 'first',\n",
        "            'weight': 'mean'  # Use 'mean' as the aggregation method for weights\n",
        "        }).reset_index(drop=True)\n",
        "\n",
        "#         print(\"clustered_edges.head():\", clustered_edges.head())  # Debug output\n",
        "\n",
        "        return clustered_edges\n",
        "\n",
        "    @staticmethod\n",
        "    def dual_hypergraph_cluster(pep, hla, after_pca, n_clusters=5):\n",
        "        \"\"\"\n",
        "        Construct the dual hypergraph and apply clustering on edge representations.\n",
        "        \"\"\"\n",
        "        # Construct the dual hypergraph as before\n",
        "        dual_hypergraph = HyperGraph_Constructor_DHT.dual_hypergraph(pep, hla, after_pca)\n",
        "        edges_df = dual_hypergraph.edges()\n",
        "\n",
        "        # Ensure that edges_df is a DataFrame\n",
        "        edges_df = pd.DataFrame(edges_df, columns=['source', 'target'])\n",
        "\n",
        "        # Get the node features from the graph\n",
        "        node_features = pd.DataFrame(dual_hypergraph.node_features(), index=dual_hypergraph.nodes())\n",
        "\n",
        "        # Generate edge features by combining the source and target node features\n",
        "        edge_features = HyperGraph_Constructor_DHT_Cluster.generate_edge_features(edges_df, node_features)\n",
        "\n",
        "        # Perform clustering on the edge features\n",
        "        clustered_edges = HyperGraph_Constructor_DHT_Cluster.hypercluster(edges_df, edge_features, n_clusters=n_clusters)\n",
        "\n",
        "        # Rebuild the hypergraph with clustered edges\n",
        "        hypergraph_clustered = StellarGraph(\n",
        "            nodes=node_features,  # Correctly formatted node features\n",
        "            edges=clustered_edges\n",
        "        )\n",
        "\n",
        "        return hypergraph_clustered\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fhUCLfoewR4r"
      },
      "outputs": [],
      "source": [
        "class HyperGraph_Constructor_DHT_HyperDrop(HyperGraph_Constructor_DHT):\n",
        "\n",
        "    @staticmethod\n",
        "    def hyperdrop(edges_df, edge_features, top_k=0.5):\n",
        "        \"\"\"\n",
        "        Perform edge dropping based on the importance score of edges.\n",
        "        This will drop the bottom (1 - top_k)% of edges and keep the top_k% for classification.\n",
        "        \"\"\"\n",
        "        # Compute scores for each edge (e.g., L2 norm or any other metric)\n",
        "        edge_scores = np.linalg.norm(edge_features, axis=1)\n",
        "\n",
        "        # Sort edges by their scores in descending order and keep top_k percentage\n",
        "        num_edges_to_keep = int(top_k * len(edge_scores))\n",
        "        top_edge_indices = np.argsort(edge_scores)[-num_edges_to_keep:]\n",
        "\n",
        "        # Convert edges_df from list of tuples to DataFrame\n",
        "        edges_df = pd.DataFrame(edges_df, columns=['source', 'target'])\n",
        "\n",
        "        # Add a default 'weight' column if it doesn't exist\n",
        "        edges_df['weight'] = 1  # Assign default weight\n",
        "\n",
        "        # Keep only the top_k percentage of edges\n",
        "        edges_dropped = edges_df.iloc[top_edge_indices]\n",
        "\n",
        "        print(\"edges_dropped.head():\", edges_dropped.head())  # Check the result\n",
        "\n",
        "        return edges_dropped\n",
        "\n",
        "    @staticmethod\n",
        "    def dual_hypergraph_hyperdrop(pep, hla, after_pca, top_k=0.5):\n",
        "        \"\"\"\n",
        "        Construct the dual hypergraph and apply edge dropping (HyperDrop) on edge representations.\n",
        "        \"\"\"\n",
        "        # Construct the dual hypergraph as before\n",
        "        dual_hypergraph = HyperGraph_Constructor_DHT.dual_hypergraph(pep, hla, after_pca)\n",
        "        edges_df = dual_hypergraph.edges()\n",
        "\n",
        "        # Apply HyperDrop to drop irrelevant edges\n",
        "        edge_features = dual_hypergraph.node_features()\n",
        "        dropped_edges = HyperGraph_Constructor_DHT_HyperDrop.hyperdrop(edges_df, edge_features, top_k=top_k)\n",
        "\n",
        "        # Ensure node features are in the correct format (DataFrame or dict)\n",
        "        node_features = pd.DataFrame(dual_hypergraph.node_features(), index=dual_hypergraph.nodes())\n",
        "\n",
        "        # Ensure correct DataFrame structure\n",
        "        assert 'source' in dropped_edges.columns, \"Missing source column in edges_dropped\"\n",
        "        assert 'target' in dropped_edges.columns, \"Missing target column in edges_dropped\"\n",
        "\n",
        "        # Rebuild the hypergraph with only the top_k% edges\n",
        "        hypergraph_dropped = StellarGraph(\n",
        "            nodes=node_features,  # Correctly formatted node features\n",
        "            edges=dropped_edges\n",
        "        )\n",
        "\n",
        "        return hypergraph_dropped\n",
        "\n",
        "    @staticmethod\n",
        "    def entrance(df, after_pca, hla_dic, dic_inventory, top_k=0.5):\n",
        "        graphs = []\n",
        "        graph_labels = []\n",
        "        for i in range(df.shape[0]):\n",
        "            pep = df['peptide'].iloc[i]\n",
        "            try:\n",
        "                hla = hla_dic[df['HLA'].iloc[i]]\n",
        "            except KeyError:\n",
        "                hla = hla_dic[rescue_unknown_hla(df['HLA'].iloc[i], dic_inventory)]\n",
        "            label = df['immunogenicity'].iloc[i]\n",
        "\n",
        "            # Construct the dual hypergraph with HyperDrop applied\n",
        "            graph = HyperGraph_Constructor_DHT_HyperDrop.dual_hypergraph_hyperdrop(pep, hla, after_pca, top_k=top_k)\n",
        "\n",
        "            graphs.append(graph)\n",
        "            graph_labels.append(label)\n",
        "\n",
        "        graph_labels = pd.Series(graph_labels)\n",
        "        return graphs, graph_labels"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "seeKiegpFv4a"
      },
      "outputs": [],
      "source": [
        "def train_fold(model, train_gen, test_gen, es, epochs):\n",
        "    history = model.fit(\n",
        "        train_gen, epochs=epochs, validation_data=test_gen, verbose=2, callbacks=[es],)\n",
        "    # calculate performance on the test data and return along with history\n",
        "    test_metrics = model.evaluate(test_gen, verbose=0)\n",
        "    test_acc = test_metrics[model.metrics_names.index(\"acc\")]\n",
        "    return history, test_acc"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1umwI2WPFxbz"
      },
      "outputs": [],
      "source": [
        "# def get_generators(train_index, test_index, graph_labels, batch_size):\n",
        "#     train_gen = generator.flow(\n",
        "#         train_index, targets=graph_labels.iloc[train_index].values, batch_size=batch_size)\n",
        "#     test_gen = generator.flow(\n",
        "#         test_index, targets=graph_labels.iloc[test_index].values, batch_size=batch_size)\n",
        "\n",
        "#     return train_gen, test_gen\n",
        "\n",
        "\n",
        "def get_generators(train_index, val_index, test_index, graph_labels, batch_size):\n",
        "    train_gen = generator.flow(\n",
        "        train_index, targets=graph_labels.iloc[train_index].values, batch_size=batch_size\n",
        "    )\n",
        "    val_gen = generator.flow(\n",
        "        val_index, targets=graph_labels.iloc[val_index].values, batch_size=batch_size\n",
        "    )\n",
        "    test_gen = generator.flow(\n",
        "        test_index, targets=graph_labels.iloc[test_index].values, batch_size=batch_size\n",
        "    )\n",
        "    return train_gen, val_gen, test_gen\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MOpd2LEkFzaS"
      },
      "outputs": [],
      "source": [
        "def draw_ROC(y_true,y_pred):\n",
        "\n",
        "    fpr,tpr,_ = roc_curve(y_true,y_pred,pos_label=1)\n",
        "    area_mine = auc(fpr,tpr)\n",
        "    fig = plt.figure()\n",
        "    lw = 2\n",
        "    plt.plot(fpr, tpr, color='darkorange',\n",
        "            lw=lw, label='ROC curve (area = %0.2f)' % area_mine)\n",
        "    plt.plot([0, 1], [0, 1], color='navy', lw=lw, linestyle='--')\n",
        "    plt.xlim([0.0, 1.0])\n",
        "    plt.ylim([0.0, 1.05])\n",
        "    plt.xlabel('False Positive Rate')\n",
        "    plt.ylabel('True Positive Rate')\n",
        "    plt.title('Receiver operating characteristic example')\n",
        "    plt.legend(loc=\"lower right\")\n",
        "    plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3KqFSmalF3kw"
      },
      "outputs": [],
      "source": [
        "def draw_PR(y_true,y_pred):\n",
        "    precision,recall,_ = precision_recall_curve(y_true,y_pred,pos_label=1)\n",
        "    area_PR = auc(recall,precision)\n",
        "    baseline = np.sum(np.array(y_true) == 1) / len(y_true)\n",
        "\n",
        "    plt.figure()\n",
        "    lw = 2\n",
        "    plt.plot(recall,precision, color='darkorange',\n",
        "            lw=lw, label='PR curve (area = %0.2f)' % area_PR)\n",
        "    plt.plot([0, 1], [baseline, baseline], color='navy', lw=lw, linestyle='--')\n",
        "    plt.xlim([0.0, 1.0])\n",
        "    plt.ylim([0.0, 1.05])\n",
        "    plt.xlabel('Recall')\n",
        "    plt.ylabel('Precision')\n",
        "    plt.title('PR curve example')\n",
        "    plt.legend(loc=\"lower right\")\n",
        "    plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rtIw24_XF6PY"
      },
      "outputs": [],
      "source": [
        "def draw_history(history):\n",
        "    plt.subplot(211)\n",
        "    plt.title('Loss')\n",
        "    plt.plot(history.history['loss'], label='train')\n",
        "    plt.plot(history.history['val_loss'], label='validation')\n",
        "    plt.legend()\n",
        "    # plot accuracy during training\n",
        "    plt.subplot(212)\n",
        "    plt.title('Accuracy')\n",
        "    plt.plot(history.history['acc'], label='train')\n",
        "    plt.plot(history.history['val_acc'], label='validation')\n",
        "    plt.legend()\n",
        "    plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RdiCZWNnGjkl"
      },
      "outputs": [],
      "source": [
        "def hla_df_to_dic(hla):\n",
        "    dic = {}\n",
        "    for i in range(hla.shape[0]):\n",
        "        col1 = hla['HLA'].iloc[i]  # HLA allele\n",
        "        col2 = hla['pseudo'].iloc[i]  # pseudo sequence\n",
        "        dic[col1] = col2\n",
        "    return dic"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "p2gs8Z53Glcq"
      },
      "outputs": [],
      "source": [
        "def retain_910(ori):\n",
        "    cond = []\n",
        "    for i in range(ori.shape[0]):\n",
        "        peptide = ori['peptide'].iloc[i]\n",
        "        if len(peptide) == 9 or len(peptide) == 10:\n",
        "            cond.append(True)\n",
        "        else:\n",
        "            cond.append(False)\n",
        "    data = ori.loc[cond]\n",
        "    data = data.set_index(pd.Index(np.arange(data.shape[0])))\n",
        "    return data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mnTsnL1LGnwn"
      },
      "outputs": [],
      "source": [
        "ori_train = pd.read_csv('D:\\\\PhD\\Subgraph_Sampling_SLOT_GNN\\\\Data\\\\remove0123_sample100.csv')\n",
        "# ori_train = pd.read_csv('D:\\\\PhD\\Subgraph_Sampling_SLOT_GNN\\\\Data\\\\remove0123_sample100_Test.csv')\n",
        "hla = pd.read_csv('D:\\\\PhD\\\\GraphImmuno_NewResults\\\\data\\\\hla2paratopeTable_aligned.txt',sep='\\t')\n",
        "after_pca = np.loadtxt('D:\\\\PhD\\\\GraphImmuno_NewResults\\\\data\\\\after_pca.txt')\n",
        "\n",
        "hla_dic = hla_df_to_dic(hla)\n",
        "inventory = list(hla_dic.keys())\n",
        "dic_inventory = dict_inventory(inventory)\n",
        "\n",
        "ori_train['immunogenicity'], ori_train['potential'] = ori_train['potential'], ori_train['immunogenicity']"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DI9y8gWrGpbP",
        "scrolled": true
      },
      "outputs": [],
      "source": [
        "# Create graphs and labels using the new HyperGraph_Constructor\n",
        "#graphs, graph_labels = HyperGraph_Constructor_DHT.entrance(ori_train, after_pca, hla_dic, dic_inventory)\n",
        "graphs, graph_labels = HyperGraph_Constructor_DHT_HyperDrop.entrance(ori_train, after_pca, hla_dic, dic_inventory, top_k=0.5)\n",
        "# graphs, graph_labels = HyperGraph_Constructor_DHT_Cluster.entrance(ori_train, after_pca, hla_dic, dic_inventory, n_clusters=5)\n",
        "\n",
        "# Initialize the generator for dual hypergraphs\n",
        "generator = PaddedGraphGenerator(graphs=graphs)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LceLQbSkwR4s"
      },
      "outputs": [],
      "source": [
        "graph_labels"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WzgCYAiEwR4s"
      },
      "outputs": [],
      "source": [
        "graph_labels = pd.get_dummies(graph_labels, drop_first=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sN-Bz4dIwR4s"
      },
      "outputs": [],
      "source": [
        "ori_train1 = ori_train"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DbQhkvKEwR4s"
      },
      "outputs": [],
      "source": [
        "ori_train1.loc[ori_train1[\"potential\"] != \"Negative\", \"potential\"] = 'Positive'\n",
        "ori_train1['potential'] = ori_train1['potential'].astype('category')\n",
        "ori_train1['potential'] = pd.factorize(ori_train1['potential'])[0]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SearMSaawR4s"
      },
      "outputs": [],
      "source": [
        "graph_labels = ori_train1['potential']"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kUzyA4IOwR4s"
      },
      "outputs": [],
      "source": [
        "graph_labels.value_counts().to_frame()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yPsVjWf5wR4s"
      },
      "outputs": [],
      "source": [
        "from stellargraph.mapper import PaddedGraphGenerator\n",
        "from stellargraph.layer import GCNSupervisedGraphClassification\n",
        "from sklearn.model_selection import train_test_split\n",
        "from tensorflow.keras.models import Model\n",
        "from tensorflow.keras.layers import Dense\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "from tensorflow.keras.callbacks import EarlyStopping\n",
        "import numpy as np\n",
        "\n",
        "# Split train and test set\n",
        "# train_index, test_index = train_test_split(graph_labels.index, train_size=0.5, stratify=graph_labels)\n",
        "\n",
        "# First split into train and temp (which will be split into validation and test)\n",
        "train_index, temp_index = train_test_split(\n",
        "    graph_labels.index, train_size=0.6, stratify=graph_labels\n",
        ")\n",
        "\n",
        "# Now split temp into validation and test\n",
        "val_index, test_index = train_test_split(\n",
        "    temp_index, train_size=0.5, stratify=graph_labels.loc[temp_index]\n",
        ")\n",
        "\n",
        "# Define the PaddedGraphGenerator for the StellarGraph model\n",
        "generator = PaddedGraphGenerator(graphs=graphs)\n",
        "\n",
        "# Define the GCN model\n",
        "gc_model = GCNSupervisedGraphClassification(\n",
        "    layer_sizes=[64, 64],\n",
        "    activations=[\"relu\", \"relu\"],\n",
        "    generator=generator,\n",
        "    dropout=0.2,\n",
        ")\n",
        "\n",
        "# Build the model structure\n",
        "x_inp, x_out = gc_model.in_out_tensors()\n",
        "\n",
        "# Add Dense layers\n",
        "predictions = Dense(units=32, activation=\"relu\")(x_out)\n",
        "predictions = Dense(units=16, activation=\"relu\")(predictions)\n",
        "predictions = Dense(units=1, activation=\"sigmoid\")(predictions)  # Sigmoid for binary classification\n",
        "\n",
        "# Define the final model\n",
        "model = Model(inputs=x_inp, outputs=predictions)\n",
        "\n",
        "# Compile the model\n",
        "model.compile(optimizer=Adam(0.001), loss='binary_crossentropy', metrics=['accuracy'])\n",
        "\n",
        "# Define the training and testing generators\n",
        "# def get_generators(train_index, test_index, graph_labels, batch_size):\n",
        "#     train_gen = generator.flow(train_index, targets=graph_labels.iloc[train_index].values, batch_size=batch_size)\n",
        "#     test_gen = generator.flow(test_index, targets=graph_labels.iloc[test_index].values, batch_size=batch_size)\n",
        "#     return train_gen, test_gen\n",
        "\n",
        "def get_generators(train_index, val_index, test_index, graph_labels, batch_size):\n",
        "    train_gen = generator.flow(\n",
        "        train_index, targets=graph_labels.iloc[train_index].values, batch_size=batch_size\n",
        "    )\n",
        "    val_gen = generator.flow(\n",
        "        val_index, targets=graph_labels.iloc[val_index].values, batch_size=batch_size\n",
        "    )\n",
        "    test_gen = generator.flow(\n",
        "        test_index, targets=graph_labels.iloc[test_index].values, batch_size=batch_size\n",
        "    )\n",
        "    return train_gen, val_gen, test_gen\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GzXUSSsVwR4s"
      },
      "outputs": [],
      "source": [
        "epochs = 100\n",
        "# Get the training and testing generators\n",
        "# train_gen, test_gen = get_generators(train_index, test_index, graph_labels, batch_size=50)\n",
        "\n",
        "train_gen, val_gen, test_gen = get_generators(train_index, val_index, test_index, graph_labels, batch_size=50)\n",
        "\n",
        "# Define EarlyStopping callback\n",
        "es = EarlyStopping(\n",
        "    monitor='val_accuracy',  # Monitor validation accuracy instead of loss\n",
        "    patience=10,             # Number of epochs with no improvement before stopping\n",
        "    restore_best_weights=True\n",
        ")\n",
        "\n",
        "# Train the model with EarlyStopping\n",
        "# history = model.fit(\n",
        "#     train_gen,\n",
        "#     epochs=100,\n",
        "#     validation_data=test_gen,\n",
        "#     verbose=2,\n",
        "#     callbacks=[es],  # EarlyStopping callback\n",
        "# )\n",
        "\n",
        "# Train the model with EarlyStopping on validation data\n",
        "history = model.fit(\n",
        "    train_gen,\n",
        "    epochs=100,\n",
        "    validation_data=val_gen,\n",
        "    verbose=2,\n",
        "    callbacks=[es],  # EarlyStopping callback\n",
        ")\n",
        "\n",
        "# # Evaluate the model on the test set\n",
        "# test_metrics = model.evaluate(test_gen, verbose=0)\n",
        "\n",
        "# # Print test metrics\n",
        "# print(\"\\nTest Set Metrics:\")\n",
        "# for name, val in zip(model.metrics_names, test_metrics):\n",
        "#     print(f\"\\t{name}: {val:.4f}\")\n",
        "\n",
        "# Evaluate the model on the test set\n",
        "test_metrics = model.evaluate(test_gen, verbose=0)\n",
        "\n",
        "# Print test metrics\n",
        "print(\"\\nTest Set Metrics:\")\n",
        "for name, val in zip(model.metrics_names, test_metrics):\n",
        "    print(f\"\\t{name}: {val:.4f}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QyMiv_CTI7o5"
      },
      "outputs": [],
      "source": [
        "test_metrics = model.evaluate(test_gen)\n",
        "print(\"\\nTest Set Metrics:\")\n",
        "for name, val in zip(model.metrics_names, test_metrics):\n",
        "    print(\"\\t{}: {:0.4f}\".format(name, val))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IyNNFNeDHjvx"
      },
      "outputs": [],
      "source": [
        "!pip install seaborn"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tcu9l-UOwR4t"
      },
      "outputs": [],
      "source": [
        "from sklearn.metrics import confusion_matrix, classification_report, roc_curve, auc, precision_recall_curve, f1_score\n",
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ELhYmKjxwR4t"
      },
      "outputs": [],
      "source": [
        "from sklearn.metrics import classification_report, confusion_matrix, roc_curve, auc, precision_recall_curve\n",
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Get the true labels and predicted probabilities from the test generator\n",
        "y_true = np.concatenate([y for _, y in test_gen])  # True labels from the test generator\n",
        "y_pred_proba = model.predict(test_gen).ravel()     # Predicted probabilities from the model\n",
        "\n",
        "# Convert predicted probabilities to binary predictions (threshold 0.5)\n",
        "y_pred = (y_pred_proba > 0.5).astype(int)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "E7GPdTn7wR4t"
      },
      "outputs": [],
      "source": [
        "# Plot confusion matrix\n",
        "def plot_confusion_matrix(y_true, y_pred):\n",
        "    cm = confusion_matrix(y_true, y_pred)\n",
        "    plt.figure(figsize=(7, 5))\n",
        "    sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', xticklabels=['Negative', 'Positive'], yticklabels=['Negative', 'Positive'])\n",
        "    plt.ylabel('Actual')\n",
        "    plt.xlabel('Predicted')\n",
        "    plt.title('Confusion Matrix')\n",
        "    plt.show()\n",
        "\n",
        "# Plot ROC curve\n",
        "def plot_roc_curve(y_true, y_pred_proba):\n",
        "    fpr, tpr, _ = roc_curve(y_true, y_pred_proba)\n",
        "    roc_auc = auc(fpr, tpr)\n",
        "\n",
        "    plt.figure()\n",
        "    plt.plot(fpr, tpr, color='darkorange', lw=2, label='ROC curve (area = %0.2f)' % roc_auc)\n",
        "    plt.plot([0, 1], [0, 1], color='navy', lw=2, linestyle='--')\n",
        "    plt.xlim([0.0, 1.0])\n",
        "    plt.ylim([0.0, 1.05])\n",
        "    plt.xlabel('False Positive Rate')\n",
        "    plt.ylabel('True Positive Rate')\n",
        "    plt.title('Receiver Operating Characteristic')\n",
        "    plt.legend(loc=\"lower right\")\n",
        "    plt.show()\n",
        "\n",
        "# Plot Precision-Recall curve\n",
        "def plot_precision_recall_curve(y_true, y_pred_proba):\n",
        "    precision, recall, _ = precision_recall_curve(y_true, y_pred_proba)\n",
        "    pr_auc = auc(recall, precision)\n",
        "\n",
        "    plt.figure()\n",
        "    plt.plot(recall, precision, color='darkorange', lw=2, label='PR curve (area = %0.2f)' % pr_auc)\n",
        "    plt.xlim([0.0, 1.0])\n",
        "    plt.ylim([0.0, 1.05])\n",
        "    plt.xlabel('Recall')\n",
        "    plt.ylabel('Precision')\n",
        "    plt.title('Precision-Recall Curve')\n",
        "    plt.legend(loc=\"lower right\")\n",
        "    plt.show()\n",
        "\n",
        "# Plot training history\n",
        "def plot_metrics(history):\n",
        "    plt.figure(figsize=(14, 5))\n",
        "\n",
        "    # Loss Plot\n",
        "    plt.subplot(1, 2, 1)\n",
        "    plt.plot(history.history['loss'], label='train')\n",
        "    plt.plot(history.history['val_loss'], label='validation')\n",
        "    plt.title('Model Loss')\n",
        "    plt.xlabel('Epoch')\n",
        "    plt.ylabel('Loss')\n",
        "    plt.legend()\n",
        "\n",
        "    # Accuracy Plot\n",
        "    plt.subplot(1, 2, 2)\n",
        "    plt.plot(history.history['accuracy'], label='train')\n",
        "    plt.plot(history.history['val_accuracy'], label='validation')\n",
        "    plt.title('Model Accuracy')\n",
        "    plt.xlabel('Epoch')\n",
        "    plt.ylabel('Accuracy')\n",
        "    plt.legend()\n",
        "\n",
        "    plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "scrolled": false,
        "id": "sh0Mrb0EwR4t"
      },
      "outputs": [],
      "source": [
        "# Plot confusion matrix\n",
        "plot_confusion_matrix(y_true, y_pred)\n",
        "\n",
        "# Print classification report\n",
        "print(classification_report(y_true, y_pred))\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "scrolled": true,
        "id": "3CA2en-RwR4t"
      },
      "outputs": [],
      "source": [
        "# Plot ROC Curve\n",
        "plot_roc_curve(y_true, y_pred_proba)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "scrolled": true,
        "id": "paV4U8-HwR4w"
      },
      "outputs": [],
      "source": [
        "\n",
        "# Plot Precision-Recall Curve\n",
        "plot_precision_recall_curve(y_true, y_pred_proba)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "scrolled": true,
        "id": "49jWba_wwR4w"
      },
      "outputs": [],
      "source": [
        "\n",
        "# Plot training history (accuracy and loss)\n",
        "plot_metrics(history)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "r_EfzptiwR4x"
      },
      "outputs": [],
      "source": [
        "from sklearn.metrics import matthews_corrcoef, classification_report, confusion_matrix, roc_curve, auc, precision_recall_curve\n",
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Calculate the MCC value\n",
        "mcc = matthews_corrcoef(y_true, y_pred)\n",
        "print(f\"Matthews Correlation Coefficient (MCC): {mcc:.4f}\")\n",
        "\n",
        "# Display other classification metrics (optional)\n",
        "print(\"\\nClassification Report:\")\n",
        "print(classification_report(y_true, y_pred))\n",
        "\n",
        "# Plot confusion matrix\n",
        "def plot_confusion_matrix(y_true, y_pred):\n",
        "    cm = confusion_matrix(y_true, y_pred)\n",
        "    plt.figure(figsize=(7, 5))\n",
        "    sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', xticklabels=['Negative', 'Positive'], yticklabels=['Negative', 'Positive'])\n",
        "    plt.ylabel('Actual')\n",
        "    plt.xlabel('Predicted')\n",
        "    plt.title('Confusion Matrix')\n",
        "    plt.show()\n",
        "\n",
        "# Plot ROC curve\n",
        "def plot_roc_curve(y_true, y_pred_proba):\n",
        "    fpr, tpr, _ = roc_curve(y_true, y_pred_proba)\n",
        "    roc_auc = auc(fpr, tpr)\n",
        "\n",
        "    plt.figure()\n",
        "    plt.plot(fpr, tpr, color='darkorange', lw=2, label='ROC curve (area = %0.2f)' % roc_auc)\n",
        "    plt.plot([0, 1], [0, 1], color='navy', lw=2, linestyle='--')\n",
        "    plt.xlim([0.0, 1.0])\n",
        "    plt.ylim([0.0, 1.05])\n",
        "    plt.xlabel('False Positive Rate')\n",
        "    plt.ylabel('True Positive Rate')\n",
        "    plt.title('Receiver Operating Characteristic')\n",
        "    plt.legend(loc=\"lower right\")\n",
        "    plt.show()\n",
        "\n",
        "# Plot Precision-Recall curve\n",
        "def plot_precision_recall_curve(y_true, y_pred_proba):\n",
        "    precision, recall, _ = precision_recall_curve(y_true, y_pred_proba)\n",
        "    pr_auc = auc(recall, precision)\n",
        "\n",
        "    plt.figure()\n",
        "    plt.plot(recall, precision, color='darkorange', lw=2, label='PR curve (area = %0.2f)' % pr_auc)\n",
        "    plt.xlim([0.0, 1.0])\n",
        "    plt.ylim([0.0, 1.05])\n",
        "    plt.xlabel('Recall')\n",
        "    plt.ylabel('Precision')\n",
        "    plt.title('Precision-Recall Curve')\n",
        "    plt.legend(loc=\"lower right\")\n",
        "    plt.show()\n",
        "\n",
        "# Example usage after the classification task\n",
        "# Plot confusion matrix\n",
        "plot_confusion_matrix(y_true, y_pred)\n",
        "\n",
        "# Print classification report and MCC value\n",
        "print(classification_report(y_true, y_pred))\n",
        "print(f\"\\nMatthews Correlation Coefficient (MCC): {mcc:.4f}\")\n",
        "\n",
        "# Plot ROC Curve\n",
        "plot_roc_curve(y_true, y_pred_proba)\n",
        "\n",
        "# Plot Precision-Recall curve\n",
        "plot_precision_recall_curve(y_true, y_pred_proba)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1WG7bWtOwR4x"
      },
      "outputs": [],
      "source": [
        "print(f\"\\nMatthews Correlation Coefficient (MCC): {mcc:.4f}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uBKSejPawR4x"
      },
      "outputs": [],
      "source": [
        "from sklearn.cluster import KMeans\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from sklearn.decomposition import PCA\n",
        "\n",
        "class HyperGraphClustering:\n",
        "    def __init__(self, n_clusters=5):\n",
        "        self.n_clusters = n_clusters\n",
        "\n",
        "    def apply_kmeans(self, node_features):\n",
        "        \"\"\"\n",
        "        Apply KMeans clustering on node features.\n",
        "\n",
        "        Args:\n",
        "        - node_features: The features of the graph nodes.\n",
        "\n",
        "        Returns:\n",
        "        - clusters: An array of cluster labels assigned to each node.\n",
        "        \"\"\"\n",
        "        kmeans = KMeans(n_clusters=self.n_clusters)\n",
        "        clusters = kmeans.fit_predict(node_features)\n",
        "        return clusters\n",
        "\n",
        "    def analyze_clusters(self, clusters):\n",
        "        \"\"\"\n",
        "        Analyze the clusters by showing the number of nodes in each cluster.\n",
        "\n",
        "        Args:\n",
        "        - clusters: Cluster labels assigned to each node.\n",
        "        \"\"\"\n",
        "        unique, counts = np.unique(clusters, return_counts=True)\n",
        "        cluster_info = dict(zip(unique, counts))\n",
        "        print(\"Cluster Distribution:\")\n",
        "        for cluster_id, size in cluster_info.items():\n",
        "            print(f\"Cluster {cluster_id}: {size} nodes\")\n",
        "\n",
        "    def visualize_clusters(self, graph, clusters):\n",
        "        \"\"\"\n",
        "        Visualize the clusters in a 2D space (using PCA for dimensionality reduction).\n",
        "\n",
        "        Args:\n",
        "        - graph: The input hypergraph (StellarGraph object).\n",
        "        - clusters: Cluster labels assigned to each node.\n",
        "        \"\"\"\n",
        "        node_features = graph.node_features()\n",
        "        pca = PCA(n_components=2)\n",
        "        reduced_features = pca.fit_transform(node_features)\n",
        "\n",
        "        plt.figure(figsize=(10, 8))\n",
        "        sns.scatterplot(\n",
        "            x=reduced_features[:, 0],\n",
        "            y=reduced_features[:, 1],\n",
        "            hue=clusters,\n",
        "            palette=\"viridis\",\n",
        "            s=100,\n",
        "            alpha=0.8\n",
        "        )\n",
        "        plt.title(\"Graph Clustering Visualization (PCA)\")\n",
        "        plt.show()\n",
        "\n",
        "# Example usage with your hypergraph:\n",
        "\n",
        "# Initialize the clustering model\n",
        "n_clusters = 5\n",
        "graph_clustering = HyperGraphClustering(n_clusters=n_clusters)\n",
        "\n",
        "# Get node features from the graph\n",
        "node_features = [graph.node_features() for graph in graphs]  # Assuming a list of graphs\n",
        "\n",
        "# Flatten node features for all graphs\n",
        "node_features = np.vstack(node_features)\n",
        "\n",
        "# Step 2: Apply KMeans Clustering on Node Features\n",
        "clusters = graph_clustering.apply_kmeans(node_features)\n",
        "\n",
        "# Step 3: Analyze the Clusters\n",
        "graph_clustering.analyze_clusters(clusters)\n",
        "\n",
        "# Step 4: Visualize the Clusters (optional, for a single graph)\n",
        "# You can pass one of the graphs and the cluster labels\n",
        "graph_clustering.visualize_clusters(graphs[0], clusters[:len(graphs[0].nodes())])\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "scrolled": true,
        "id": "8vcO6M5HwR4x"
      },
      "outputs": [],
      "source": [
        "# Create the hypergraphs using the existing workflow\n",
        "# graphs, graph_labels = HyperGraph_Constructor_DHT_Cluster.entrance(ori_train, after_pca, hla_dic, dic_inventory, n_clusters=5)\n",
        "\n",
        "# Initialize the clustering class\n",
        "graph_clustering = HyperGraphClustering(n_clusters=5)\n",
        "\n",
        "# Get node features for all graphs (assuming each graph has node features)\n",
        "node_features = [graph.node_features() for graph in graphs]\n",
        "\n",
        "# Flatten node features for all graphs\n",
        "node_features = np.vstack(node_features)\n",
        "\n",
        "# Apply KMeans clustering to the node features\n",
        "clusters = graph_clustering.apply_kmeans(node_features)\n",
        "\n",
        "# Analyze the clusters (print number of nodes in each cluster)\n",
        "graph_clustering.analyze_clusters(clusters)\n",
        "\n",
        "# Visualize the clusters for a single graph (optional)\n",
        "graph_clustering.visualize_clusters(graphs[0], clusters[:len(graphs[0].nodes())])\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "edbmQpHYwR4x"
      },
      "outputs": [],
      "source": [
        "# Validation on external dataset Dengue"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bdqNSgHjwR4x"
      },
      "outputs": [],
      "source": [
        "# ori_train = pd.read_csv('D:\\\\PhD\\Subgraph_Sampling_SLOT_GNN\\\\Data\\\\remove0123_sample100.csv')\n",
        "ori_train_Dengue = pd.read_csv('D:\\\\PhD\\Subgraph_Sampling_SLOT_GNN\\\\Data\\\\dengue_test.csv')\n",
        "ori_train_Dengue_bk = ori_train_Dengue\n",
        "hla = pd.read_csv('D:\\\\PhD\\\\GraphImmuno_NewResults\\\\data\\\\hla2paratopeTable_aligned.txt',sep='\\t')\n",
        "after_pca = np.loadtxt('D:\\\\PhD\\\\GraphImmuno_NewResults\\\\data\\\\after_pca.txt')\n",
        "\n",
        "hla_dic = hla_df_to_dic(hla)\n",
        "inventory = list(hla_dic.keys())\n",
        "dic_inventory = dict_inventory(inventory)\n",
        "\n",
        "# ori_train['immunogenicity'], ori_train['potential'] = ori_train_external['potential'], ori_train_external['immunogenicity']"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wHNP2U68wR4x"
      },
      "outputs": [],
      "source": [
        "# Create graphs and labels using the new HyperGraph_Constructor\n",
        "#graphs, graph_labels = HyperGraph_Constructor_DHT.entrance(ori_train, after_pca, hla_dic, dic_inventory)\n",
        "#graphs, graph_labels = HyperGraph_Constructor_DHT_HyperDrop.entrance(ori_train, after_pca, hla_dic, dic_inventory, top_k=0.5)\n",
        "external_graphs, external_graph_labels = HyperGraph_Constructor_DHT_Cluster.entrance(ori_train_Dengue, after_pca, hla_dic, dic_inventory, n_clusters=5)\n",
        "\n",
        "# Initialize the generator for dual hypergraphs\n",
        "generator = PaddedGraphGenerator(graphs=graphs)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ylOF21iLwR4x"
      },
      "outputs": [],
      "source": [
        "# graph_labels = pd.get_dummies(graph_labels, drop_first=True)\n",
        "# ori_train1 = ori_train\n",
        "# ori_train1.loc[ori_train1[\"potential\"] != \"Negative\", \"potential\"] = 'Positive'\n",
        "# ori_train1['potential'] = ori_train1['potential'].astype('category')\n",
        "# ori_train1['potential'] = pd.factorize(ori_train1['potential'])[0]\n",
        "graph_labels = ori_train_Dengue['immunogenicity']\n",
        "graph_labels.value_counts().to_frame()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LROGYDoJwR4x"
      },
      "outputs": [],
      "source": [
        "len(external_graph_labels)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "djYdbcJvwR4x"
      },
      "outputs": [],
      "source": [
        "# ori_train_Dengue"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IFtrbjZ1wR4y"
      },
      "outputs": [],
      "source": [
        "# ori_train_Dengue_bk['immunogenicity']"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "U7N7LDvTwR4y"
      },
      "outputs": [],
      "source": [
        "assert len(external_graphs) == len(external_graph_labels), \"Mismatch in number of graphs and labels.\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MUPbWNR0wR4y"
      },
      "outputs": [],
      "source": [
        "# Step 1: Load external datasets\n",
        "from stellargraph import StellarGraph\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "# Ensure data integrity\n",
        "assert len(external_graphs) == len(external_graph_labels), \"Mismatch in graphs and labels.\"\n",
        "\n",
        "# Step 2: Preprocess external graphs (if necessary)\n",
        "node_feature_size = graphs[0].node_features().shape[1]\n",
        "\n",
        "def preprocess_graph(graph):\n",
        "    features = graph.node_features()\n",
        "    if features.shape[1] != node_feature_size:\n",
        "        padded_features = np.zeros((features.shape[0], node_feature_size))\n",
        "        padded_features[:, :features.shape[1]] = features\n",
        "        graph = StellarGraph(graph.to_networkx(), node_features=padded_features)\n",
        "    return graph\n",
        "\n",
        "external_graphs = [preprocess_graph(g) for g in external_graphs]\n",
        "\n",
        "# Step 3: Create data generators\n",
        "from stellargraph.mapper import PaddedGraphGenerator\n",
        "\n",
        "external_generator = PaddedGraphGenerator(graphs=external_graphs)\n",
        "external_index = np.arange(len(external_graphs))\n",
        "\n",
        "external_gen = external_generator.flow(\n",
        "    external_index,\n",
        "    targets=external_graph_labels.values,\n",
        "    batch_size=50,\n",
        "    symmetric_normalization=False,\n",
        ")\n",
        "\n",
        "# Step 4: Predict on external data\n",
        "external_predictions_proba = model.predict(external_gen).ravel()\n",
        "\n",
        "# Adjust the threshold based on precision-recall curve\n",
        "from sklearn.metrics import precision_recall_curve\n",
        "\n",
        "precision, recall, thresholds = precision_recall_curve(external_graph_labels.values, external_predictions_proba)\n",
        "f1_scores = 2 * (precision * recall) / (precision + recall)\n",
        "optimal_idx = np.argmax(f1_scores)\n",
        "optimal_threshold = thresholds[optimal_idx]\n",
        "print(f\"Optimal Threshold: {optimal_threshold:.4f}\")\n",
        "\n",
        "# Apply the optimal threshold\n",
        "external_predictions = (external_predictions_proba > optimal_threshold).astype(int)\n",
        "\n",
        "# Step 5: Compute and display metrics\n",
        "# Step 5: Compute and display metrics\n",
        "from sklearn.metrics import accuracy_score, matthews_corrcoef, roc_auc_score, average_precision_score\n",
        "\n",
        "external_y_true = external_graph_labels.values\n",
        "\n",
        "# Check if more than one class is present\n",
        "unique_classes = np.unique(external_y_true)\n",
        "if len(unique_classes) > 1:\n",
        "    # Compute ROC AUC and PR AUC\n",
        "    external_roc_auc = roc_auc_score(external_y_true, external_predictions_proba)\n",
        "    external_pr_auc = average_precision_score(external_y_true, external_predictions_proba)\n",
        "else:\n",
        "    external_roc_auc = None\n",
        "    external_pr_auc = None\n",
        "    print(\"Cannot compute ROC AUC or PR AUC - only one class present in y_true.\")\n",
        "\n",
        "# Compute other metrics\n",
        "external_accuracy = accuracy_score(external_y_true, external_predictions)\n",
        "\n",
        "# Handle MCC similarly\n",
        "if len(unique_classes) > 1:\n",
        "    external_mcc = matthews_corrcoef(external_y_true, external_predictions)\n",
        "else:\n",
        "    external_mcc = None\n",
        "    print(\"Cannot compute Matthews Correlation Coefficient (MCC) - only one class present in y_true.\")\n",
        "\n",
        "print(f\"Accuracy on External Data: {external_accuracy:.4f}\")\n",
        "if external_mcc is not None:\n",
        "    print(f\"Matthews Correlation Coefficient (MCC): {external_mcc:.4f}\")\n",
        "if external_roc_auc is not None:\n",
        "    print(f\"ROC AUC Score: {external_roc_auc:.4f}\")\n",
        "if external_pr_auc is not None:\n",
        "    print(f\"Precision-Recall AUC: {external_pr_auc:.4f}\")\n",
        "\n",
        "# Classification report\n",
        "print(\"\\nClassification Report on External Data:\")\n",
        "print(classification_report(external_y_true, external_predictions, zero_division=0))\n",
        "\n",
        "# Proceed with confusion matrix and other available metrics\n",
        "def plot_confusion_matrix(y_true, y_pred):\n",
        "    cm = confusion_matrix(y_true, y_pred)\n",
        "    plt.figure(figsize=(5, 4))\n",
        "    sns.heatmap(\n",
        "        cm,\n",
        "        annot=True,\n",
        "        fmt='d',\n",
        "        cmap='Blues',\n",
        "        xticklabels=['Predicted Class 1'],\n",
        "        yticklabels=['Actual Class 1']\n",
        "    )\n",
        "    plt.ylabel('Actual')\n",
        "    plt.xlabel('Predicted')\n",
        "    plt.title('Confusion Matrix on External Data')\n",
        "    plt.show()\n",
        "\n",
        "# Plot the confusion matrix\n",
        "plot_confusion_matrix(external_y_true, external_predictions)\n",
        "\n",
        "\n",
        "# Plot ROC Curve\n",
        "def plot_roc_curve(y_true, y_proba):\n",
        "    from sklearn.metrics import roc_curve, auc\n",
        "    fpr, tpr, _ = roc_curve(y_true, y_proba)\n",
        "    roc_auc = auc(fpr, tpr)\n",
        "\n",
        "    plt.figure()\n",
        "    plt.plot(fpr, tpr, color='darkorange', lw=2, label=f'ROC curve (AUC = {roc_auc:.2f})')\n",
        "    plt.plot([0, 1], [0, 1], color='navy', lw=2, linestyle='--')\n",
        "    plt.xlim([-0.05, 1.0])\n",
        "    plt.ylim([0.0, 1.05])\n",
        "    plt.xlabel('False Positive Rate')\n",
        "    plt.ylabel('True Positive Rate')\n",
        "    plt.title('ROC Curve on External Data')\n",
        "    plt.legend(loc='lower right')\n",
        "    plt.show()\n",
        "\n",
        "plot_roc_curve(external_y_true, external_predictions_proba)\n",
        "\n",
        "# Plot Precision-Recall Curve\n",
        "def plot_precision_recall_curve(y_true, y_proba):\n",
        "    from sklearn.metrics import precision_recall_curve, auc\n",
        "    precision, recall, _ = precision_recall_curve(y_true, y_proba)\n",
        "    pr_auc = auc(recall, precision)\n",
        "\n",
        "    plt.figure()\n",
        "    plt.plot(recall, precision, color='blue', lw=2, label=f'PR curve (AUC = {pr_auc:.2f})')\n",
        "    plt.xlabel('Recall')\n",
        "    plt.ylabel('Precision')\n",
        "    plt.title('Precision-Recall Curve on External Data')\n",
        "    plt.legend(loc='lower left')\n",
        "    plt.show()\n",
        "\n",
        "plot_precision_recall_curve(external_y_true, external_predictions_proba)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6BfKjo2ywR4y"
      },
      "outputs": [],
      "source": [
        "#Cell data check"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IIwVmIiOwR4y"
      },
      "outputs": [],
      "source": [
        "\n",
        "# ori_train = pd.read_csv('D:\\\\PhD\\Subgraph_Sampling_SLOT_GNN\\\\Data\\\\remove0123_sample100.csv')\n",
        "ori_train_Dengue = pd.read_csv('D:\\\\PhD\\Subgraph_Sampling_SLOT_GNN\\\\Data\\\\deephlapan_result_cell.csv')\n",
        "ori_train_Dengue_bk = ori_train_Dengue\n",
        "hla = pd.read_csv('D:\\\\PhD\\\\GraphImmuno_NewResults\\\\data\\\\hla2paratopeTable_aligned.txt',sep='\\t')\n",
        "after_pca = np.loadtxt('D:\\\\PhD\\\\GraphImmuno_NewResults\\\\data\\\\after_pca.txt')\n",
        "\n",
        "hla_dic = hla_df_to_dic(hla)\n",
        "inventory = list(hla_dic.keys())\n",
        "dic_inventory = dict_inventory(inventory)\n",
        "\n",
        "# ori_train['immunogenicity'], ori_train['potential'] = ori_train_external['potential'], ori_train_external['immunogenicity']\n",
        "\n",
        "# Create graphs and labels using the new HyperGraph_Constructor\n",
        "#graphs, graph_labels = HyperGraph_Constructor_DHT.entrance(ori_train, after_pca, hla_dic, dic_inventory)\n",
        "#graphs, graph_labels = HyperGraph_Constructor_DHT_HyperDrop.entrance(ori_train, after_pca, hla_dic, dic_inventory, top_k=0.5)\n",
        "external_graphs, external_graph_labels = HyperGraph_Constructor_DHT_Cluster.entrance(ori_train_Dengue, after_pca, hla_dic, dic_inventory, n_clusters=5)\n",
        "\n",
        "# Initialize the generator for dual hypergraphs\n",
        "generator = PaddedGraphGenerator(graphs=graphs)\n",
        "\n",
        "count_graph_labels = ori_train_Dengue['immunogenicity']\n",
        "count_graph_labels.value_counts().to_frame()\n",
        "\n",
        "assert len(external_graphs) == len(external_graph_labels), \"Mismatch in number of graphs and labels.\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cQK-fGd0wR4y"
      },
      "outputs": [],
      "source": [
        "# Step 1: Load external datasets\n",
        "from stellargraph import StellarGraph\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "# Ensure data integrity\n",
        "assert len(external_graphs) == len(external_graph_labels), \"Mismatch in graphs and labels.\"\n",
        "\n",
        "# Step 2: Preprocess external graphs (if necessary)\n",
        "node_feature_size = graphs[0].node_features().shape[1]\n",
        "\n",
        "def preprocess_graph(graph):\n",
        "    features = graph.node_features()\n",
        "    if features.shape[1] != node_feature_size:\n",
        "        padded_features = np.zeros((features.shape[0], node_feature_size))\n",
        "        padded_features[:, :features.shape[1]] = features\n",
        "        graph = StellarGraph(graph.to_networkx(), node_features=padded_features)\n",
        "    return graph\n",
        "\n",
        "external_graphs = [preprocess_graph(g) for g in external_graphs]\n",
        "\n",
        "# Step 3: Create data generators\n",
        "from stellargraph.mapper import PaddedGraphGenerator\n",
        "\n",
        "external_generator = PaddedGraphGenerator(graphs=external_graphs)\n",
        "external_index = np.arange(len(external_graphs))\n",
        "\n",
        "external_gen = external_generator.flow(\n",
        "    external_index,\n",
        "    targets=external_graph_labels.values,\n",
        "    batch_size=50,\n",
        "    symmetric_normalization=False,\n",
        ")\n",
        "\n",
        "# Step 4: Predict on external data\n",
        "external_predictions_proba = model.predict(external_gen).ravel()\n",
        "\n",
        "# Adjust the threshold based on precision-recall curve\n",
        "from sklearn.metrics import precision_recall_curve\n",
        "\n",
        "precision, recall, thresholds = precision_recall_curve(external_graph_labels.values, external_predictions_proba)\n",
        "f1_scores = 2 * (precision * recall) / (precision + recall)\n",
        "optimal_idx = np.argmax(f1_scores)\n",
        "optimal_threshold = thresholds[optimal_idx]\n",
        "print(f\"Optimal Threshold: {optimal_threshold:.4f}\")\n",
        "\n",
        "# Apply the optimal threshold\n",
        "external_predictions = (external_predictions_proba > optimal_threshold).astype(int)\n",
        "\n",
        "# Step 5: Compute and display metrics\n",
        "from sklearn.metrics import accuracy_score, matthews_corrcoef, roc_auc_score, average_precision_score\n",
        "\n",
        "external_y_true = external_graph_labels.values\n",
        "external_accuracy = accuracy_score(external_y_true, external_predictions)\n",
        "external_mcc = matthews_corrcoef(external_y_true, external_predictions)\n",
        "external_roc_auc = roc_auc_score(external_y_true, external_predictions_proba)\n",
        "external_pr_auc = average_precision_score(external_y_true, external_predictions_proba)\n",
        "\n",
        "print(f\"Accuracy on External Data: {external_accuracy:.4f}\")\n",
        "print(f\"Matthews Correlation Coefficient (MCC): {external_mcc:.4f}\")\n",
        "print(f\"ROC AUC Score: {external_roc_auc:.4f}\")\n",
        "print(f\"Precision-Recall AUC: {external_pr_auc:.4f}\")\n",
        "\n",
        "# Step 6: Additional metrics and visualization\n",
        "from sklearn.metrics import classification_report, confusion_matrix\n",
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "print(\"\\nClassification Report on External Data:\")\n",
        "print(classification_report(external_y_true, external_predictions))\n",
        "\n",
        "def plot_confusion_matrix(y_true, y_pred):\n",
        "    cm = confusion_matrix(y_true, y_pred)\n",
        "    plt.figure(figsize=(7, 5))\n",
        "    sns.heatmap(\n",
        "        cm,\n",
        "        annot=True,\n",
        "        fmt='d',\n",
        "        cmap='Blues',\n",
        "        xticklabels=['Negative', 'Positive'],\n",
        "        yticklabels=['Negative', 'Positive']\n",
        "    )\n",
        "    plt.ylabel('Actual')\n",
        "    plt.xlabel('Predicted')\n",
        "    plt.title('Confusion Matrix on External Data')\n",
        "    plt.show()\n",
        "\n",
        "# Plot the confusion matrix\n",
        "plot_confusion_matrix(external_y_true, external_predictions)\n",
        "\n",
        "# Plot ROC Curve\n",
        "def plot_roc_curve(y_true, y_proba):\n",
        "    from sklearn.metrics import roc_curve, auc\n",
        "    fpr, tpr, _ = roc_curve(y_true, y_proba)\n",
        "    roc_auc = auc(fpr, tpr)\n",
        "\n",
        "    plt.figure()\n",
        "    plt.plot(fpr, tpr, color='darkorange', lw=2, label=f'ROC curve (AUC = {roc_auc:.2f})')\n",
        "    plt.plot([0, 1], [0, 1], color='navy', lw=2, linestyle='--')\n",
        "    plt.xlim([-0.05, 1.0])\n",
        "    plt.ylim([0.0, 1.05])\n",
        "    plt.xlabel('False Positive Rate')\n",
        "    plt.ylabel('True Positive Rate')\n",
        "    plt.title('ROC Curve on External Data')\n",
        "    plt.legend(loc='lower right')\n",
        "    plt.show()\n",
        "\n",
        "plot_roc_curve(external_y_true, external_predictions_proba)\n",
        "\n",
        "# Plot Precision-Recall Curve\n",
        "def plot_precision_recall_curve(y_true, y_proba):\n",
        "    from sklearn.metrics import precision_recall_curve, auc\n",
        "    precision, recall, _ = precision_recall_curve(y_true, y_proba)\n",
        "    pr_auc = auc(recall, precision)\n",
        "\n",
        "    plt.figure()\n",
        "    plt.plot(recall, precision, color='blue', lw=2, label=f'PR curve (AUC = {pr_auc:.2f})')\n",
        "    plt.xlabel('Recall')\n",
        "    plt.ylabel('Precision')\n",
        "    plt.title('Precision-Recall Curve on External Data')\n",
        "    plt.legend(loc='lower left')\n",
        "    plt.show()\n",
        "\n",
        "plot_precision_recall_curve(external_y_true, external_predictions_proba)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uCxYMQEowR4y"
      },
      "outputs": [],
      "source": [
        "# Covid convalescent"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "S5NXzSi5wR4y"
      },
      "outputs": [],
      "source": [
        "\n",
        "# ori_train = pd.read_csv('D:\\\\PhD\\Subgraph_Sampling_SLOT_GNN\\\\Data\\\\remove0123_sample100.csv')\n",
        "ori_train_Dengue = pd.read_csv('D:\\\\PhD\\Subgraph_Sampling_SLOT_GNN\\\\Data\\\\sars_cov_2_result.csv')\n",
        "ori_train_Dengue_bk = ori_train_Dengue\n",
        "hla = pd.read_csv('D:\\\\PhD\\\\GraphImmuno_NewResults\\\\data\\\\hla2paratopeTable_aligned.txt',sep='\\t')\n",
        "after_pca = np.loadtxt('D:\\\\PhD\\\\GraphImmuno_NewResults\\\\data\\\\after_pca.txt')\n",
        "\n",
        "hla_dic = hla_df_to_dic(hla)\n",
        "inventory = list(hla_dic.keys())\n",
        "dic_inventory = dict_inventory(inventory)\n",
        "\n",
        "ori_train_Dengue['immunogenicity'] = ori_train_Dengue['immunogenicity-con']\n",
        "\n",
        "# ori_train['immunogenicity'], ori_train['potential'] = ori_train_external['potential'], ori_train_external['immunogenicity']\n",
        "\n",
        "# Create graphs and labels using the new HyperGraph_Constructor\n",
        "#graphs, graph_labels = HyperGraph_Constructor_DHT.entrance(ori_train, after_pca, hla_dic, dic_inventory)\n",
        "#graphs, graph_labels = HyperGraph_Constructor_DHT_HyperDrop.entrance(ori_train, after_pca, hla_dic, dic_inventory, top_k=0.5)\n",
        "external_graphs, external_graph_labels = HyperGraph_Constructor_DHT_Cluster.entrance(ori_train_Dengue, after_pca, hla_dic, dic_inventory, n_clusters=5)\n",
        "\n",
        "# Initialize the generator for dual hypergraphs\n",
        "generator = PaddedGraphGenerator(graphs=graphs)\n",
        "\n",
        "count_graph_labels = ori_train_Dengue['immunogenicity']\n",
        "count_graph_labels.value_counts().to_frame()\n",
        "\n",
        "assert len(external_graphs) == len(external_graph_labels), \"Mismatch in number of graphs and labels.\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "L9pUsK2XwR4y"
      },
      "outputs": [],
      "source": [
        "# Step 1: Load external datasets\n",
        "from stellargraph import StellarGraph\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "# Ensure data integrity\n",
        "assert len(external_graphs) == len(external_graph_labels), \"Mismatch in graphs and labels.\"\n",
        "\n",
        "# Step 2: Preprocess external graphs (if necessary)\n",
        "# Example preprocessing (ensure node features have the correct size)\n",
        "node_feature_size = graphs[0].node_features().shape[1]\n",
        "\n",
        "def preprocess_graph(graph):\n",
        "    features = graph.node_features()\n",
        "    if features.shape[1] != node_feature_size:\n",
        "        padded_features = np.zeros((features.shape[0], node_feature_size))\n",
        "        padded_features[:, :features.shape[1]] = features\n",
        "        graph = StellarGraph(graph.to_networkx(), node_features=padded_features)\n",
        "    return graph\n",
        "\n",
        "external_graphs = [preprocess_graph(g) for g in external_graphs]\n",
        "\n",
        "# Step 3: Create data generators\n",
        "from stellargraph.mapper import PaddedGraphGenerator\n",
        "\n",
        "external_generator = PaddedGraphGenerator(graphs=external_graphs)\n",
        "external_index = np.arange(len(external_graphs))\n",
        "\n",
        "external_gen = external_generator.flow(\n",
        "    external_index,\n",
        "    targets=external_graph_labels.values,\n",
        "    batch_size=50,  # Adjust batch size if needed\n",
        "    symmetric_normalization=False,\n",
        ")\n",
        "\n",
        "# Step 4: Predict on external data\n",
        "external_predictions_proba = model.predict(external_gen).ravel()\n",
        "external_predictions = (external_predictions_proba > 0.5).astype(int)\n",
        "\n",
        "# Step 5: Compute and display metrics\n",
        "from sklearn.metrics import accuracy_score, matthews_corrcoef\n",
        "\n",
        "external_y_true = external_graph_labels.values\n",
        "external_accuracy = accuracy_score(external_y_true, external_predictions)\n",
        "external_mcc = matthews_corrcoef(external_y_true, external_predictions)\n",
        "\n",
        "print(f\"Accuracy on External Data: {external_accuracy:.4f}\")\n",
        "print(f\"Matthews Correlation Coefficient (MCC) on External Data: {external_mcc:.4f}\")\n",
        "\n",
        "# Step 6: Additional metrics and visualization\n",
        "from sklearn.metrics import classification_report, confusion_matrix\n",
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "print(\"\\nClassification Report on External Data:\")\n",
        "print(classification_report(external_y_true, external_predictions))\n",
        "\n",
        "def plot_confusion_matrix(y_true, y_pred):\n",
        "    cm = confusion_matrix(y_true, y_pred)\n",
        "    plt.figure(figsize=(7, 5))\n",
        "    sns.heatmap(\n",
        "        cm,\n",
        "        annot=True,\n",
        "        fmt='d',\n",
        "        cmap='Blues',\n",
        "        xticklabels=['Negative', 'Positive'],\n",
        "        yticklabels=['Negative', 'Positive']\n",
        "    )\n",
        "    plt.ylabel('Actual')\n",
        "    plt.xlabel('Predicted')\n",
        "    plt.title('Confusion Matrix on External Data')\n",
        "    plt.show()\n",
        "\n",
        "# Plot the confusion matrix\n",
        "plot_confusion_matrix(external_y_true, external_predictions)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6fXOJsidwR4z"
      },
      "outputs": [],
      "source": [
        "# Step 1: Load external datasets\n",
        "from stellargraph import StellarGraph\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "# Ensure data integrity\n",
        "assert len(external_graphs) == len(external_graph_labels), \"Mismatch in graphs and labels.\"\n",
        "\n",
        "# Step 2: Preprocess external graphs (if necessary)\n",
        "node_feature_size = graphs[0].node_features().shape[1]\n",
        "\n",
        "def preprocess_graph(graph):\n",
        "    features = graph.node_features()\n",
        "    if features.shape[1] != node_feature_size:\n",
        "        padded_features = np.zeros((features.shape[0], node_feature_size))\n",
        "        padded_features[:, :features.shape[1]] = features\n",
        "        graph = StellarGraph(graph.to_networkx(), node_features=padded_features)\n",
        "    return graph\n",
        "\n",
        "external_graphs = [preprocess_graph(g) for g in external_graphs]\n",
        "\n",
        "# Step 3: Create data generators\n",
        "from stellargraph.mapper import PaddedGraphGenerator\n",
        "\n",
        "external_generator = PaddedGraphGenerator(graphs=external_graphs)\n",
        "external_index = np.arange(len(external_graphs))\n",
        "\n",
        "external_gen = external_generator.flow(\n",
        "    external_index,\n",
        "    targets=external_graph_labels.values,\n",
        "    batch_size=50,\n",
        "    symmetric_normalization=False,\n",
        ")\n",
        "\n",
        "# Step 4: Predict on external data\n",
        "external_predictions_proba = model.predict(external_gen).ravel()\n",
        "\n",
        "# Adjust the threshold based on precision-recall curve\n",
        "from sklearn.metrics import precision_recall_curve\n",
        "\n",
        "precision, recall, thresholds = precision_recall_curve(external_graph_labels.values, external_predictions_proba)\n",
        "f1_scores = 2 * (precision * recall) / (precision + recall)\n",
        "optimal_idx = np.argmax(f1_scores)\n",
        "optimal_threshold = thresholds[optimal_idx]\n",
        "print(f\"Optimal Threshold: {optimal_threshold:.4f}\")\n",
        "\n",
        "# Apply the optimal threshold\n",
        "external_predictions = (external_predictions_proba > optimal_threshold).astype(int)\n",
        "\n",
        "# Step 5: Compute and display metrics\n",
        "from sklearn.metrics import accuracy_score, matthews_corrcoef, roc_auc_score, average_precision_score\n",
        "\n",
        "external_y_true = external_graph_labels.values\n",
        "external_accuracy = accuracy_score(external_y_true, external_predictions)\n",
        "external_mcc = matthews_corrcoef(external_y_true, external_predictions)\n",
        "external_roc_auc = roc_auc_score(external_y_true, external_predictions_proba)\n",
        "external_pr_auc = average_precision_score(external_y_true, external_predictions_proba)\n",
        "\n",
        "print(f\"Accuracy on External Data: {external_accuracy:.4f}\")\n",
        "print(f\"Matthews Correlation Coefficient (MCC): {external_mcc:.4f}\")\n",
        "print(f\"ROC AUC Score: {external_roc_auc:.4f}\")\n",
        "print(f\"Precision-Recall AUC: {external_pr_auc:.4f}\")\n",
        "\n",
        "# Step 6: Additional metrics and visualization\n",
        "from sklearn.metrics import classification_report, confusion_matrix\n",
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "print(\"\\nClassification Report on External Data:\")\n",
        "print(classification_report(external_y_true, external_predictions))\n",
        "\n",
        "def plot_confusion_matrix(y_true, y_pred):\n",
        "    cm = confusion_matrix(y_true, y_pred)\n",
        "    plt.figure(figsize=(7, 5))\n",
        "    sns.heatmap(\n",
        "        cm,\n",
        "        annot=True,\n",
        "        fmt='d',\n",
        "        cmap='Blues',\n",
        "        xticklabels=['Negative', 'Positive'],\n",
        "        yticklabels=['Negative', 'Positive']\n",
        "    )\n",
        "    plt.ylabel('Actual')\n",
        "    plt.xlabel('Predicted')\n",
        "    plt.title('Confusion Matrix on External Data')\n",
        "    plt.show()\n",
        "\n",
        "# Plot the confusion matrix\n",
        "plot_confusion_matrix(external_y_true, external_predictions)\n",
        "\n",
        "# Plot ROC Curve\n",
        "def plot_roc_curve(y_true, y_proba):\n",
        "    from sklearn.metrics import roc_curve, auc\n",
        "    fpr, tpr, _ = roc_curve(y_true, y_proba)\n",
        "    roc_auc = auc(fpr, tpr)\n",
        "\n",
        "    plt.figure()\n",
        "    plt.plot(fpr, tpr, color='darkorange', lw=2, label=f'ROC curve (AUC = {roc_auc:.2f})')\n",
        "    plt.plot([0, 1], [0, 1], color='navy', lw=2, linestyle='--')\n",
        "    plt.xlim([-0.05, 1.0])\n",
        "    plt.ylim([0.0, 1.05])\n",
        "    plt.xlabel('False Positive Rate')\n",
        "    plt.ylabel('True Positive Rate')\n",
        "    plt.title('ROC Curve on External Data')\n",
        "    plt.legend(loc='lower right')\n",
        "    plt.show()\n",
        "\n",
        "plot_roc_curve(external_y_true, external_predictions_proba)\n",
        "\n",
        "# Plot Precision-Recall Curve\n",
        "def plot_precision_recall_curve(y_true, y_proba):\n",
        "    from sklearn.metrics import precision_recall_curve, auc\n",
        "    precision, recall, _ = precision_recall_curve(y_true, y_proba)\n",
        "    pr_auc = auc(recall, precision)\n",
        "\n",
        "    plt.figure()\n",
        "    plt.plot(recall, precision, color='blue', lw=2, label=f'PR curve (AUC = {pr_auc:.2f})')\n",
        "    plt.xlabel('Recall')\n",
        "    plt.ylabel('Precision')\n",
        "    plt.title('Precision-Recall Curve on External Data')\n",
        "    plt.legend(loc='lower left')\n",
        "    plt.show()\n",
        "\n",
        "plot_precision_recall_curve(external_y_true, external_predictions_proba)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SRCdkOf-wR4z"
      },
      "outputs": [],
      "source": [
        "# Covid unexposed"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WBiXGjlTwR4z"
      },
      "outputs": [],
      "source": [
        "\n",
        "# ori_train = pd.read_csv('D:\\\\PhD\\Subgraph_Sampling_SLOT_GNN\\\\Data\\\\remove0123_sample100.csv')\n",
        "ori_train_Dengue = pd.read_csv('D:\\\\PhD\\Subgraph_Sampling_SLOT_GNN\\\\Data\\\\sars_cov_2_result.csv')\n",
        "ori_train_Dengue_bk = ori_train_Dengue\n",
        "hla = pd.read_csv('D:\\\\PhD\\\\GraphImmuno_NewResults\\\\data\\\\hla2paratopeTable_aligned.txt',sep='\\t')\n",
        "after_pca = np.loadtxt('D:\\\\PhD\\\\GraphImmuno_NewResults\\\\data\\\\after_pca.txt')\n",
        "\n",
        "hla_dic = hla_df_to_dic(hla)\n",
        "inventory = list(hla_dic.keys())\n",
        "dic_inventory = dict_inventory(inventory)\n",
        "\n",
        "ori_train_Dengue['immunogenicity'] = ori_train_Dengue['immunogenicity-un']\n",
        "\n",
        "# ori_train['immunogenicity'], ori_train['potential'] = ori_train_external['potential'], ori_train_external['immunogenicity']\n",
        "\n",
        "# Create graphs and labels using the new HyperGraph_Constructor\n",
        "#graphs, graph_labels = HyperGraph_Constructor_DHT.entrance(ori_train, after_pca, hla_dic, dic_inventory)\n",
        "#graphs, graph_labels = HyperGraph_Constructor_DHT_HyperDrop.entrance(ori_train, after_pca, hla_dic, dic_inventory, top_k=0.5)\n",
        "external_graphs, external_graph_labels = HyperGraph_Constructor_DHT_Cluster.entrance(ori_train_Dengue, after_pca, hla_dic, dic_inventory, n_clusters=5)\n",
        "\n",
        "# Initialize the generator for dual hypergraphs\n",
        "generator = PaddedGraphGenerator(graphs=graphs)\n",
        "\n",
        "count_graph_labels = ori_train_Dengue['immunogenicity']\n",
        "count_graph_labels.value_counts().to_frame()\n",
        "\n",
        "assert len(external_graphs) == len(external_graph_labels), \"Mismatch in number of graphs and labels.\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "scrolled": true,
        "id": "bmGC2rahwR4z"
      },
      "outputs": [],
      "source": [
        "# Step 1: Load external datasets\n",
        "from stellargraph import StellarGraph\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "# Ensure data integrity\n",
        "assert len(external_graphs) == len(external_graph_labels), \"Mismatch in graphs and labels.\"\n",
        "\n",
        "# Step 2: Preprocess external graphs (if necessary)\n",
        "node_feature_size = graphs[0].node_features().shape[1]\n",
        "\n",
        "def preprocess_graph(graph):\n",
        "    features = graph.node_features()\n",
        "    if features.shape[1] != node_feature_size:\n",
        "        padded_features = np.zeros((features.shape[0], node_feature_size))\n",
        "        padded_features[:, :features.shape[1]] = features\n",
        "        graph = StellarGraph(graph.to_networkx(), node_features=padded_features)\n",
        "    return graph\n",
        "\n",
        "external_graphs = [preprocess_graph(g) for g in external_graphs]\n",
        "\n",
        "# Step 3: Create data generators\n",
        "from stellargraph.mapper import PaddedGraphGenerator\n",
        "\n",
        "external_generator = PaddedGraphGenerator(graphs=external_graphs)\n",
        "external_index = np.arange(len(external_graphs))\n",
        "\n",
        "external_gen = external_generator.flow(\n",
        "    external_index,\n",
        "    targets=external_graph_labels.values,\n",
        "    batch_size=50,\n",
        "    symmetric_normalization=False,\n",
        ")\n",
        "\n",
        "# Step 4: Predict on external data\n",
        "external_predictions_proba = model.predict(external_gen).ravel()\n",
        "\n",
        "# Adjust the threshold based on precision-recall curve\n",
        "from sklearn.metrics import precision_recall_curve\n",
        "\n",
        "precision, recall, thresholds = precision_recall_curve(external_graph_labels.values, external_predictions_proba)\n",
        "f1_scores = 2 * (precision * recall) / (precision + recall)\n",
        "optimal_idx = np.argmax(f1_scores)\n",
        "optimal_threshold = thresholds[optimal_idx]\n",
        "print(f\"Optimal Threshold: {optimal_threshold:.4f}\")\n",
        "\n",
        "# Apply the optimal threshold\n",
        "external_predictions = (external_predictions_proba > optimal_threshold).astype(int)\n",
        "\n",
        "# Step 5: Compute and display metrics\n",
        "from sklearn.metrics import accuracy_score, matthews_corrcoef, roc_auc_score, average_precision_score\n",
        "\n",
        "external_y_true = external_graph_labels.values\n",
        "external_accuracy = accuracy_score(external_y_true, external_predictions)\n",
        "external_mcc = matthews_corrcoef(external_y_true, external_predictions)\n",
        "external_roc_auc = roc_auc_score(external_y_true, external_predictions_proba)\n",
        "external_pr_auc = average_precision_score(external_y_true, external_predictions_proba)\n",
        "\n",
        "print(f\"Accuracy on External Data: {external_accuracy:.4f}\")\n",
        "print(f\"Matthews Correlation Coefficient (MCC): {external_mcc:.4f}\")\n",
        "print(f\"ROC AUC Score: {external_roc_auc:.4f}\")\n",
        "print(f\"Precision-Recall AUC: {external_pr_auc:.4f}\")\n",
        "\n",
        "# Step 6: Additional metrics and visualization\n",
        "from sklearn.metrics import classification_report, confusion_matrix\n",
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "print(\"\\nClassification Report on External Data:\")\n",
        "print(classification_report(external_y_true, external_predictions))\n",
        "\n",
        "def plot_confusion_matrix(y_true, y_pred):\n",
        "    cm = confusion_matrix(y_true, y_pred)\n",
        "    plt.figure(figsize=(7, 5))\n",
        "    sns.heatmap(\n",
        "        cm,\n",
        "        annot=True,\n",
        "        fmt='d',\n",
        "        cmap='Blues',\n",
        "        xticklabels=['Negative', 'Positive'],\n",
        "        yticklabels=['Negative', 'Positive']\n",
        "    )\n",
        "    plt.ylabel('Actual')\n",
        "    plt.xlabel('Predicted')\n",
        "    plt.title('Confusion Matrix on External Data')\n",
        "    plt.show()\n",
        "\n",
        "# Plot the confusion matrix\n",
        "plot_confusion_matrix(external_y_true, external_predictions)\n",
        "\n",
        "# Plot ROC Curve\n",
        "def plot_roc_curve(y_true, y_proba):\n",
        "    from sklearn.metrics import roc_curve, auc\n",
        "    fpr, tpr, _ = roc_curve(y_true, y_proba)\n",
        "    roc_auc = auc(fpr, tpr)\n",
        "\n",
        "    plt.figure()\n",
        "    plt.plot(fpr, tpr, color='darkorange', lw=2, label=f'ROC curve (AUC = {roc_auc:.2f})')\n",
        "    plt.plot([0, 1], [0, 1], color='navy', lw=2, linestyle='--')\n",
        "    plt.xlim([-0.05, 1.0])\n",
        "    plt.ylim([0.0, 1.05])\n",
        "    plt.xlabel('False Positive Rate')\n",
        "    plt.ylabel('True Positive Rate')\n",
        "    plt.title('ROC Curve on External Data')\n",
        "    plt.legend(loc='lower right')\n",
        "    plt.show()\n",
        "\n",
        "plot_roc_curve(external_y_true, external_predictions_proba)\n",
        "\n",
        "# Plot Precision-Recall Curve\n",
        "def plot_precision_recall_curve(y_true, y_proba):\n",
        "    from sklearn.metrics import precision_recall_curve, auc\n",
        "    precision, recall, _ = precision_recall_curve(y_true, y_proba)\n",
        "    pr_auc = auc(recall, precision)\n",
        "\n",
        "    plt.figure()\n",
        "    plt.plot(recall, precision, color='blue', lw=2, label=f'PR curve (AUC = {pr_auc:.2f})')\n",
        "    plt.xlabel('Recall')\n",
        "    plt.ylabel('Precision')\n",
        "    plt.title('Precision-Recall Curve on External Data')\n",
        "    plt.legend(loc='lower left')\n",
        "    plt.show()\n",
        "\n",
        "plot_precision_recall_curve(external_y_true, external_predictions_proba)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RBNM4wRqwR4z"
      },
      "outputs": [],
      "source": [
        "type(graphs[0])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sEjOJ4KhwR4z"
      },
      "outputs": [],
      "source": [
        "# Unsupervised Learning"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BIF55vTfwR4z"
      },
      "outputs": [],
      "source": [
        "!pip install graph2vec"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xMgFcXwywR4z"
      },
      "outputs": [],
      "source": [
        "!pip install karateclub"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6O08b-U0wR4z"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "\n",
        "import networkx as nx\n",
        "from karateclub import Graph2Vec\n",
        "\n",
        "from sklearn.cluster import KMeans\n",
        "from sklearn.decomposition import PCA\n",
        "from sklearn.manifold import TSNE\n",
        "\n",
        "\n",
        "\n",
        "# Convert StellarGraph objects to NetworkX graphs and relabel nodes\n",
        "nx_graphs = []\n",
        "for g in graphs:\n",
        "    # Convert to NetworkX graph\n",
        "    nx_g = g.to_networkx()\n",
        "\n",
        "    # Relabel nodes to consecutive integers starting from 0\n",
        "    nx_g = nx.convert_node_labels_to_integers(nx_g, first_label=0, ordering='default')\n",
        "    nx_graphs.append(nx_g)\n",
        "\n",
        "# Instantiate Graph2Vec model\n",
        "graph2vec_model = Graph2Vec(dimensions=128, workers=4, epochs=10, min_count=1)\n",
        "\n",
        "# Fit the model to the list of NetworkX graphs\n",
        "print(\"Fitting Graph2Vec model...\")\n",
        "graph2vec_model.fit(nx_graphs)\n",
        "\n",
        "# Get the embeddings for each graph\n",
        "graph_embeddings = graph2vec_model.get_embedding()\n",
        "\n",
        "# Apply KMeans clustering\n",
        "n_clusters = 5\n",
        "kmeans = KMeans(n_clusters=n_clusters, random_state=42)\n",
        "clusters = kmeans.fit_predict(graph_embeddings)\n",
        "\n",
        "# Analyze clusters\n",
        "unique, counts = np.unique(clusters, return_counts=True)\n",
        "cluster_info = dict(zip(unique, counts))\n",
        "print(\"Cluster Distribution:\")\n",
        "for cluster_id, size in cluster_info.items():\n",
        "    print(f\"Cluster {cluster_id}: {size} graphs\")\n",
        "\n",
        "# Visualize with PCA\n",
        "pca = PCA(n_components=2)\n",
        "reduced_embeddings = pca.fit_transform(graph_embeddings)\n",
        "\n",
        "plt.figure(figsize=(10, 8))\n",
        "sns.scatterplot(\n",
        "    x=reduced_embeddings[:, 0],\n",
        "    y=reduced_embeddings[:, 1],\n",
        "    hue=clusters,\n",
        "    palette=\"viridis\",\n",
        "    s=50,\n",
        "    alpha=0.8\n",
        ")\n",
        "plt.title(\"Graph Clustering Visualization with PCA\")\n",
        "plt.xlabel(\"Principal Component 1\")\n",
        "plt.ylabel(\"Principal Component 2\")\n",
        "plt.legend(title='Cluster')\n",
        "plt.show()\n",
        "\n",
        "# Visualize with t-SNE\n",
        "tsne = TSNE(n_components=2, perplexity=30, n_iter=1000, random_state=42)\n",
        "tsne_embeddings = tsne.fit_transform(graph_embeddings)\n",
        "\n",
        "plt.figure(figsize=(10, 8))\n",
        "sns.scatterplot(\n",
        "    x=tsne_embeddings[:, 0],\n",
        "    y=tsne_embeddings[:, 1],\n",
        "    hue=clusters,\n",
        "    palette=\"viridis\",\n",
        "    s=50,\n",
        "    alpha=0.8\n",
        ")\n",
        "plt.title(\"Graph Clustering Visualization with t-SNE\")\n",
        "plt.xlabel(\"t-SNE Dimension 1\")\n",
        "plt.ylabel(\"t-SNE Dimension 2\")\n",
        "plt.legend(title='Cluster')\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VRMu5-lowR40"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "\n",
        "import networkx as nx\n",
        "from stellargraph import StellarGraph\n",
        "from karateclub import Graph2Vec\n",
        "\n",
        "from sklearn.cluster import KMeans\n",
        "from sklearn.decomposition import PCA\n",
        "from sklearn.manifold import TSNE\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0-7PkZ6jwR40"
      },
      "outputs": [],
      "source": [
        "nx_graphs = []\n",
        "for g in graphs:\n",
        "    # Convert to NetworkX graph\n",
        "    nx_g = g.to_networkx()\n",
        "\n",
        "    # Relabel nodes to consecutive integers starting from 0\n",
        "    nx_g = nx.convert_node_labels_to_integers(nx_g, first_label=0, ordering='default')\n",
        "    nx_graphs.append(nx_g)\n",
        "# Instantiate Graph2Vec model\n",
        "graph2vec_model = Graph2Vec(dimensions=128, workers=4, epochs=10, min_count=1)\n",
        "\n",
        "# Fit the model to the list of NetworkX graphs\n",
        "print(\"Fitting Graph2Vec model...\")\n",
        "graph2vec_model.fit(nx_graphs)\n",
        "\n",
        "# Get the embeddings for each graph\n",
        "graph_embeddings = graph2vec_model.get_embedding()\n",
        "\n",
        "# Define the number of clusters\n",
        "n_clusters = 5\n",
        "\n",
        "# Apply KMeans clustering\n",
        "kmeans = KMeans(n_clusters=n_clusters, random_state=42)\n",
        "clusters = kmeans.fit_predict(graph_embeddings)\n",
        "\n",
        "# Analyze clusters\n",
        "unique, counts = np.unique(clusters, return_counts=True)\n",
        "cluster_info = dict(zip(unique, counts))\n",
        "print(\"Cluster Distribution:\")\n",
        "for cluster_id, size in cluster_info.items():\n",
        "    print(f\"Cluster {cluster_id}: {size} graphs\")\n",
        "# Reduce dimensionality with PCA\n",
        "pca = PCA(n_components=2)\n",
        "reduced_embeddings = pca.fit_transform(graph_embeddings)\n",
        "\n",
        "# Plot the clusters\n",
        "plt.figure(figsize=(10, 8))\n",
        "sns.scatterplot(\n",
        "    x=reduced_embeddings[:, 0],\n",
        "    y=reduced_embeddings[:, 1],\n",
        "    hue=clusters,\n",
        "    palette=\"viridis\",\n",
        "    s=50,\n",
        "    alpha=0.8\n",
        ")\n",
        "plt.title(\"Graph Clustering Visualization with PCA\")\n",
        "plt.xlabel(\"Principal Component 1\")\n",
        "plt.ylabel(\"Principal Component 2\")\n",
        "plt.legend(title='Cluster')\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SlOepxuXwR40"
      },
      "outputs": [],
      "source": [
        "# Reduce dimensionality with t-SNE\n",
        "tsne = TSNE(n_components=2, perplexity=30, n_iter=1000, random_state=42)\n",
        "tsne_embeddings = tsne.fit_transform(graph_embeddings)\n",
        "\n",
        "# Plot the clusters\n",
        "plt.figure(figsize=(10, 8))\n",
        "sns.scatterplot(\n",
        "    x=tsne_embeddings[:, 0],\n",
        "    y=tsne_embeddings[:, 1],\n",
        "    hue=clusters,\n",
        "    palette=\"viridis\",\n",
        "    s=50,\n",
        "    alpha=0.8\n",
        ")\n",
        "plt.title(\"Graph Clustering Visualization with t-SNE\")\n",
        "plt.xlabel(\"t-SNE Dimension 1\")\n",
        "plt.ylabel(\"t-SNE Dimension 2\")\n",
        "plt.legend(title='Cluster')\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cYxzAXoqwR40"
      },
      "outputs": [],
      "source": [
        "#Graph Kernel Methods"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BK_wV1GawR40"
      },
      "outputs": [],
      "source": [
        "!pip install grakel"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RME3WQk-wR40"
      },
      "outputs": [],
      "source": [
        "#3. Subgraph/Motif Mining"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Tgt2kMd_wR40"
      },
      "outputs": [],
      "source": [
        "import networkx as nx\n",
        "\n",
        "# Find cliques (fully connected subgraphs) in each graph\n",
        "for i, nx_g in enumerate(nx_graphs):\n",
        "    cliques = list(nx.find_cliques(nx_g))\n",
        "    print(f\"Graph {i} has {len(cliques)} cliques\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xg3_H445wR41"
      },
      "outputs": [],
      "source": [
        "import networkx as nx\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Assuming nx_graphs is your list of NetworkX graphs\n",
        "# Ensure that your graphs are simple graphs (no multiple edges)\n",
        "for i, nx_g in enumerate(nx_graphs):\n",
        "    if isinstance(nx_g, nx.MultiGraph) or isinstance(nx_g, nx.MultiDiGraph):\n",
        "        nx_g = nx.Graph(nx_g)  # Convert to simple graph\n",
        "        nx_graphs[i] = nx_g\n",
        "\n",
        "# Calculate clique sizes\n",
        "clique_sizes = []\n",
        "for i, nx_g in enumerate(nx_graphs):\n",
        "    # Find all maximal cliques in the graph\n",
        "    cliques = list(nx.find_cliques(nx_g))\n",
        "    # Get the sizes of each clique and extend the list\n",
        "    clique_sizes.extend([len(clique) for clique in cliques])\n",
        "    print(f\"Graph {i+1} has {len(cliques)} cliques\")\n",
        "\n",
        "# Plot the distribution of clique sizes\n",
        "plt.figure(figsize=(10, 6))\n",
        "plt.hist(clique_sizes, bins=range(1, max(clique_sizes)+2), align='left', edgecolor='black', rwidth=0.8)\n",
        "plt.title('Distribution of Clique Sizes Across All Graphs')\n",
        "plt.xlabel('Clique Size')\n",
        "plt.ylabel('Frequency')\n",
        "plt.xticks(range(1, max(clique_sizes)+1))\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ynq8I2J2wR41"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import seaborn as sns\n",
        "\n",
        "# Create a DataFrame from the clique sizes\n",
        "clique_sizes_df = pd.DataFrame({'Clique Size': clique_sizes})\n",
        "\n",
        "# Count the frequency of each clique size\n",
        "clique_size_counts = clique_sizes_df['Clique Size'].value_counts().sort_index()\n",
        "\n",
        "# Plot the bar chart\n",
        "plt.figure(figsize=(10, 6))\n",
        "sns.barplot(x=clique_size_counts.index, y=clique_size_counts.values, color='skyblue')\n",
        "plt.title('Distribution of Clique Sizes Across All Graphs')\n",
        "plt.xlabel('Clique Size')\n",
        "plt.ylabel('Frequency')\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "R-kMeetcwR41"
      },
      "outputs": [],
      "source": [
        "import networkx as nx\n",
        "\n",
        "# Select a graph to visualize\n",
        "graph_index = 0  # Change to the index of the graph you want to visualize\n",
        "nx_g = nx_graphs[graph_index]\n",
        "\n",
        "# Find cliques\n",
        "cliques = list(nx.find_cliques(nx_g))\n",
        "\n",
        "# Get the largest cliques\n",
        "max_clique_size = max(len(clique) for clique in cliques)\n",
        "largest_cliques = [clique for clique in cliques if len(clique) == max_clique_size]\n",
        "\n",
        "# Visualize the graph highlighting the largest cliques\n",
        "pos = nx.spring_layout(nx_g)\n",
        "plt.figure(figsize=(12, 8))\n",
        "nx.draw_networkx_edges(nx_g, pos, alpha=0.5)\n",
        "nx.draw_networkx_nodes(nx_g, pos, node_color='lightblue', node_size=300)\n",
        "nx.draw_networkx_labels(nx_g, pos, font_size=10)\n",
        "\n",
        "# Highlight nodes in the largest cliques\n",
        "for clique in largest_cliques:\n",
        "    nx.draw_networkx_nodes(nx_g, pos, nodelist=clique, node_color='red', node_size=300)\n",
        "\n",
        "plt.title(f'Graph {graph_index+1} with Largest Cliques Highlighted')\n",
        "plt.axis('off')\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HKi7--0owR41"
      },
      "outputs": [],
      "source": [
        "import networkx as nx\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Assuming nx_graphs is your list of NetworkX graphs\n",
        "# Ensure that your graphs are simple graphs (no multiple edges)\n",
        "for i, nx_g in enumerate(nx_graphs):\n",
        "    if isinstance(nx_g, nx.MultiGraph) or isinstance(nx_g, nx.MultiDiGraph):\n",
        "        nx_g = nx.Graph(nx_g)  # Convert to simple graph\n",
        "        nx_graphs[i] = nx_g\n",
        "\n",
        "# Calculate clique sizes\n",
        "clique_sizes = []\n",
        "for i, nx_g in enumerate(nx_graphs):\n",
        "    # Find all maximal cliques in the graph\n",
        "    cliques = list(nx.find_cliques(nx_g))\n",
        "    # Get the sizes of each clique and extend the list\n",
        "    clique_sizes.extend([len(clique) for clique in cliques])\n",
        "    print(f\"Graph {i+1} has {len(cliques)} cliques\")\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CPFQ79x5wR41"
      },
      "outputs": [],
      "source": [
        "# Plot the distribution of clique sizes\n",
        "plt.figure(figsize=(10, 6))\n",
        "plt.hist(clique_sizes, bins=range(1, max(clique_sizes)+2), align='left', edgecolor='black', rwidth=0.8)\n",
        "plt.title('Distribution of Clique Sizes Across All Graphs')\n",
        "plt.xlabel('Clique Size')\n",
        "plt.ylabel('Frequency')\n",
        "plt.xticks(range(1, max(clique_sizes)+1))\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pqUKvwwBwR41"
      },
      "outputs": [],
      "source": [
        "#4. Graph Generative Models"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "A_pZWK5BwR41"
      },
      "outputs": [],
      "source": [
        "#5. Clustering Based on Graph Features"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4NKbd91nwR42"
      },
      "outputs": [],
      "source": [
        "# Extract features for each graph\n",
        "import networkx as nx\n",
        "\n",
        "graph_features = []\n",
        "\n",
        "for nx_g in nx_graphs:\n",
        "    # Convert MultiGraph to simple Graph\n",
        "    if isinstance(nx_g, nx.MultiGraph) or isinstance(nx_g, nx.MultiDiGraph):\n",
        "        nx_g = nx.Graph(nx_g)\n",
        "\n",
        "    features = {}\n",
        "    features['num_nodes'] = nx_g.number_of_nodes()\n",
        "    features['num_edges'] = nx_g.number_of_edges()\n",
        "    features['average_degree'] = sum(dict(nx_g.degree()).values()) / nx_g.number_of_nodes()\n",
        "    features['density'] = nx.density(nx_g)\n",
        "    features['average_clustering'] = nx.average_clustering(nx_g)\n",
        "    # Add more features as needed\n",
        "\n",
        "    graph_features.append(features)\n",
        "\n",
        "# Convert to DataFrame\n",
        "import pandas as pd\n",
        "graph_features_df = pd.DataFrame(graph_features)\n",
        "\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "\n",
        "scaler = StandardScaler()\n",
        "scaled_features = scaler.fit_transform(graph_features_df)\n",
        "from sklearn.cluster import KMeans\n",
        "\n",
        "n_clusters = 5\n",
        "kmeans = KMeans(n_clusters=n_clusters, random_state=42)\n",
        "clusters = kmeans.fit_predict(scaled_features)\n",
        "\n",
        "# Add clusters to DataFrame\n",
        "graph_features_df['cluster'] = clusters\n",
        "\n",
        "# Reduce dimensionality for visualization\n",
        "from sklearn.decomposition import PCA\n",
        "\n",
        "pca = PCA(n_components=2)\n",
        "reduced_features = pca.fit_transform(scaled_features)\n",
        "\n",
        "plt.figure(figsize=(10, 8))\n",
        "sns.scatterplot(\n",
        "    x=reduced_features[:, 0],\n",
        "    y=reduced_features[:, 1],\n",
        "    hue=clusters,\n",
        "    palette=\"viridis\",\n",
        "    s=50,\n",
        "    alpha=0.8\n",
        ")\n",
        "plt.title(\"Graph Clustering Based on Structural Features\")\n",
        "plt.xlabel(\"Principal Component 1\")\n",
        "plt.ylabel(\"Principal Component 2\")\n",
        "plt.legend(title='Cluster')\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_WmIU3TawR42"
      },
      "outputs": [],
      "source": [
        "#6. Node Embedding Learning Across Multiple Graphs"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MctpZnDQwR42"
      },
      "outputs": [],
      "source": [
        "!pip install node2vec"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CCtql5-rwR42"
      },
      "outputs": [],
      "source": [
        "from node2vec import Node2Vec\n",
        "\n",
        "node_embeddings = []\n",
        "\n",
        "for i, nx_g in enumerate(nx_graphs):\n",
        "    print(f\"Processing graph {i+1}/{len(nx_graphs)}\")\n",
        "\n",
        "    # Ensure the graph is connected and has enough nodes\n",
        "    if nx_g.number_of_nodes() < 2:\n",
        "        print(f\"Graph {i+1} is too small. Skipping.\")\n",
        "        continue\n",
        "\n",
        "    if not nx.is_connected(nx_g):\n",
        "        # Take the largest connected component\n",
        "        largest_cc = max(nx.connected_components(nx_g), key=len)\n",
        "        nx_g = nx_g.subgraph(largest_cc).copy()\n",
        "\n",
        "    try:\n",
        "        # Create and fit Node2Vec model\n",
        "        node2vec = Node2Vec(\n",
        "            nx_g,\n",
        "            dimensions=32,\n",
        "            walk_length=10,\n",
        "            num_walks=50,\n",
        "            workers=1\n",
        "        )\n",
        "        model = node2vec.fit(window=5, min_count=1, batch_words=4)\n",
        "\n",
        "        embeddings = model.wv\n",
        "        node_embeddings.append(embeddings)\n",
        "    except Exception as e:\n",
        "        print(f\"Error processing graph {i+1}: {e}\")\n",
        "        continue\n",
        "\n",
        "    # Fit model\n",
        "    model = node2vec.fit(window=10, min_count=1, batch_words=4)\n",
        "\n",
        "    # Get embeddings as a dictionary {node_id: embedding_vector}\n",
        "    embeddings = model.wv\n",
        "    node_embeddings.append(embeddings)\n",
        "\n",
        "    graph_embeddings = []\n",
        "\n",
        "for embeddings in node_embeddings:\n",
        "    # Aggregate node embeddings to create a graph embedding\n",
        "    # You can take the mean of all node embeddings\n",
        "    emb_vectors = np.array([embeddings[node_id] for node_id in embeddings.key_to_index])\n",
        "    graph_emb = np.mean(emb_vectors, axis=0)\n",
        "    graph_embeddings.append(graph_emb)\n",
        "\n",
        "graph_embeddings = np.array(graph_embeddings)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "U5IEI_3lwR44"
      },
      "outputs": [],
      "source": [
        "graph_embeddings"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "isbBo7erwR44"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "\n",
        "from sklearn.decomposition import PCA\n",
        "from sklearn.manifold import TSNE\n",
        "from sklearn.cluster import KMeans\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KzzHHDPuwR44"
      },
      "outputs": [],
      "source": [
        "!pip install umap-learn\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ltVdgO1TwR45"
      },
      "outputs": [],
      "source": [
        "!pip install importlib-metadata"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hkSmOnOZwR45"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "\n",
        "from sklearn.decomposition import PCA\n",
        "from sklearn.manifold import TSNE\n",
        "from sklearn.cluster import KMeans\n",
        "\n",
        "# Assuming graph_embeddings is your numpy array of shape (num_graphs, embedding_dim)\n",
        "\n",
        "# Verify that graph_embeddings is a NumPy array\n",
        "print(type(graph_embeddings))\n",
        "print(graph_embeddings.shape)\n",
        "\n",
        "# Proceed if it's a NumPy array\n",
        "if isinstance(graph_embeddings, np.ndarray):\n",
        "    # Clustering\n",
        "    n_clusters = 5  # Adjust as needed\n",
        "    kmeans = KMeans(n_clusters=n_clusters, random_state=42)\n",
        "    clusters = kmeans.fit_predict(graph_embeddings)\n",
        "\n",
        "    # PCA Visualization\n",
        "    pca = PCA(n_components=2)\n",
        "    graph_embeddings_pca = pca.fit_transform(graph_embeddings)\n",
        "\n",
        "    plt.figure(figsize=(10, 8))\n",
        "    sns.scatterplot(\n",
        "        x=graph_embeddings_pca[:, 0],\n",
        "        y=graph_embeddings_pca[:, 1],\n",
        "        hue=clusters,\n",
        "        palette='viridis',\n",
        "        s=50,\n",
        "        alpha=0.8\n",
        "    )\n",
        "    plt.title('Graph Embeddings with PCA and KMeans Clustering')\n",
        "    plt.xlabel('Principal Component 1')\n",
        "    plt.ylabel('Principal Component 2')\n",
        "    plt.legend(title='Cluster')\n",
        "    plt.show()\n",
        "else:\n",
        "    print(\"graph_embeddings is not a NumPy array. Please check your code.\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Jdoa6AmUwR45"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gPEG83f1wR45"
      },
      "outputs": [],
      "source": [
        "#Using Graph Neural Networks for Graph Embeddings"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wJHsEzo2wR46"
      },
      "outputs": [],
      "source": [
        "from stellargraph.mapper import PaddedGraphGenerator\n",
        "from stellargraph.layer import GCNSupervisedGraphClassification\n",
        "from tensorflow.keras.models import Model\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "\n",
        "generator = PaddedGraphGenerator(graphs=graphs)\n",
        "\n",
        "# Define the GCN model\n",
        "gcn_model = GCNSupervisedGraphClassification(\n",
        "    layer_sizes=[64, 64],\n",
        "    activations=[\"relu\", \"relu\"],\n",
        "    generator=generator,\n",
        "    dropout=0.5,\n",
        ")\n",
        "\n",
        "# Build the model\n",
        "x_inp, x_out = gcn_model.in_out_tensors()\n",
        "graph_embeddings = x_out  # This is the graph embedding\n",
        "\n",
        "# Since we are doing unsupervised learning, we can use the embeddings directly\n",
        "embedding_model = Model(inputs=x_inp, outputs=graph_embeddings)\n",
        "\n",
        "# Generate embeddings for all graphs\n",
        "graph_indices = np.arange(len(graphs))\n",
        "embeddings = embedding_model.predict(generator.flow(graph_indices, batch_size=50), verbose=1)\n",
        "\n",
        "# Now, embeddings can be used for clustering or visualization\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CMGAYGmnwR46"
      },
      "outputs": [],
      "source": [
        "# Plot the clusters\n",
        "plt.figure(figsize=(10, 8))\n",
        "sns.scatterplot(\n",
        "    x=embeddings[:, 0],\n",
        "    y=embeddings[:, 1],\n",
        "    hue=clusters,\n",
        "    palette=\"viridis\",\n",
        "    s=50,\n",
        "    alpha=0.8\n",
        ")\n",
        "plt.title(\"Graph Neural Networks for Graph Embeddings\")\n",
        "plt.xlabel(\"Dimension 1\")\n",
        "plt.ylabel(\"Dimension 2\")\n",
        "plt.legend(title='Cluster')\n",
        "plt.show()"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.16"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}